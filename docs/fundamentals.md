# Fundamentals

## Application setup

In a `freeact` application you need:

- A **code execution container**. This is an [`ipybox`](https://gradion-ai.github.io/ipybox/) Docker container running a resource server for providing skill sources and a [Jupyter Kernel Gateway](https://jupyter-kernel-gateway.readthedocs.io/) for stateful code execution in [IPython kernels](https://ipython.readthedocs.io/). `ipybox` containers are managed by the [`CodeExecutionContainer`][freeact.environment.CodeExecutionContainer] context manager. The container in the following example is based on the prebuilt [`ghcr.io/gradion-ai/ipybox:basic`](environment.md#prebuilt-docker-images) Docker image.

- A **code provider** for downloading skill sources from the resource server and registering MCP servers. During registration, the resource server automatically generates Python client functions from MCP tool metadata so that the generated sources can be served to clients. A code provider is created with the [`CodeProvider`][freeact.environment.CodeProvider] context manager. It communicates with the container's resource server at `resource_port`.

- A **code executor** for executing code actions generated by a code action model. A code executor is created with the [`CodeExecutor`][freeact.environment.CodeExecutor] context manager. On entering the context, it connects to the container's kernel gateway at `executor_port` and creates a new IPython kernel. Code executions made with the same `CodeExecutor` instance are stateful i.e. executed code can access definitions and results from previous executions.

- A **code action model** for generating code actions in response to user queries and code execution results or errors. Code action models are created with the [`LiteCodeActModel`][freeact.model.litellm.LiteCodeActModel] class which supports any model that is also supported by [LiteLLM](https://github.com/BerriAI/litellm), including locally deployed models. Skill sources loaded with the code provider are passed to the model's constructor.

- A **code action agent** for coordinating the interaction between a code action model, a code executor and the user or another agent. `freeact` agents are created with the [`CodeActAgent`][freeact.agent.CodeActAgent] class. Depending on the complexity of a user query, the agent generates and executes a sequence of one or more code actions until the model provides a final response to the user[^1]. 

[^1]: For trivial queries, the model may also decide to provide a final response directly, without generating a code action.

This is demonstrated in the following example. You need an [Anthropic](https://console.anthropic.com/settings/keys) and a [Gemini](https://aistudio.google.com/app/apikey) API key for running it.

```python title="examples/fundamentals_1.py"
--8<-- "examples/fundamentals_1.py"
```

1. A `GEMINI_API_KEY` is needed for [generative Google search with Gemini](https://ai.google.dev/gemini-api/docs/grounding?lang=python) in the code execution container.

2. A module that provides [generative Google search with Gemini](https://ai.google.dev/gemini-api/docs/grounding?lang=python). It is pre-installed in the code execution container.

3. Needed for the code actions model. Added here for clarity but can be omitted if the `ANTHROPIC_API_KEY` is set as environment variable.

4. Runs a minimalistic text-based interface for [interacting with the agent](#agent-protocol).

!!! Info

    Using a code provider is optional if you don't want to provide skill sources to a code action model. At the moment, it is the responsibility of an application to provide skill sources to a code action model. We will soon enhance `freeact` agents to [retrieve skill sources autonomously](skills/autonomous-learning.md) depending on the user query and current state.

!!! Tip "MCP Integration"

    For examples how to use [MCP](https://modelcontextprotocol.io/) servers in code actions, see sections [Quickstart](quickstart.md) and [MCP integration](mcp-integration.md).

## Higher-level API

Using the lower-level API, as in the [previous section](#application-setup), comes with some boilerplate code for constructing `CodeExecutionContainer`, `CodeProvider` and `CodeExecutor` instances. You can avoid this using the higher-level [`execution_environment`][freeact.environment.execution_environment] context manager which creates a [`CodeExecutionEnvironment`][freeact.environment.CodeExecutionEnvironment] instance that provides more convenient context managers for code provider and code executor. The following code is equivalent to the previous one:

```python title="examples/fundamentals_2.py"
--8<-- "examples/fundamentals_2.py"
```

1. Runs a minimalistic text-based interface for [interacting with the agent](#agent-protocol).

## Agent protocol

`freeact` provides a wide spectrum of options for interacting with agents, from getting just the final response to streaming model and code execution outputs as they are generated at all steps. 

### Final response

For obtaining only the final response, await [`response`][freeact.agent.CodeActAgentTurn.response] on a [`CodeActAgentTurn`][freeact.agent.CodeActAgentTurn] which is returned by the [`CodeActAgent.run`][freeact.agent.CodeActAgent.run] method. This waits until the task-specific sequence of model interactions and code executions is complete.

```python title="examples/utils.py::final_response"
--8<-- "examples/utils.py:final_response"
```

1. Waits until the sequence of model interactions and code executions is complete. 

### Progressive streaming

For streaming model and code execution outputs as they are generated at each step, consume `stream` on [`CodeActAgentTurn`][freeact.agent.CodeActAgentTurn], [`CodeActModelTurn`][freeact.model.CodeActModelTurn] and [`CodeExecution`][freeact.environment.CodeExecution] objects. After a stream has been fully consumed at a given step, the corresponding aggregated [`CodeActAgentResponse`][freeact.agent.CodeActAgentResponse], [`CodeActModelResponse`][freeact.model.CodeActModelResponse] or [`CodeExecutionResult`][freeact.environment.CodeExecutionResult] objects are available immediately without waiting, as demonstrated in the following example:

```python title="examples/utils.py::stream_conversation"
--8<-- "examples/utils.py:stream_conversation"
--8<-- "examples/utils.py:stream_turn"
```

1. Returns immediately as `turn.stream()` is already consumed.

2. The code action produced by the model. If `None`, the response is a final response.

3. Returns immediately as `execution.stream()` is already consumed.

4. Accumulates token usage and costs across multiple agent runs in a conversation. See also [usage statistics](#usage-statistics).

## Usage statistics

[`CodeActModelResponse.usage`][freeact.model.CodeActModelResponse.usage] contains token usage and costs for a single model interaction, [`CodeActAgentResponse.usage`][freeact.agent.CodeActAgentResponse.usage] contains accumulated token usage and costs for a single agent [`run`][freeact.agent.CodeActAgent.run]. For accumulating token usage and costs across agent runs in a conversation, create a [`CodeActModelUsage`][freeact.model.CodeActModelUsage] object on application level and update it with [`usage`][freeact.agent.CodeActAgentResponse.usage] objects from agent responses.
