{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p><code>freeact</code> is a lightweight AI agent library using Python as the common language to define executable actions and tool interfaces. This is in contrast to traditional approaches where actions and tools are described with JSON.</p> <p>A unified code-based approach enables <code>freeact</code> agents to reuse actions from earlier steps as tools or skills in later steps.  This design allows agents to build on their previous work and compose more complex actions from simpler ones.<sup>1</sup></p> <p> </p> A unified code-based approach for defining actions and skills."},{"location":"#overview","title":"Overview","text":"<p><code>freeact</code> agents are LLM agents that:</p> <ul> <li>generate code actions in Python instead of function calls in JSON<sup>2</sup></li> <li>act by executing these code actions in a sandboxed environment</li> <li>use tools described through code and docstrings rather than JSON</li> <li>can use any feature from any Python package as tool definitions</li> <li>can store code actions as reusable skills in long-term memory</li> <li>can use these skills as tools in code actions and improve on them</li> <li>support invocation and composition of MCP tools in code actions</li> </ul> <p>Supported models</p> <p><code>freeact</code> supports usage of any LLM from any provider as code action model via LiteLLM.</p> <p>Sponsored by</p> <p></p>"},{"location":"#motivation","title":"Motivation","text":"<p>Most LLMs today excel at understanding and generating code.  It is therefore a natural choice to provide agents with tool specifications described in plain Python source code. This is often source code of modules that provide the interfaces or facades of larger packages, rather than implementation details that aren't relevant for tool usage. This code-based approach enables <code>freeact</code> agents to go beyond simple function calling.  By formulating actions as code, they can instantiate classes that are part of tool definitions, use their methods for stateful processing, or act on complex result types that not only provide data but also expose behavior via methods on them. </p> <p>Because tool interfaces and code actions share the same programming language, tools can be natively included and composed into code actions.  Another advantage of this approach is that code actions generated at one step can be reused as tools in later steps. This allows <code>freeact</code> agents to learn from past experiences and compose more complex actions from simpler ones. We prefer using the term skills instead of tools throughout our documentation, to convey their greater generality.</p> <ol> <li> <p>This approach became popular with the Voyager paper where it was applied to Minecraft playing agents.\u00a0\u21a9</p> </li> <li> <p>Code actions can significantly outperform JSON-based approaches, showing up to 20% higher success rates as shown in the CodeAct paper.\u00a0\u21a9</p> </li> </ol>"},{"location":"environment/","title":"Execution environment","text":"<p><code>freeact</code> uses <code>ipybox</code> as sandboxed code execution environment, a solution based on IPython and Docker. There are several options for providing dependencies needed by code actions in <code>ipybox</code>:</p> <ul> <li>Use a prebuilt Docker image.</li> <li>Build a custom Docker image.</li> <li>Install dependencies at runtime</li> </ul>"},{"location":"environment/#prebuilt-docker-images","title":"Prebuilt Docker images","text":"<p><code>freeact</code> provides prebuilt Docker images in variants <code>minimal</code>, <code>basic</code>, and <code>example</code>. They have the following dependencies pre-installed:</p> <ul> <li> <p><code>ghcr.io/gradion-ai/ipybox:minimal</code>: </p> docker/dependencies-minimal.txt<pre><code>freeact-skills = {version = \"0.0.8\", extras = [\"search-google\", \"search-perplexity\"]}\n</code></pre> </li> <li> <p><code>ghcr.io/gradion-ai/ipybox:basic</code>: </p> docker/dependencies-basic.txt<pre><code>freeact-skills = {version = \"0.0.8\", extras = [\"search-google\", \"search-perplexity\"]}\nmatplotlib = \"^3.10\"\nnumpy = \"^2.2\"\npandas = \"^2.2\"\nsympy = \"^1.13\"\n</code></pre> </li> <li> <p><code>ghcr.io/gradion-ai/ipybox:example</code>: </p> docker/dependencies-example.txt<pre><code>freeact-skills = {version = \"0.0.8\", extras = [\"all\"]}\nscikit-learn = \"^1.6\"\nmatplotlib = \"^3.10\"\nnumpy = \"^2.2\"\npandas = \"^2.2\"\npygithub = \"^2.5\"\nsympy = \"^1.13\"\n</code></pre> </li> </ul> <p>Note</p> <p>The <code>freeact-skills</code> package provides predefined example skills for the <code>freeact</code> agent library.</p> <p>Note</p> <p>Prebuilt <code>ipybox</code> images run with root privileges. For non-root execution, build a custom Docker image (see also <code>ipybox</code>'s installation guide).</p>"},{"location":"environment/#custom-docker-image","title":"Custom Docker image","text":"<p>To build a custom <code>ipybox</code> image, create a <code>dependencies.txt</code> file with your custom dependencies. For example:</p> dependencies.txt<pre><code>freeact-skills = {version = \"0.0.8\", extras = [\"search-google\"]}\nnumpy = \"^2.2\"\n# ...\n</code></pre> <p>Note</p> <p><code>dependencies.txt</code> must follow the Poetry dependency specification format.</p> <p>Then build a custom Docker image with <code>ipybox</code>'s <code>build</code> command:</p> <pre><code>python -m ipybox build -t ghcr.io/gradion-ai/ipybox:custom -d dependencies.txt\n</code></pre> <p>To use the image, reference it in <code>CodeExecutionContainer</code> with <code>tag=\"ghcr.io/gradion-ai/ipybox:custom\"</code> or in <code>execution_environment</code> with <code>ipybox_tag=\"ghcr.io/gradion-ai/ipybox:custom\"</code>:</p> <pre><code>from freeact import execution_environment\n\nasync with execution_environment(ipybox_tag=\"ghcr.io/gradion-ai/ipybox:custom\") as env:\n    ...\n</code></pre>"},{"location":"environment/#installing-dependencies-at-runtime","title":"Installing dependencies at runtime","text":"<p>Dependencies can also be installed at runtime with <code>!pip install &lt;package&gt;</code> using a code executor. When agents require additional Python packages for executing their code actions, they usually install them on demand, either based on prior knowledge or by reacting on code execution errors when an <code>import</code> failed. Alternatively, application code may also install packages at runtime prior to running an agent:</p> <pre><code>from freeact import execution_environment\n\nasync with execution_environment(ipybox_tag=\"ghcr.io/gradion-ai/ipybox:basic\") as env:\n    async with env.code_executor() as executor:\n        # Install the serpapi package prior to running an agent\n        await executor.execute(\"!pip install serpapi\")\n\n    async with env.code_provider() as provider:\n        # Load skill modules that depend on serpapi\n        skill_sources = await provider.get_sources(\n            module_names=[\"my_skill_module_1\", \"my_skill_module_2\"],\n        )\n\n    async with env.code_executor() as executor:\n        # Initialize and run agent\n        # ...\n</code></pre> <p>Tip</p> <p>For production use, it's recommended to include frequently used dependencies in a custom Docker image.</p>"},{"location":"fundamentals/","title":"Fundamentals","text":""},{"location":"fundamentals/#application-setup","title":"Application setup","text":"<p>In a <code>freeact</code> application you need:</p> <ul> <li> <p>A code execution container. This is an <code>ipybox</code> Docker container running a resource server for providing skill sources and a Jupyter Kernel Gateway for stateful code execution in IPython kernels. <code>ipybox</code> containers are managed by the <code>CodeExecutionContainer</code> context manager. The container in the following example is based on the prebuilt <code>ghcr.io/gradion-ai/ipybox:basic</code> Docker image.</p> </li> <li> <p>A code provider for downloading skill sources from the resource server and registering MCP servers. During registration, the resource server automatically generates Python client functions from MCP tool metadata so that the generated sources can be served to clients. A code provider is created with the <code>CodeProvider</code> context manager. It communicates with the container's resource server at <code>resource_port</code>.</p> </li> <li> <p>A code executor for executing code actions generated by a code action model. A code executor is created with the <code>CodeExecutor</code> context manager. On entering the context, it connects to the container's kernel gateway at <code>executor_port</code> and creates a new IPython kernel. Code executions made with the same <code>CodeExecutor</code> instance are stateful i.e. executed code can access definitions and results from previous executions.</p> </li> <li> <p>A code action model for generating code actions in response to user queries and code execution results or errors. Code action models are created with the <code>LiteCodeActModel</code> class which supports any model that is also supported by LiteLLM, including locally deployed models. Skill sources loaded with the code provider are passed to the model's constructor.</p> </li> <li> <p>A code action agent for coordinating the interaction between a code action model, a code executor and the user or another agent. <code>freeact</code> agents are created with the <code>CodeActAgent</code> class. Depending on the complexity of a user query, the agent generates and executes a sequence of one or more code actions until the model provides a final response to the user<sup>1</sup>. </p> </li> </ul> <p>This is demonstrated in the following example. You need an Anthropic and a Gemini API key for running it.</p> examples/fundamentals_1.py<pre><code>import asyncio\nimport os\n\nfrom examples.utils import stream_conversation\nfrom freeact import (\n    CodeActAgent,\n    CodeExecutionContainer,\n    CodeExecutor,\n    CodeProvider,\n    LiteCodeActModel,\n)\n\n\nasync def main():\n    async with CodeExecutionContainer(\n        tag=\"ghcr.io/gradion-ai/ipybox:basic\",\n        env={\"GEMINI_API_KEY\": os.environ[\"GEMINI_API_KEY\"]},  # (1)!\n    ) as container:\n        async with CodeProvider(\n            workspace=container.workspace,\n            port=container.resource_port,\n        ) as provider:\n            skill_sources = await provider.get_sources(\n                module_names=[\"freeact_skills.search.google.stream.api\"],  # (2)!\n            )\n\n        model = LiteCodeActModel(\n            model_name=\"anthropic/claude-3-7-sonnet-20250219\",\n            reasoning_effort=\"low\",\n            skill_sources=skill_sources,\n            api_key=os.environ[\"ANTHROPIC_API_KEY\"],  # (3)!\n        )\n\n        async with CodeExecutor(\n            workspace=container.workspace,\n            port=container.executor_port,\n        ) as executor:\n            agent = CodeActAgent(model=model, executor=executor)\n            await stream_conversation(agent)  # (4)!\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <ol> <li> <p>A <code>GEMINI_API_KEY</code> is needed for generative Google search with Gemini in the code execution container.</p> </li> <li> <p>A module that provides generative Google search with Gemini. It is pre-installed in the code execution container.</p> </li> <li> <p>Needed for the code actions model. Added here for clarity but can be omitted if the <code>ANTHROPIC_API_KEY</code> is set as environment variable.</p> </li> <li> <p>Runs a minimalistic text-based interface for interacting with the agent.</p> </li> </ol> <p>Info</p> <p>Using a code provider is optional if you don't want to provide skill sources to a code action model. At the moment, it is the responsibility of an application to provide skill sources to a code action model. We will soon enhance <code>freeact</code> agents to retrieve skill sources autonomously depending on the user query and current state.</p> <p>MCP Integration</p> <p>For examples how to use MCP servers in code actions, see sections Quickstart and MCP integration.</p>"},{"location":"fundamentals/#higher-level-api","title":"Higher-level API","text":"<p>Using the lower-level API, as in the previous section, comes with some boilerplate code for constructing <code>CodeExecutionContainer</code>, <code>CodeProvider</code> and <code>CodeExecutor</code> instances. You can avoid this using the higher-level <code>execution_environment</code> context manager which creates a <code>CodeExecutionEnvironment</code> instance that provides more convenient context managers for code provider and code executor. The following code is equivalent to the previous one:</p> examples/fundamentals_2.py<pre><code>import asyncio\nimport os\n\nfrom examples.utils import stream_conversation\nfrom freeact import CodeActAgent, LiteCodeActModel, execution_environment\n\n\nasync def main():\n    async with execution_environment(\n        ipybox_tag=\"ghcr.io/gradion-ai/ipybox:basic\",\n        ipybox_env={\"GEMINI_API_KEY\": os.environ[\"GEMINI_API_KEY\"]},\n    ) as env:\n        async with env.code_provider() as provider:\n            skill_sources = await provider.get_sources(\n                module_names=[\"freeact_skills.search.google.stream.api\"],\n            )\n        async with env.code_executor() as executor:\n            model = LiteCodeActModel(\n                model_name=\"anthropic/claude-3-7-sonnet-20250219\",\n                reasoning_effort=\"low\",\n                skill_sources=skill_sources,\n                api_key=os.environ[\"ANTHROPIC_API_KEY\"],\n            )\n            agent = CodeActAgent(model=model, executor=executor)\n            await stream_conversation(agent)  # (1)!\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <ol> <li>Runs a minimalistic text-based interface for interacting with the agent.</li> </ol>"},{"location":"fundamentals/#agent-protocol","title":"Agent protocol","text":"<p><code>freeact</code> provides a wide spectrum of options for interacting with agents, from getting just the final response to streaming model and code execution outputs as they are generated at all steps. </p>"},{"location":"fundamentals/#final-response","title":"Final response","text":"<p>For obtaining only the final response, await <code>response</code> on a <code>CodeActAgentTurn</code> which is returned by the <code>CodeActAgent.run</code> method. This waits until the task-specific sequence of model interactions and code executions is complete.</p> examples/utils.py::final_response<pre><code>async def final_response(agent: CodeActAgent, user_message: str) -&gt; str:\n    turn: CodeActAgentTurn = agent.run(user_message)\n    resp: CodeActAgentResponse = await turn.response()  # (1)!\n    return resp.text\n</code></pre> <ol> <li>Waits until the sequence of model interactions and code executions is complete. </li> </ol>"},{"location":"fundamentals/#progressive-streaming","title":"Progressive streaming","text":"<p>For streaming model and code execution outputs as they are generated at each step, consume <code>stream</code> on <code>CodeActAgentTurn</code>, <code>CodeActModelTurn</code> and <code>CodeExecution</code> objects. After a stream has been fully consumed at a given step, the corresponding aggregated <code>CodeActAgentResponse</code>, <code>CodeActModelResponse</code> or <code>CodeExecutionResult</code> objects are available immediately without waiting, as demonstrated in the following example:</p> examples/utils.py::stream_conversation<pre><code>async def stream_conversation(agent: CodeActAgent, **kwargs):\n    usage = CodeActModelUsage()\n\n    while True:\n        user_message = await ainput(\"User message: \")\n\n        if user_message.lower() == \"q\":\n            break\n\n        agent_turn = agent.run(user_message, **kwargs)\n        await stream_turn(agent_turn)\n\n        agent_response = await agent_turn.response()\n        usage.update(agent_response.usage)  # (4)!\n\n        print(\"Accumulated usage:\")\n        print(json.dumps(asdict(usage), indent=2))\n        print()\n\n\nasync def stream_turn(agent_turn: CodeActAgentTurn):\n    produced_images: Dict[Path, Image.Image] = {}\n\n    async for activity in agent_turn.stream():\n        match activity:\n            case CodeActModelTurn() as turn:\n                print(\"Model response:\")\n                async for s in turn.stream():\n                    print(s, end=\"\", flush=True)\n                print()\n\n                response = await turn.response()  # (1)!\n                if response.code:  # (2)!\n                    print(\"\\n```python\")\n                    print(response.code)\n                    print(\"```\\n\")\n\n            case CodeExecution() as execution:\n                print(\"Execution result:\")\n                async for s in execution.stream():\n                    print(s, end=\"\", flush=True)\n                result = await execution.result()  # (3)!\n                produced_images.update(result.images)\n                print()\n\n    if produced_images:\n        print(\"\\n\\nProduced images:\")\n    for path in produced_images.keys():\n        print(str(path))\n</code></pre> <ol> <li> <p>Returns immediately as <code>turn.stream()</code> is already consumed.</p> </li> <li> <p>The code action produced by the model. If <code>None</code>, the response is a final response.</p> </li> <li> <p>Returns immediately as <code>execution.stream()</code> is already consumed.</p> </li> <li> <p>Accumulates token usage and costs across multiple agent runs in a conversation. See also usage statistics.</p> </li> </ol>"},{"location":"fundamentals/#usage-statistics","title":"Usage statistics","text":"<p><code>CodeActModelResponse.usage</code> contains token usage and costs for a single model interaction, <code>CodeActAgentResponse.usage</code> contains accumulated token usage and costs for a single agent <code>run</code>. For accumulating token usage and costs across agent runs in a conversation, create a <code>CodeActModelUsage</code> object on application level and update it with <code>usage</code> objects from agent responses.</p> <ol> <li> <p>For trivial queries, the model may also decide to provide a final response directly, without generating a code action.\u00a0\u21a9</p> </li> </ol>"},{"location":"installation/","title":"Installation","text":"<pre><code>pip install freeact\n</code></pre>"},{"location":"mcp-integration/","title":"MCP integration","text":"<p>To leverage the vast ecosystem of MCP servers and their tools, <code>freeact</code> generates Python client functions from MCP tool metadata and provides them as skills to <code>freeact</code> agents. When <code>freeact</code> agents use these skills in their code actions, they invoke the corresponding MCP server tools. <code>stdio</code> based MCP servers are executed within the sandboxed environment while <code>streamable-http</code> or <code>sse</code> based MCP servers are expected to run elsewhere.</p> <p>MCP servers are first registered at the execution environment with <code>register_mcp_servers</code>. Registration loads the MCP tool metadata from the MCP server and generates Python client functions from it. The sources of these functions (or a subset thereof) can then be loaded with <code>get_sources</code> and provided as skill sources to code action models. This is demonstrated in the following example:</p> PythonCLI <pre><code>import asyncio\nimport os\n\nfrom rich.console import Console\n\nfrom freeact import CodeActAgent, LiteCodeActModel, execution_environment\nfrom freeact.cli.utils import stream_conversation\n\n\nasync def main():\n    async with execution_environment(\n        ipybox_tag=\"ghcr.io/gradion-ai/ipybox:basic\",\n    ) as env:\n        async with env.code_provider() as provider:\n            tool_names = await provider.register_mcp_servers(  # (1)!\n                {\n                    \"firecrawl\": {\n                        \"command\": \"npx\",\n                        \"args\": [\"-y\", \"firecrawl-mcp\"],\n                        \"env\": {\"FIRECRAWL_API_KEY\": os.getenv(\"FIRECRAWL_API_KEY\")},\n                    }\n                }\n            )\n\n            assert \"firecrawl_scrape\" in tool_names[\"firecrawl\"]\n            assert \"firecrawl_extract\" in tool_names[\"firecrawl\"]\n\n            skill_sources = await provider.get_sources(\n                mcp_tool_names={\n                    \"firecrawl\": [\"firecrawl_scrape\", \"firecrawl_extract\"],  # (2)!\n                }\n            )\n\n        async with env.code_executor() as executor:\n            model = LiteCodeActModel(\n                model_name=\"gpt-4.1\",\n                skill_sources=skill_sources,\n                api_key=os.getenv(\"OPENAI_API_KEY\"),\n            )\n            agent = CodeActAgent(model=model, executor=executor)\n            await stream_conversation(agent, console=Console())\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <ol> <li> <p>Registration generates MCP skill sources and returns the tool names of registered servers e.g.</p> <pre><code>{\n    \"firecrawl\": [\n        \"firecrawl_scrape\",\n        \"firecrawl_map\",\n        \"firecrawl_crawl\",\n        \"firecrawl_check_crawl_status\",\n        \"firecrawl_search\",\n        \"firecrawl_extract\",\n        \"firecrawl_deep_research\",\n        \"firecrawl_generate_llmstxt\"\n    ]\n}\n</code></pre> </li> <li> <p>Here, we load only a subset of MCP skill sources. For loading all skill sources, use <code>mcp_tool_names=tool_names</code> or <code>mcp_tool_names={\"firecrawl\": None}</code>.</p> </li> </ol> <p>Add MCP server data to an <code>mcp.json</code> file under key <code>mcpServers</code>. An optional <code>mcpTools</code> key supports the selection of a subset of MCP tools.</p> mcp.json<pre><code>{\n    \"mcpServers\": {\n        \"firecrawl\": {\n            \"command\": \"npx\",\n            \"args\": [\"-y\", \"firecrawl-mcp\"],\n            \"env\": {\"FIRECRAWL_API_KEY\": \"your-firecrawl-api-key\"}\n        }\n    },\n    \"mcpTools\": {\n        \"firecrawl\": [\"firecrawl_scrape\", \"firecrawl_extract\"]\n    }\n}\n</code></pre> <p>Then start the agent from the command line with:</p> <pre><code>uvx freeact \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:basic \\\n  --model-name=gpt-4.1 \\\n  --mcp-servers=mcp.json \\\n  --api-key=$OPENAI_API_KEY\n</code></pre> <p>Generated skill sources</p> <ul> <li><code>firecrawl_scrape</code></li> <li><code>firecrawl_extract</code></li> </ul> <p>Example</p> <p></p>"},{"location":"observability/","title":"Observability","text":"<p><code>freeact</code> provides observability by tracing agent activities, code executions and model calls, including token usage and accumulated costs.  We currently support Langfuse as the observability backend for storing and visualizing trace data.</p>"},{"location":"observability/#setup","title":"Setup","text":"<p>To use tracing in <code>freeact</code>, either setup a self-hosted Langfuse instance or create a Langfuse Cloud account.  Generate API credentials (secret and public keys) from your Langfuse project settings and place the keys together with the Langfuse host information in a <code>.env</code> file:</p> .env<pre><code>LANGFUSE_PUBLIC_KEY=pk-lf-...\nLANGFUSE_SECRET_KEY=sk-lf-...\nLANGFUSE_HOST=http://localhost:3000\n</code></pre>"},{"location":"observability/#agent-tracing","title":"Agent tracing","text":"PythonCLI <p>Agent tracing in <code>freeact</code> is enabled by calling <code>tracing.configure()</code> at application startup. Once configured, all agent activities are automatically exported to Langfuse.</p> <p>By default, agent activities of a multi-turn conversation are grouped into a session. For custom session boundaries or a custom <code>session_id</code> use the <code>tracing.session()</code> context manager.</p> <pre><code>import asyncio\n\nfrom rich.console import Console\n\nfrom freeact import CodeActAgent, LiteCodeActModel, execution_environment, tracing\nfrom freeact.cli.utils import stream_conversation\n\ntracing.configure()  # (1)!\n\n\nasync def main():\n    async with execution_environment(\n        ipybox_tag=\"ghcr.io/gradion-ai/ipybox:basic\",\n    ) as env:\n        async with env.code_provider() as provider:\n            skill_sources = await provider.get_sources(\n                module_names=[\"freeact_skills.search.google.stream.api\"],\n            )\n        async with env.code_executor() as executor:\n            model = LiteCodeActModel(\n                model_name=\"anthropic/claude-3-7-sonnet-20250219\",\n                reasoning_effort=\"low\",\n                skill_sources=skill_sources,\n            )\n            agent = CodeActAgent(model=model, executor=executor)\n\n            with tracing.session(session_id=\"session-123\"):  # (2)!\n                await stream_conversation(agent, console=Console())\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <ol> <li><code>tracing.configure()</code> configures an application to export agent traces to Langfuse. Accepts all Langfuse configuration options via parameters or as environment variables.</li> <li>All agent activities within this context are grouped into the session <code>session-123</code>. Use <code>None</code> to generate a random session id.</li> </ol> <p>Agent tracing in the CLI is enabled by setting the <code>--tracing</code> parameter.</p> <pre><code>uvx freeact \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:basic \\\n  --model-name=anthropic/claude-3-7-sonnet-20250219 \\\n  --reasoning-effort=low \\\n  --skill-modules=freeact_skills.search.google.stream.api \\\n  --tracing\n</code></pre> <p>Info</p> <p>A shutdown hook in <code>freeact</code> automatically flushes pending traces on application exit. For manual shutdown control, call <code>tracing.shutdown()</code> explicitly.</p> <p>Example</p> <p>This example demonstrates tracing of a multi-turn conversation starting with the query <code>What was the first spacecraft to successfully orbit Mars?</code></p> <p>The screenshots below show how the collected trace data is displayed in the Langfuse Web UI.</p> <p> Session View: Displays the session created for the conversation with all related agent interactions </p> <p> Trace View: Shows aggregated metrics, span hierarchy and execution timeline for a single agent interaction </p> <p> Trace Detail View: Shows model prompts and completions, token usage statistics and costs for a single LLM call </p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>The following example runs a conversational <code>freeact</code> agent through a terminal user interface, configured with <code>anthropic/claude-3-7-sonnet-20250219</code> as code action model and skills for generative Google search with Gemini and PubMed literature search with an MCP server. Both skills are executed in context of code actions in a sandboxed environment based on IPython and Docker. Code actions are generated by the code action model.</p> <p>The code action model learns about these skills via source code. It understands how to perform generative Google search with Gemini by inspecting the sources of the <code>freeact_skills.search.google.stream.api</code> skill module, which is pre-installed on the <code>ghcr.io/gradion-ai/ipybox:basic</code> code execution container. The source code to invoke PubMed MCP tools is automatically generated by <code>freeact</code> during registration of the MCP server.</p>"},{"location":"quickstart/#start-agent","title":"Start agent","text":"<p>Place API keys for Anthropic and Gemini in a <code>.env</code> file:</p> .env<pre><code>ANTHROPIC_API_KEY=...\nGEMINI_API_KEY=...\n</code></pre> PythonCLI <p>Install the <code>freeact</code> package:</p> <pre><code>pip install freeact\n</code></pre> <p>Then start the agent with this Python script:</p> <pre><code>import asyncio\n\nfrom rich.console import Console\n\nfrom freeact import CodeActAgent, LiteCodeActModel, execution_environment\nfrom freeact.cli.utils import stream_conversation\n\n\nasync def main():\n    async with execution_environment(\n        ipybox_tag=\"ghcr.io/gradion-ai/ipybox:basic\",\n    ) as env:\n        async with env.code_provider() as provider:\n            mcp_tool_names = await provider.register_mcp_servers(\n                {\n                    \"pubmed\": {\n                        \"command\": \"uvx\",\n                        \"args\": [\"--quiet\", \"pubmedmcp@0.1.3\"],\n                        \"env\": {\"UV_PYTHON\": \"3.12\"},\n                    }\n                }\n            )\n            skill_sources = await provider.get_sources(\n                module_names=[\"freeact_skills.search.google.stream.api\"],\n                mcp_tool_names=mcp_tool_names,\n            )\n\n        async with env.code_executor() as executor:\n            model = LiteCodeActModel(\n                model_name=\"anthropic/claude-3-7-sonnet-20250219\",\n                reasoning_effort=\"low\",\n                skill_sources=skill_sources,\n            )\n            agent = CodeActAgent(model=model, executor=executor)\n\n            # provides a terminal user interface for interacting with the agent\n            await stream_conversation(agent, console=Console())\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Add the MCP server data to an <code>mcp.json</code> file:</p> mcp.json<pre><code>{\n    \"mcpServers\": {\n        \"pubmed\": {\n            \"command\": \"uvx\",\n            \"args\": [\"--quiet\", \"pubmedmcp@0.1.3\"],\n            \"env\": {\"UV_PYTHON\": \"3.12\"}\n        }\n    }\n}\n</code></pre> <p>Then start the agent with <code>uvx</code> via the <code>freeact</code> CLI:</p> <pre><code>uvx freeact \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:basic \\\n  --model-name=anthropic/claude-3-7-sonnet-20250219 \\\n  --reasoning-effort=low \\\n  --skill-modules=freeact_skills.search.google.stream.api \\\n  --mcp-servers=mcp.json\n</code></pre> <p>Info</p> <p>An initial start of the agent may take a few minutes as it downloads the Docker image of the code execution container. Subsequent runs have much lower startup latencies.</p>"},{"location":"quickstart/#use-agent","title":"Use agent","text":"<p>Example</p> <p></p>"},{"location":"skills/","title":"Skills","text":""},{"location":"skills/#learn-by-example","title":"Learn by example","text":"<p><code>freeact</code> agents can also leverage code snippets from tutorials, user guides, and other sources as guidance how to correctly use 3rd party Python packages in code actions. These snippets are usually retrieved by <code>freeact</code> agents themselves using skills that provide specialized search functionality.</p>"},{"location":"system-prompt/","title":"System prompt","text":""},{"location":"system-prompt/#default-templates","title":"Default templates","text":"<p><code>freeact</code> uses default system prompt templates for instructing models to generate code actions:</p> <ul> <li><code>CODE_TAG_SYSTEM_TEMPLATE</code> instructs a model to include code actions directly in the response text, enclosed in <code>&lt;code-action&gt;</code> tags.</li> <li><code>TOOL_USE_SYSTEM_TEMPLATE</code> instructs a model to use an <code>execute_ipython_cell</code> tool and pass code actions as <code>code</code> argument.</li> </ul> <p>These are chosen by <code>freeact</code> based on the <code>use_executor_tool</code> argument of the <code>LiteCodeActModel</code> constructor, or the model name and provider if <code>use_executor_tool=None</code>.</p> <p>Tip</p> <p>Default templates cover only generic agent behavior. To adapt an agent to application- and domain-specific requirements, it can be useful to provide custom system templates.</p>"},{"location":"system-prompt/#custom-templates","title":"Custom templates","text":"<p>The following example uses a custom system template that extends the default system template (diff) with instructions for the agent to:</p> <ul> <li>create and share a plan before acting</li> <li>asking the user for feedback on the plan</li> <li>adjust the plan if necessary</li> <li>suggest 3 follow-up queries after the final response</li> </ul> PythonCLI examples/custom.py<pre><code>import asyncio\nimport os\n\nfrom rich.console import Console\n\nfrom examples.custom_template import TOOL_USE_SYSTEM_TEMPLATE as CUSTOM_TEMPLATE\nfrom freeact import CodeActAgent, LiteCodeActModel, execution_environment\nfrom freeact.cli.utils import stream_conversation\n\n\nasync def main():\n    async with execution_environment(\n        ipybox_tag=\"ghcr.io/gradion-ai/ipybox:basic\",\n        ipybox_env={\"GEMINI_API_KEY\": os.environ[\"GEMINI_API_KEY\"]},\n    ) as env:\n        async with env.code_provider() as provider:\n            skill_sources = await provider.get_sources(\n                module_names=[\"freeact_skills.search.google.stream.api\"],\n            )\n        async with env.code_executor() as executor:\n            model = LiteCodeActModel(\n                model_name=\"anthropic/claude-3-7-sonnet-20250219\",\n                reasoning_effort=\"low\",\n                skill_sources=skill_sources,\n                system_template=CUSTOM_TEMPLATE,  # (1)!\n                use_executor_tool=True,  # (2)!\n                api_key=os.environ[\"ANTHROPIC_API_KEY\"],\n            )\n            agent = CodeActAgent(model=model, executor=executor)\n            await stream_conversation(agent, console=Console())\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <ol> <li> <p>Configures the code action model with the custom system template.</p> </li> <li> <p>This is the default for <code>anthropic/claude-3-7-sonnet-20250219</code> but added here for clarity.</p> </li> </ol> <pre><code>curl -o custom_template.txt https://raw.githubusercontent.com/gradion-ai/freeact/refs/heads/main/examples/template.txt\n</code></pre> <pre><code>uvx freeact \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:example \\\n  --model-name=anthropic/claude-3-7-sonnet-20250219 \\\n  --reasoning-effort=low \\\n  --system-template=custom_template.txt \\\n  --api-key=$ANTHROPIC_API_KEY\n</code></pre> <p>Example</p> <p></p>"},{"location":"api/agent/","title":"Agent","text":""},{"location":"api/agent/#freeact.agent.MaxStepsReached","title":"MaxStepsReached","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the maximum number of steps per agent <code>run</code> is reached.</p>"},{"location":"api/agent/#freeact.agent.CodeActAgentResponse","title":"CodeActAgentResponse  <code>dataclass</code>","text":"<pre><code>CodeActAgentResponse(text: str, usage: CodeActModelUsage)\n</code></pre> <p>A response from an single interaction with a code action agent.</p>"},{"location":"api/agent/#freeact.agent.CodeActAgentResponse.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre> <p>The final response text to the user.</p>"},{"location":"api/agent/#freeact.agent.CodeActAgentResponse.usage","title":"usage  <code>instance-attribute</code>","text":"<pre><code>usage: CodeActModelUsage\n</code></pre> <p>Accumulated model usage during this interaction.</p>"},{"location":"api/agent/#freeact.agent.CodeActAgentTurn","title":"CodeActAgentTurn","text":"<pre><code>CodeActAgentTurn(iter: AsyncIterator[CodeActModelTurn | CodeExecution | CodeActAgentResponse], trace_name: str, trace_input: dict[str, Any], trace_session_id: str | None = None)\n</code></pre> <p>A single interaction with the code action agent.</p> <p>An interaction consists of a sequence of model interaction and code execution pairs, continuing until the code action model provides a final response or the maximum number of steps is reached.</p> Source code in <code>freeact/agent.py</code> <pre><code>def __init__(\n    self,\n    iter: AsyncIterator[CodeActModelTurn | CodeExecution | CodeActAgentResponse],\n    trace_name: str,\n    trace_input: dict[str, Any],\n    trace_session_id: str | None = None,\n):\n    self._iter = iter\n    self._response: CodeActAgentResponse | None = None\n    self._trace_name = trace_name\n    self._trace_input = trace_input\n    self._trace_session_id = trace_session_id\n</code></pre>"},{"location":"api/agent/#freeact.agent.CodeActAgentTurn.response","title":"response  <code>async</code>","text":"<pre><code>response() -&gt; CodeActAgentResponse\n</code></pre> <p>Retrieves the final response from the code action agent for this interaction. Waits until the sequence of model interactions and code executions is complete.</p> <p>Returns:</p> Type Description <code>CodeActAgentResponse</code> <p>The final response from the code action model as <code>CodeActAgentResponse</code> object.</p> <p>Raises:</p> Type Description <code>MaxStepsReached</code> <p>If the interaction exceeds the maximum number of steps without completion.</p> Source code in <code>freeact/agent.py</code> <pre><code>async def response(self) -&gt; CodeActAgentResponse:\n    \"\"\"Retrieves the final response from the code action agent for this\n    interaction. Waits until the sequence of model interactions and code\n    executions is complete.\n\n    Returns:\n        The final response from the code action model as `CodeActAgentResponse`\n            object.\n\n    Raises:\n        MaxStepsReached: If the interaction exceeds the maximum number of\n            steps without completion.\n    \"\"\"\n    if self._response is None:\n        async for _ in self.stream():\n            pass\n    return self._response  # type: ignore\n</code></pre>"},{"location":"api/agent/#freeact.agent.CodeActAgentTurn.stream","title":"stream  <code>async</code>","text":"<pre><code>stream() -&gt; AsyncIterator[CodeActModelTurn | CodeExecution]\n</code></pre> <p>Streams the sequence of model interaction and code execution pairs as they occur:</p> <ul> <li><code>CodeActModelTurn</code>: The current interaction with the code action model</li> <li><code>CodeExecution</code>: The current execution of a code action in the code   execution environment</li> </ul> <p>The sequence continues until the model provides a final response. Once the stream is consumed, <code>response</code> is immediately available without waiting and contains the final response text and accumulated usage statistics.</p> <p>Raises:</p> Type Description <code>MaxStepsReached</code> <p>If the interaction exceeds the maximum number of steps without completion.</p> Source code in <code>freeact/agent.py</code> <pre><code>async def stream(self) -&gt; AsyncIterator[CodeActModelTurn | CodeExecution]:\n    \"\"\"Streams the sequence of model interaction and code execution pairs\n    as they occur:\n\n    - `CodeActModelTurn`: The current interaction with the code action model\n    - `CodeExecution`: The current execution of a code action in the code\n      execution environment\n\n    The sequence continues until the model provides a final response. Once\n    the stream is consumed, [`response`][freeact.agent.CodeActAgentTurn.response]\n    is immediately available without waiting and contains the final response\n    text and accumulated usage statistics.\n\n    Raises:\n        MaxStepsReached: If the interaction exceeds the maximum number of\n            steps without completion.\n    \"\"\"\n    async with tracing.trace(\n        name=self._trace_name,\n        input=self._trace_input,\n        session_id=self._trace_session_id,\n    ) as trace:\n        async for elem in self._iter:\n            match elem:\n                case CodeActAgentResponse() as response:\n                    self._response = response\n                    await trace.update(output=response.text)\n                case _:\n                    yield elem\n</code></pre>"},{"location":"api/agent/#freeact.agent.CodeActAgent","title":"CodeActAgent","text":"<pre><code>CodeActAgent(model: CodeActModel, executor: CodeExecutor)\n</code></pre> <p>An agent that iteratively generates and executes code actions to process user queries.</p> <p>The agent implements a loop that:</p> <ol> <li>Generates code actions using a <code>CodeActModel</code></li> <li>Executes the code using a <code>CodeExecutor</code></li> <li>Provides execution feedback to the <code>CodeActModel</code></li> <li>Continues until the model generates a final response.</li> </ol> <p>A single interaction with the agent is initiated with its <code>run</code> method. The agent maintains conversational state and can have multiple interactions with the user.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>CodeActModel</code> <p>Model instance for generating code actions</p> required <code>executor</code> <code>CodeExecutor</code> <p>Executor instance for executing code actions</p> required Source code in <code>freeact/agent.py</code> <pre><code>def __init__(self, model: CodeActModel, executor: CodeExecutor):\n    self.model = model\n    self.executor = executor\n    self._trace_session_id = tracing.create_session_id()\n</code></pre>"},{"location":"api/agent/#freeact.agent.CodeActAgent.run","title":"run","text":"<pre><code>run(user_query: str, max_steps: int = 30, step_timeout: float = 120, **kwargs) -&gt; CodeActAgentTurn\n</code></pre> <p>Initiates an interaction with the agent from a user query. The query is processed through a sequence of model interaction and code execution steps, driven by interacting with the returned <code>CodeActAgentTurn</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>user_query</code> <code>str</code> <p>The user query (a question, instruction, etc.)</p> required <code>max_steps</code> <code>int</code> <p>Maximum number of steps before raising <code>MaxStepsReached</code></p> <code>30</code> <code>step_timeout</code> <code>float</code> <p>Timeout in seconds per code execution step</p> <code>120</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the model</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>CodeActAgentTurn</code> <code>CodeActAgentTurn</code> <p>An object for retrieving the agent's processing steps and response.</p> <p>Raises:</p> Type Description <code>MaxStepsReached</code> <p>If the interaction exceeds <code>max_steps</code> without completion.</p> Source code in <code>freeact/agent.py</code> <pre><code>def run(\n    self,\n    user_query: str,\n    max_steps: int = 30,\n    step_timeout: float = 120,\n    **kwargs,\n) -&gt; CodeActAgentTurn:\n    \"\"\"Initiates an interaction with the agent from a user query. The query\n    is processed through a sequence of model interaction and code execution\n    steps, driven by interacting with the returned `CodeActAgentTurn` object.\n\n    Args:\n        user_query: The user query (a question, instruction, etc.)\n        max_steps: Maximum number of steps before raising `MaxStepsReached`\n        step_timeout: Timeout in seconds per code execution step\n        **kwargs: Additional keyword arguments passed to the model\n\n    Returns:\n        CodeActAgentTurn: An object for retrieving the agent's processing steps\n            and response.\n\n    Raises:\n        MaxStepsReached: If the interaction exceeds `max_steps` without completion.\n    \"\"\"\n\n    trace_name = \"Agent run\"\n    trace_input = {\n        \"user_query\": user_query,\n        \"max_steps\": max_steps,\n        \"step_timeout\": step_timeout,\n        **kwargs,\n    }\n    iter = self._stream(\n        user_query=user_query,\n        max_steps=max_steps,\n        step_timeout=step_timeout,\n        **kwargs,\n    )\n    return CodeActAgentTurn(iter, trace_name, trace_input, self._trace_session_id)\n</code></pre>"},{"location":"api/environment/","title":"Environment","text":""},{"location":"api/environment/#freeact.environment.Workspace","title":"Workspace","text":"<pre><code>Workspace(path: Path | str | None = None, key: str | None = None)\n</code></pre> <p>A workspace for private and shared agent skills i.e. Python modules that implement special agent skills. These are skills that are not pre-installed in the code execution container.</p> <p>A workspace defines paths for private and shared skills, both in the container and on the host machine. Workspace paths on the host machine can be bind-mounted into the container, if desired. This is especially useful when skills are being (inter)actively developed, so that they can be inspected and edited on the host machine while being executed in the container.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str | None</code> <p>Root path of the workspace directory on the host.</p> <code>None</code> <code>key</code> <code>str | None</code> <p>A key to designate:</p> <ul> <li>a private skill sub-directory on the host</li> <li>a private image sub-directory on the host</li> </ul> <code>None</code> Source code in <code>freeact/environment.py</code> <pre><code>def __init__(self, path: Path | str | None = None, key: str | None = None):\n    self._path = Path(path) if path else Path(\"workspace\")\n    self._key = key or \"default\"\n</code></pre>"},{"location":"api/environment/#freeact.environment.Workspace.skills_host_path","title":"skills_host_path  <code>property</code>","text":"<pre><code>skills_host_path: Path\n</code></pre> <p>Path to skills root directory on host.</p>"},{"location":"api/environment/#freeact.environment.Workspace.private_skills_host_path","title":"private_skills_host_path  <code>property</code>","text":"<pre><code>private_skills_host_path: Path\n</code></pre> <p>Path to private skills directory on host.</p>"},{"location":"api/environment/#freeact.environment.Workspace.shared_skills_host_path","title":"shared_skills_host_path  <code>property</code>","text":"<pre><code>shared_skills_host_path: Path\n</code></pre> <p>Path to shared skills directory on host.</p>"},{"location":"api/environment/#freeact.environment.Workspace.private_images_host_path","title":"private_images_host_path  <code>property</code>","text":"<pre><code>private_images_host_path: Path\n</code></pre> <p>Path to private images directory on host.</p>"},{"location":"api/environment/#freeact.environment.Workspace.private_skills_container_path","title":"private_skills_container_path  <code>property</code>","text":"<pre><code>private_skills_container_path: str\n</code></pre> <p>Path to private skills directory in container.</p>"},{"location":"api/environment/#freeact.environment.Workspace.shared_skills_container_path","title":"shared_skills_container_path  <code>property</code>","text":"<pre><code>shared_skills_container_path: str\n</code></pre> <p>Path to shared skills directory in container.</p>"},{"location":"api/environment/#freeact.environment.Workspace.private_mcp_container_path","title":"private_mcp_container_path  <code>property</code>","text":"<pre><code>private_mcp_container_path: str\n</code></pre> <p>Path to private MCP directory in container.</p>"},{"location":"api/environment/#freeact.environment.CodeExecutionContainer","title":"CodeExecutionContainer","text":"<pre><code>CodeExecutionContainer(tag: str, env: dict[str, str] | None = None, executor_port: int | None = None, resource_port: int | None = None, show_pull_progress: bool = True, workspace_path: Path | str | None = None, workspace_key: str | None = None)\n</code></pre> <p>               Bases: <code>ExecutionContainer</code></p> <p>Context manager for the lifecycle of a code execution Docker container.</p> <p>Extends ipybox's <code>ExecutionContainer</code> with workspace-specific bind mounts of skill directories.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>Name and optionally tag of the <code>ipybox</code> Docker image to use (format: <code>name:tag</code>)</p> required <code>env</code> <code>dict[str, str] | None</code> <p>Environment variables to set in the container</p> <code>None</code> <code>executor_port</code> <code>int | None</code> <p>Host port for the container's executor port. A random port is allocated if not specified.</p> <code>None</code> <code>resource_port</code> <code>int | None</code> <p>Host port for the container's resource port. A random port is allocated if not specified.</p> <code>None</code> <code>show_pull_progress</code> <code>bool</code> <p>Whether to show progress when pulling the Docker image.</p> <code>True</code> <code>workspace_path</code> <code>Path | str | None</code> <p>Path to workspace directory on host. Defaults to \"workspace\".</p> <code>None</code> <code>workspace_key</code> <code>str | None</code> <p>Key to designate private sub-directories on host. Defaults to \"default\".</p> <code>None</code> Source code in <code>freeact/environment.py</code> <pre><code>def __init__(\n    self,\n    tag: str,\n    env: dict[str, str] | None = None,\n    executor_port: int | None = None,\n    resource_port: int | None = None,\n    show_pull_progress: bool = True,\n    workspace_path: Path | str | None = None,\n    workspace_key: str | None = None,\n):\n    self._workspace = Workspace(workspace_path, workspace_key)\n\n    binds = {\n        self._workspace.private_skills_host_path: self._workspace.private_skills_container_path,\n        self._workspace.shared_skills_host_path: self._workspace.shared_skills_container_path,\n    }\n\n    env = (env or {}) | {\n        \"PYTHONPATH\": f\".:/app/{self._workspace.shared_skills_container_path}:/app/{self._workspace.private_skills_container_path}\",\n    }\n\n    super().__init__(\n        binds=binds,\n        tag=tag,\n        env=env,\n        executor_port=executor_port,\n        resource_port=resource_port,\n        show_pull_progress=show_pull_progress,\n    )\n</code></pre>"},{"location":"api/environment/#freeact.environment.CodeExecutionContainer.workspace","title":"workspace  <code>property</code>","text":"<pre><code>workspace: Workspace\n</code></pre> <p>The container's workspace.</p>"},{"location":"api/environment/#freeact.environment.CodeExecutionResult","title":"CodeExecutionResult  <code>dataclass</code>","text":"<pre><code>CodeExecutionResult(text: str, images: Dict[Path, Image], is_error: bool)\n</code></pre> <p>Result of a code execution in a code executor.</p>"},{"location":"api/environment/#freeact.environment.CodeExecutionResult.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre> <p>Execution output text or error trace.</p>"},{"location":"api/environment/#freeact.environment.CodeExecutionResult.images","title":"images  <code>instance-attribute</code>","text":"<pre><code>images: Dict[Path, Image]\n</code></pre> <p>Images generated during code execution. Keys are image file paths in the <code>container.workspace</code>, values are pre-loaded images from these files.</p>"},{"location":"api/environment/#freeact.environment.CodeExecutionResult.is_error","title":"is_error  <code>instance-attribute</code>","text":"<pre><code>is_error: bool\n</code></pre> <p>Whether the execution resulted in an error. If <code>True</code>, <code>text</code> contains the corresponding error trace.</p>"},{"location":"api/environment/#freeact.environment.CodeExecution","title":"CodeExecution","text":"<pre><code>CodeExecution(execution: Execution, images_dir: Path)\n</code></pre> <p>A code execution running in a code executor.</p> Source code in <code>freeact/environment.py</code> <pre><code>def __init__(self, execution: Execution, images_dir: Path):\n    self.execution = execution\n    self.images_dir = images_dir\n    self._result: CodeExecutionResult | None = None\n</code></pre>"},{"location":"api/environment/#freeact.environment.CodeExecution.result","title":"result  <code>async</code>","text":"<pre><code>result(timeout: float = 120) -&gt; CodeExecutionResult\n</code></pre> <p>Retrieves the complete result of this code execution. Waits until the result is available.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Maximum time in seconds to wait for the execution result</p> <code>120</code> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If code execution duration exceeds the specified timeout</p> Source code in <code>freeact/environment.py</code> <pre><code>async def result(self, timeout: float = 120) -&gt; CodeExecutionResult:\n    \"\"\"Retrieves the complete result of this code execution. Waits until the\n    result is available.\n\n    Args:\n        timeout: Maximum time in seconds to wait for the execution result\n\n    Raises:\n        asyncio.TimeoutError: If code execution duration exceeds the specified timeout\n    \"\"\"\n    if self._result is None:\n        async for _ in self.stream(timeout=timeout):\n            pass\n    return self._result  # type: ignore\n</code></pre>"},{"location":"api/environment/#freeact.environment.CodeExecution.stream","title":"stream  <code>async</code>","text":"<pre><code>stream(timeout: float = 120) -&gt; AsyncIterator[str]\n</code></pre> <p>Streams the code execution result as it is generated. Once the stream is consumed, a <code>result</code> is immediately available without waiting.</p> <p>Generated images are not streamed. They can be obtained from the return value of <code>result</code>.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Maximum time in seconds to wait for the complete execution result</p> <code>120</code> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If code execution duration exceeds the specified timeout</p> Source code in <code>freeact/environment.py</code> <pre><code>async def stream(self, timeout: float = 120) -&gt; AsyncIterator[str]:\n    \"\"\"Streams the code execution result as it is generated. Once the stream\n    is consumed, a [`result`][freeact.environment.CodeExecution.result] is\n    immediately available without waiting.\n\n    Generated images are not streamed. They can be obtained from the\n    return value of [`result`][freeact.environment.CodeExecution.result].\n\n    Args:\n        timeout: Maximum time in seconds to wait for the complete execution result\n\n    Raises:\n        asyncio.TimeoutError: If code execution duration exceeds the specified timeout\n    \"\"\"\n    images = {}\n\n    try:\n        async for chunk in self.execution.stream(timeout=timeout):\n            yield chunk\n    except ExecutionError as e:\n        is_error = True\n        text = e.trace\n        yield text\n    except asyncio.TimeoutError:\n        is_error = True\n        text = \"Execution timed out\"\n        yield text\n    else:\n        result = await self.execution.result()\n        text = result.text\n        is_error = False\n\n        if result.images:\n            chunk = \"\\n\\nProduced images:\"\n            yield chunk\n            text += chunk\n\n        for i, image in enumerate(result.images):\n            path = await self._save_image(image)\n            chunk = f\"\\n![image_{i}]({path})\"\n            yield chunk\n            text += chunk\n            images[path] = image\n\n    self._result = CodeExecutionResult(text=text, images=images, is_error=is_error)\n</code></pre>"},{"location":"api/environment/#freeact.environment.CodeExecutor","title":"CodeExecutor","text":"<pre><code>CodeExecutor(workspace: Workspace, port: int, host: str = 'localhost')\n</code></pre> <p>Context manager for executing code in an IPython kernel running in a <code>CodeExecutionContainer</code>. The kernel is created on entering the context and destroyed on exit.</p> <p>Code execution is stateful for a given <code>CodeExecutor</code> instance. Definitions and variables of previous executions are available to subsequent executions.</p> <p>Parameters:</p> Name Type Description Default <code>workspace</code> <code>Workspace</code> <p>The workspace of the code execution container</p> required <code>port</code> <code>int</code> <p>Host port for the container's executor port</p> required <code>host</code> <code>str</code> <p>Hostname or IP address of the container's host</p> <code>'localhost'</code> Source code in <code>freeact/environment.py</code> <pre><code>def __init__(self, workspace: Workspace, port: int, host: str = \"localhost\"):\n    self.workspace = workspace\n    self.workspace.private_images_host_path.mkdir(parents=True, exist_ok=True)\n\n    self._client = ExecutionClient(port=port, host=host)\n</code></pre>"},{"location":"api/environment/#freeact.environment.CodeExecutor.execute","title":"execute  <code>async</code>","text":"<pre><code>execute(code: str, timeout: float = 120) -&gt; CodeExecutionResult\n</code></pre> <p>Executes code in this executor's IPython kernel and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>Code to execute</p> required <code>timeout</code> <code>float</code> <p>Maximum time in seconds to wait for the execution result</p> <code>120</code> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If code execution duration exceeds the specified timeout</p> Source code in <code>freeact/environment.py</code> <pre><code>async def execute(self, code: str, timeout: float = 120) -&gt; CodeExecutionResult:\n    \"\"\"Executes code in this executor's IPython kernel and returns the result.\n\n    Args:\n        code: Code to execute\n        timeout: Maximum time in seconds to wait for the execution result\n\n    Raises:\n        asyncio.TimeoutError: If code execution duration exceeds the specified timeout\n    \"\"\"\n    code_exec = await self.submit(code)\n    return await code_exec.result(timeout=timeout)\n</code></pre>"},{"location":"api/environment/#freeact.environment.CodeExecutor.submit","title":"submit  <code>async</code>","text":"<pre><code>submit(code: str) -&gt; CodeExecution\n</code></pre> <p>Submits code for execution in this executor's IPython kernel and returns a <code>CodeExecution</code> object for consuming the execution result.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>Python code to execute</p> required <p>Returns:</p> Type Description <code>CodeExecution</code> <p>A <code>CodeExecution</code> object to track the code execution</p> Source code in <code>freeact/environment.py</code> <pre><code>async def submit(self, code: str) -&gt; CodeExecution:\n    \"\"\"Submits code for execution in this executor's IPython kernel and returns\n    a [`CodeExecution`][freeact.environment.CodeExecution] object for consuming the\n    execution result.\n\n    Args:\n        code: Python code to execute\n\n    Returns:\n        A [`CodeExecution`][freeact.environment.CodeExecution] object to track the code execution\n    \"\"\"\n    code_exec = await self._client.submit(code)\n    return CodeExecution(code_exec, self.workspace.private_images_host_path)\n</code></pre>"},{"location":"api/environment/#freeact.environment.CodeProvider","title":"CodeProvider","text":"<pre><code>CodeProvider(workspace: Workspace, port: int, host: str = 'localhost')\n</code></pre> <p>Context manager for</p> <ul> <li>loading the source code of Python modules and generated MCP client functions   from a <code>CodeExecutionContainer</code>.</li> <li>registering MCP servers and generating client functions for their tools in a   <code>CodeExecutionContainer</code>.</li> </ul> <p>Source code loaded with this context manager is provided as skill sources to code action models so that they can include them into code actions.</p> <p>Parameters:</p> Name Type Description Default <code>workspace</code> <code>Workspace</code> <p>The workspace of the code execution container</p> required <code>port</code> <code>int</code> <p>Host port for the container's resource port</p> required <code>host</code> <code>str</code> <p>Hostname or IP address of the container's host</p> <code>'localhost'</code> Source code in <code>freeact/environment.py</code> <pre><code>def __init__(self, workspace: Workspace, port: int, host: str = \"localhost\"):\n    self.workspace = workspace\n    self._client = ResourceClient(port, host)\n</code></pre>"},{"location":"api/environment/#freeact.environment.CodeProvider.register_mcp_servers","title":"register_mcp_servers  <code>async</code>","text":"<pre><code>register_mcp_servers(server_params_dict: dict[str, dict[str, Any]]) -&gt; dict[str, list[str]]\n</code></pre> <p>Registers MCP servers and generates Python client functions for their tools. These functions can be included into code actions, and calling them runs the corresponding MCP server tools. This works for both <code>stdio</code> and <code>sse</code> based MCP servers.</p> <p>The source code of generated client functions can be loaded with the <code>get_sources</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>server_params_dict</code> <code>dict[str, dict[str, Any]]</code> <p>Dictionary of application-defined server names and their MCP server parameters. <code>stdio</code> server parameters must specify at least a <code>command</code> key, <code>sse</code> server parameters must specify a <code>url</code> key. Application-defined server names must be valid Python module names.</p> required <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>Dictionary of server names and provided tool names. Tool names are sanitized</p> <code>dict[str, list[str]]</code> <p>to be valid Python module names.</p> Source code in <code>freeact/environment.py</code> <pre><code>async def register_mcp_servers(self, server_params_dict: dict[str, dict[str, Any]]) -&gt; dict[str, list[str]]:\n    \"\"\"Registers MCP servers and generates Python client functions for their tools. These\n    functions can be included into code actions, and calling them runs the corresponding\n    MCP server tools. This works for both `stdio` and `sse` based MCP servers.\n\n    The source code of generated client functions can be loaded with the\n    [`get_sources`][freeact.environment.CodeProvider.get_sources] method.\n\n    Args:\n        server_params_dict: Dictionary of application-defined server names and their MCP\n            server parameters. `stdio` server parameters must specify at least a `command`\n            key, `sse` server parameters must specify a `url` key. Application-defined\n            server names must be valid Python module names.\n\n    Returns:\n        Dictionary of server names and provided tool names. Tool names are sanitized\n        to be valid Python module names.\n    \"\"\"\n    result = {}\n    for server_name, server_params in server_params_dict.items():\n        tool_names = await self._client.generate_mcp_sources(\n            self.workspace.private_mcp_container_path, server_name, server_params\n        )\n        result[server_name] = tool_names\n    return result\n</code></pre>"},{"location":"api/environment/#freeact.environment.CodeProvider.get_sources","title":"get_sources  <code>async</code>","text":"<pre><code>get_sources(module_names: list[str] | None = None, mcp_tool_names: Mapping[str, list[str] | None] | None = None) -&gt; str\n</code></pre> <p>Loads the source code of given Python modules and generated MCP client functions and returns them in the following format:</p> <pre><code>```python\n# Module: {module_name_1}\n{module_source_1}\n```\n\n```python\n# Module: {module_name_2}\n{module_source_2}\n```\n\n...\n</code></pre> <p>Module names of generated MCP client functions follow the pattern <code>mcpgen.{server_name}.{tool_name}</code>. Hence, calling</p> <pre><code>await get_sources(mcp_tool_names={\"my_server\": [\"my_tool\"]})\n</code></pre> <p>is equivalent to</p> <pre><code>await get_sources(module_names=[\"mcpgen.my_server.my_tool\"])\n</code></pre> <p>For loading the source code of all generated client functions for an MCP server, use <code>None</code> as value in the <code>mcp_tool_names</code> dictionary:</p> <pre><code>await get_sources(mcp_tool_names={\"my_server\": None})\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>module_names</code> <code>list[str] | None</code> <p>Names of modules available on the container's Python path</p> <code>None</code> <code>mcp_tool_names</code> <code>Mapping[str, list[str] | None] | None</code> <p>Dictionary of MCP server names and their tool names (as returned by <code>register_mcp_servers</code>). Values can be <code>None</code> which means all tool names for a given server name.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The formatted source code of all requested Python modules and generated MCP client functions.</p> Source code in <code>freeact/environment.py</code> <pre><code>async def get_sources(\n    self,\n    module_names: list[str] | None = None,\n    mcp_tool_names: Mapping[str, list[str] | None] | None = None,\n) -&gt; str:\n    \"\"\"\n    Loads the source code of given Python modules and generated MCP client functions\n    and returns them in the following format:\n\n        ```python\n        # Module: {module_name_1}\n        {module_source_1}\n        ```\n\n        ```python\n        # Module: {module_name_2}\n        {module_source_2}\n        ```\n\n        ...\n\n    Module names of generated MCP client functions follow the pattern\n    `mcpgen.{server_name}.{tool_name}`. Hence, calling\n\n    ```python\n    await get_sources(mcp_tool_names={\"my_server\": [\"my_tool\"]})\n    ```\n\n    is equivalent to\n\n    ```python\n    await get_sources(module_names=[\"mcpgen.my_server.my_tool\"])\n    ```\n\n    For loading the source code of all generated client functions for an MCP server,\n    use `None` as value in the `mcp_tool_names` dictionary:\n\n    ```python\n    await get_sources(mcp_tool_names={\"my_server\": None})\n    ```\n\n    Args:\n        module_names: Names of modules available on the container's Python path\n        mcp_tool_names: Dictionary of MCP server names and their tool names (as returned by\n            [`register_mcp_servers`][freeact.environment.CodeProvider.register_mcp_servers]).\n            Values can be `None` which means all tool names for a given server name.\n\n    Returns:\n        The formatted source code of all requested Python modules and generated MCP client functions.\n    \"\"\"\n    mod_sources = await self._get_module_sources(module_names or [])\n    mcp_sources = await self._get_mcp_sources(mcp_tool_names or {})\n    return self._render(mod_sources | mcp_sources)\n</code></pre>"},{"location":"api/environment/#freeact.environment.CodeExecutionEnvironment","title":"CodeExecutionEnvironment","text":"<pre><code>CodeExecutionEnvironment(container: CodeExecutionContainer, host: str = 'localhost')\n</code></pre> <p>An environment for</p> <ul> <li>executing code actions in,</li> <li>loading source code from,</li> <li>and registering MCP servers at</li> </ul> <p>a running <code>CodeExecutionContainer</code>.</p> <p>Parameters:</p> Name Type Description Default <code>container</code> <code>CodeExecutionContainer</code> <p>A running code execution container.</p> required Source code in <code>freeact/environment.py</code> <pre><code>def __init__(self, container: CodeExecutionContainer, host: str = \"localhost\"):\n    self.container = container\n    self.host = host\n</code></pre>"},{"location":"api/environment/#freeact.environment.CodeExecutionEnvironment.code_executor","title":"code_executor  <code>async</code>","text":"<pre><code>code_executor() -&gt; AsyncIterator[CodeExecutor]\n</code></pre> <p>Context manager for <code>CodeExecutor</code>s in this environment.</p> Source code in <code>freeact/environment.py</code> <pre><code>@asynccontextmanager\nasync def code_executor(self) -&gt; AsyncIterator[CodeExecutor]:\n    \"\"\"Context manager for [`CodeExecutor`][freeact.environment.CodeExecutor]s in this environment.\"\"\"\n    async with CodeExecutor(\n        workspace=self.container.workspace,\n        port=self.container.executor_port,\n        host=self.host,\n    ) as executor:\n        yield executor\n</code></pre>"},{"location":"api/environment/#freeact.environment.CodeExecutionEnvironment.code_provider","title":"code_provider  <code>async</code>","text":"<pre><code>code_provider() -&gt; AsyncIterator[CodeProvider]\n</code></pre> <p>Context manager for <code>CodeProvider</code>s in this environment.</p> Source code in <code>freeact/environment.py</code> <pre><code>@asynccontextmanager\nasync def code_provider(self) -&gt; AsyncIterator[CodeProvider]:\n    \"\"\"Context manager for [`CodeProvider`][freeact.environment.CodeProvider]s in this environment.\"\"\"\n    async with CodeProvider(\n        workspace=self.container.workspace,\n        port=self.container.resource_port,\n        host=self.host,\n    ) as provider:\n        yield provider\n</code></pre>"},{"location":"api/environment/#freeact.environment.dotenv_variables","title":"dotenv_variables","text":"<pre><code>dotenv_variables(dotenv_path: Path | None = Path('.env'), export: bool = True, **kwargs) -&gt; Dict[str, str]\n</code></pre> <p>Load environment variables from a <code>.env</code> file.</p> <p>Reads environment variables from a <code>.env</code> file and optionally exports them to <code>os.environ</code>. If no path is provided, searches for a <code>.env</code> file in parent directories.</p> <p>Parameters:</p> Name Type Description Default <code>dotenv_path</code> <code>Path | None</code> <p>Path to the <code>.env</code> file. Defaults to <code>.env</code> in current directory.</p> <code>Path('.env')</code> <code>export</code> <code>bool</code> <p>Whether to export variables to current environment. Defaults to <code>True</code>.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments passed to <code>DotEnv</code> constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dictionary mapping environment variable names to their values.</p> Source code in <code>freeact/environment.py</code> <pre><code>def dotenv_variables(dotenv_path: Path | None = Path(\".env\"), export: bool = True, **kwargs) -&gt; Dict[str, str]:\n    \"\"\"Load environment variables from a `.env` file.\n\n    Reads environment variables from a `.env` file and optionally exports them to `os.environ`.\n    If no path is provided, searches for a `.env` file in parent directories.\n\n    Args:\n        dotenv_path: Path to the `.env` file. Defaults to `.env` in current directory.\n        export: Whether to export variables to current environment. Defaults to `True`.\n        **kwargs: Additional keyword arguments passed to `DotEnv` constructor.\n\n    Returns:\n        Dictionary mapping environment variable names to their values.\n    \"\"\"\n\n    if dotenv_path is None:\n        dotenv_path = find_dotenv()\n\n    dotenv = DotEnv(dotenv_path=dotenv_path, **kwargs)\n\n    if export:\n        dotenv.set_as_environment_variables()\n\n    return {k: v for k, v in dotenv.dict().items() if v is not None}\n</code></pre>"},{"location":"api/environment/#freeact.environment.execution_environment","title":"execution_environment  <code>async</code>","text":"<pre><code>execution_environment(host: str = 'localhost', ipybox_tag: str = 'ghcr.io/gradion-ai/ipybox:minimal', ipybox_env: dict[str, str] = dotenv_variables(), executor_port: int | None = None, resource_port: int | None = None, workspace_path: Path | str | None = None, workspace_key: str | None = None) -&gt; AsyncIterator[CodeExecutionEnvironment]\n</code></pre> <p>Context manager providing a <code>CodeExecutionEnvironment</code>. It manages the lifecycle of the environment's <code>CodeExecutionContainer</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ipybox_tag</code> <code>str</code> <p>Name and optionally tag of the <code>ipybox</code> Docker image to use (format: <code>name:tag</code>)</p> <code>'ghcr.io/gradion-ai/ipybox:minimal'</code> <code>ipybox_env</code> <code>dict[str, str]</code> <p>Environment variables to set in the <code>ipybox</code> Docker container</p> <code>dotenv_variables()</code> <code>executor_port</code> <code>int | None</code> <p>Host port for the container's executor port. A random port is allocated if not specified</p> <code>None</code> <code>resource_port</code> <code>int | None</code> <p>Host port for the container's resource port. A random port is allocated if not specified</p> <code>None</code> <code>workspace_path</code> <code>Path | str | None</code> <p>Path to workspace directory on host. Defaults to \"workspace\".</p> <code>None</code> <code>workspace_key</code> <code>str | None</code> <p>Key to designate private sub-directories on host. Defaults to \"default\".</p> <code>None</code> Source code in <code>freeact/environment.py</code> <pre><code>@asynccontextmanager\nasync def execution_environment(\n    host: str = \"localhost\",\n    ipybox_tag: str = \"ghcr.io/gradion-ai/ipybox:minimal\",\n    ipybox_env: dict[str, str] = dotenv_variables(),\n    executor_port: int | None = None,\n    resource_port: int | None = None,\n    workspace_path: Path | str | None = None,\n    workspace_key: str | None = None,\n) -&gt; AsyncIterator[CodeExecutionEnvironment]:\n    \"\"\"Context manager providing a [`CodeExecutionEnvironment`][freeact.environment.CodeExecutionEnvironment]. It\n    manages the lifecycle of the environment's [`CodeExecutionContainer`][freeact.environment.CodeExecutionContainer].\n\n    Args:\n        ipybox_tag: Name and optionally tag of the `ipybox` Docker image to use (format: `name:tag`)\n        ipybox_env: Environment variables to set in the `ipybox` Docker container\n        executor_port: Host port for the container's executor port. A random port is allocated if not specified\n        resource_port: Host port for the container's resource port. A random port is allocated if not specified\n        workspace_path: Path to workspace directory on host. Defaults to \"workspace\".\n        workspace_key: Key to designate private sub-directories on host. Defaults to \"default\".\n    \"\"\"\n    async with CodeExecutionContainer(\n        tag=ipybox_tag,\n        env=ipybox_env,\n        executor_port=executor_port,\n        resource_port=resource_port,\n        workspace_path=workspace_path,\n        workspace_key=workspace_key,\n    ) as container:\n        yield CodeExecutionEnvironment(container=container, host=host)\n</code></pre>"},{"location":"api/model/","title":"Model","text":""},{"location":"api/model/#freeact.model.base.CodeActModelUsage","title":"CodeActModelUsage  <code>dataclass</code>","text":"<pre><code>CodeActModelUsage(total_tokens: int = 0, input_tokens: int = 0, thinking_tokens: int = 0, output_tokens: int = 0, cache_write_tokens: int = 0, cache_read_tokens: int = 0, cost: float | None = None)\n</code></pre> <p>Tracks token usage and costs from interactions with code action models.</p>"},{"location":"api/model/#freeact.model.base.CodeActModelUsage.cost","title":"cost  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cost: float | None = None\n</code></pre> <p>Cost of code action model usage in <code>USD</code> based on token counts or <code>None</code> if cost estimation is not available for the used model.</p>"},{"location":"api/model/#freeact.model.base.CodeActModelUsage.update","title":"update","text":"<pre><code>update(other: CodeActModelUsage)\n</code></pre> <p>Adds token counts and cost of <code>other</code> to this instance. This is used to accumulate usage across multiple interactions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>CodeActModelUsage</code> <p>The usage instance to add to this instance.</p> required Source code in <code>freeact/model/base.py</code> <pre><code>def update(self, other: \"CodeActModelUsage\"):\n    \"\"\"Adds token counts and cost of `other` to this instance.\n    This is used to accumulate usage across multiple interactions.\n\n    Args:\n        other: The usage instance to add to this instance.\n    \"\"\"\n\n    self.total_tokens += other.total_tokens\n    self.input_tokens += other.input_tokens\n    self.thinking_tokens += other.thinking_tokens\n    self.output_tokens += other.output_tokens\n    self.cache_write_tokens += other.cache_write_tokens\n    self.cache_read_tokens += other.cache_read_tokens\n\n    if self.cost is None and other.cost is not None:\n        self.cost = other.cost\n    elif self.cost is not None and other.cost is not None:\n        self.cost += other.cost\n</code></pre>"},{"location":"api/model/#freeact.model.base.CodeActModelResponse","title":"CodeActModelResponse  <code>dataclass</code>","text":"<pre><code>CodeActModelResponse(text: str, is_error: bool, usage: CodeActModelUsage = CodeActModelUsage())\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>A response from a code action model. If the <code>code</code> property is <code>None</code> it is a final response to the user, otherwise it is a code action.</p>"},{"location":"api/model/#freeact.model.base.CodeActModelResponse.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre> <p>Response text generated by a code action model. Depending on the strategy to generate code actions, this may or may not include the generated code. If it contains code, it is extracted and available in the <code>code</code> property.</p>"},{"location":"api/model/#freeact.model.base.CodeActModelResponse.is_error","title":"is_error  <code>instance-attribute</code>","text":"<pre><code>is_error: bool\n</code></pre> <p>Whether the response <code>text</code> contains error information. If <code>True</code>, <code>text</code> contains error information that is NOT related to code execution errors. Not handled by applications but rather <code>freeact</code>-internally.</p>"},{"location":"api/model/#freeact.model.base.CodeActModelResponse.usage","title":"usage  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>usage: CodeActModelUsage = field(default_factory=CodeActModelUsage)\n</code></pre> <p>Token usage and costs from the interaction with a code action model.</p>"},{"location":"api/model/#freeact.model.base.CodeActModelResponse.code","title":"code  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>code: str | None\n</code></pre> <p>Executable code generated by a code action model. If <code>None</code>, this response is a final response to the user.</p>"},{"location":"api/model/#freeact.model.base.CodeActModelTurn","title":"CodeActModelTurn","text":"<p>               Bases: <code>ABC</code></p> <p>A single interaction with a code action model. This is either initiated by a user query or code execution feedback (code action results or execution errors).</p>"},{"location":"api/model/#freeact.model.base.CodeActModelTurn.response","title":"response  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>response() -&gt; CodeActModelResponse\n</code></pre> <p>Retrieve the complete response from a code action model. Waits until the response is available.</p> Source code in <code>freeact/model/base.py</code> <pre><code>@abstractmethod\nasync def response(self) -&gt; CodeActModelResponse:\n    \"\"\"Retrieve the complete response from a code action model. Waits until\n    the response is available.\n    \"\"\"\n</code></pre>"},{"location":"api/model/#freeact.model.base.CodeActModelTurn.stream","title":"stream  <code>abstractmethod</code>","text":"<pre><code>stream() -&gt; AsyncIterator[str]\n</code></pre> <p>Stream the code action model's response as it is generated. Once the stream is consumed, <code>response</code> is immediately available without waiting.</p> Source code in <code>freeact/model/base.py</code> <pre><code>@abstractmethod\ndef stream(self) -&gt; AsyncIterator[str]:\n    \"\"\"Stream the code action model's response as it is generated. Once the\n    stream is consumed, [`response`][freeact.model.base.CodeActModelTurn.response]\n    is immediately available without waiting.\n    \"\"\"\n</code></pre>"},{"location":"api/model/#freeact.model.base.CodeActModel","title":"CodeActModel","text":"<p>               Bases: <code>ABC</code></p> <p>A code action model.</p> <p>A code action model is a model that responds with code if wants to perform an an action. An action is performed by executing the generated code.</p> <p>A code action model responds to user queries and code execution feedback by returning a <code>CodeActModelTurn</code> object which is used to retrieve the model response.</p>"},{"location":"api/model/#freeact.model.base.CodeActModel.request","title":"request  <code>abstractmethod</code>","text":"<pre><code>request(user_query: str, **kwargs) -&gt; CodeActModelTurn\n</code></pre> <p>Initiates an interaction with this model from a user query.</p> <p>Parameters:</p> Name Type Description Default <code>user_query</code> <code>str</code> <p>The user query (a question, instruction, etc.)</p> required <code>**kwargs</code> <p>Additional interaction-specific parameters</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>CodeActModelTurn</code> <code>CodeActModelTurn</code> <p>An object for retrieving the model's response.</p> Source code in <code>freeact/model/base.py</code> <pre><code>@abstractmethod\ndef request(self, user_query: str, **kwargs) -&gt; CodeActModelTurn:\n    \"\"\"Initiates an interaction with this model from a user query.\n\n    Args:\n        user_query: The user query (a question, instruction, etc.)\n        **kwargs: Additional interaction-specific parameters\n\n    Returns:\n        CodeActModelTurn: An object for retrieving the model's response.\n    \"\"\"\n</code></pre>"},{"location":"api/model/#freeact.model.base.CodeActModel.feedback","title":"feedback  <code>abstractmethod</code>","text":"<pre><code>feedback(feedback: str, is_error: bool, tool_use_id: str | None, tool_use_name: str | None, **kwargs) -&gt; CodeActModelTurn\n</code></pre> <p>Initiates an interaction with this model from code execution feedback, allowing the model to refine or correct previous responses, or returning a final response to the user. A <code>feedback</code> call must follow a previous <code>request</code> or <code>feedback</code> call.</p> <p>Parameters:</p> Name Type Description Default <code>feedback</code> <code>str</code> <p>The feedback text from code execution.</p> required <code>is_error</code> <code>bool</code> <p>Whether the <code>feedback</code> text contains error information.</p> required <code>**kwargs</code> <p>Additional model-specific parameters for the feedback.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>CodeActModelTurn</code> <code>CodeActModelTurn</code> <p>An object for retrieving the model's response.</p> Source code in <code>freeact/model/base.py</code> <pre><code>@abstractmethod\ndef feedback(\n    self,\n    feedback: str,\n    is_error: bool,\n    tool_use_id: str | None,\n    tool_use_name: str | None,\n    **kwargs,\n) -&gt; CodeActModelTurn:\n    \"\"\"Initiates an interaction with this model from code execution feedback,\n    allowing the model to refine or correct previous responses, or returning\n    a final response to the user. A `feedback` call must follow a previous\n    `request` or `feedback` call.\n\n    Args:\n        feedback: The feedback text from code execution.\n        is_error: Whether the `feedback` text contains error information.\n        **kwargs: Additional model-specific parameters for the feedback.\n\n    Returns:\n        CodeActModelTurn: An object for retrieving the model's response.\n    \"\"\"\n</code></pre>"},{"location":"api/model/#freeact.model.litellm.LiteCodeActModel","title":"LiteCodeActModel","text":"<pre><code>LiteCodeActModel(model_name: str, skill_sources: str | None = None, system_template: str | None = None, execution_output_template: str | None = None, execution_error_template: str | None = None, use_executor_tool: bool | None = None, use_editor_tool: bool | None = None, **kwargs)\n</code></pre> <p>               Bases: <code>CodeActModel</code></p> <p>A LiteLLM-based code action model.</p> <p>Code actions are generated differently depending on the <code>use_executor_tool</code> argument:</p> <ul> <li> <p><code>use_executor_tool=False</code>: Code actions are included directly into the model's response text,   enclosed in <code>&lt;code-action&gt; ... &lt;/code-action&gt;</code> tags. Uses the   <code>CODE_TAG_SYSTEM_TEMPLATE</code> by default.</p> </li> <li> <p><code>use_executor_tool=True</code>: Code actions are generated by calling an internal <code>execute_ipython_cell</code>   tool. Uses the <code>TOOL_USE_SYSTEM_TEMPLATE</code> by default.</p> </li> <li> <p><code>use_executor_tool=None</code>: A sensible default is chosen based on the model name and provider.   Currently, a tool use approach is used for Anthropic and OpenAI models, a code tag approach   is used for all other models.</p> </li> </ul> <p>A custom system template can be provided with the <code>system_template</code> constructor argument. Its semantics should match the <code>use_executor_tool</code> argument value.</p> <p>Models created with <code>use_editor_tool=True</code> are also able to create and edit files. This allows them to store and edit code actions on disk (= long-term memory). Stored code actions can be loaded as custom skills via <code>get_sources</code>. If <code>use_editor_tool</code> is <code>None</code>, a sensible default is chosen based on the model name and provider. Currently, Anthropic and OpenAI models are configured to use the editor tool.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>A model name supported by LiteLLM.</p> required <code>skill_sources</code> <code>str | None</code> <p>Source code of Python modules offered to the model as skills. They are utilized by generated code actions, if useful for the task. Skill sources are usually loaded and formatted with <code>get_sources</code>.</p> <code>None</code> <code>system_template</code> <code>str | None</code> <p>A system template that guides the model to generate code actions. Must define a <code>{python_modules}</code> placeholder for <code>skill_sources</code>.</p> <code>None</code> <code>execution_output_template</code> <code>str | None</code> <p>A prompt template for formatting successful code execution output. Must define an <code>{execution_feedback}</code> placeholder.</p> <code>None</code> <code>execution_error_template</code> <code>str | None</code> <p>A prompt template for formatting code execution errors. Must define an <code>{execution_feedback}</code> placeholder.</p> <code>None</code> <code>use_executor_tool</code> <code>bool | None</code> <p>Whether to use a tool-based approach for generating code actions (<code>True</code>) or a code tag based approach (<code>False</code>). If <code>None</code>, a sensible default is chosen based on <code>model_name</code>.</p> <code>None</code> <code>use_editor_tool</code> <code>bool | None</code> <p>Whether to use a file editor tool for creating and editing code action modules on disk. If <code>None</code>, a sensible default is chosen based on <code>model_name</code>.</p> <code>None</code> <code>**kwargs</code> <p>Default chat completion <code>kwargs</code> for <code>request</code> and <code>feedback</code> calls. These are merged with <code>request</code> and <code>feedback</code> specific completion <code>kwargs</code> where the latter have higher precedence in case of conflicting keys. The following <code>kwargs</code> are set internally and must not be set here: <code>stream</code>, <code>stream_options</code>, <code>messages</code>, and <code>tools</code>.</p> <code>{}</code> Source code in <code>freeact/model/litellm.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    skill_sources: str | None = None,\n    system_template: str | None = None,\n    execution_output_template: str | None = None,\n    execution_error_template: str | None = None,\n    use_executor_tool: bool | None = None,\n    use_editor_tool: bool | None = None,\n    **kwargs,\n):\n    self.model_name = model_name\n    self.completion_kwargs = kwargs\n\n    if \"max_tokens\" not in self.completion_kwargs:\n        self.completion_kwargs[\"max_tokens\"] = 8192\n\n    if use_executor_tool is None:\n        use_executor_tool = code_executor_tool_use_default(model_name, self.provider_name)\n\n    if use_editor_tool is None:\n        use_editor_tool = code_editor_tool_use_default(model_name, self.provider_name)\n\n    if execution_output_template is None:\n        execution_output_template = EXECUTION_OUTPUT_TEMPLATE\n\n    if execution_error_template is None:\n        execution_error_template = EXECUTION_ERROR_TEMPLATE\n\n    if system_template is None:\n        system_template = TOOL_USE_SYSTEM_TEMPLATE if use_executor_tool else CODE_TAG_SYSTEM_TEMPLATE\n\n    self.execution_output_template = execution_output_template\n    self.execution_error_template = execution_error_template\n\n    system_instruction = system_template.format(python_modules=skill_sources or \"\")\n\n    if self.provider_name == \"anthropic\":\n        if self.completion_kwargs.pop(\"prompt_caching\", True):\n            system_instruction = [  # type: ignore\n                {\n                    \"type\": \"text\",\n                    \"text\": system_instruction,\n                    \"cache_control\": {\n                        \"type\": \"ephemeral\",\n                    },\n                }\n            ]\n\n    self.history: list[dict[str, Any]] = [{\"role\": \"system\", \"content\": system_instruction}]\n    self.tools: list[dict[str, Any]] = []\n\n    if use_executor_tool:\n        self.tools.append(code_executor_tool(model_name))\n\n    if use_editor_tool:\n        self.tools.append(code_editor_tool(model_name))\n\n        if flag := beta_flag(model_name):\n            self.completion_kwargs[\"extra_headers\"] = flag\n</code></pre>"},{"location":"api/model/#freeact.model.litellm.LiteCodeActModel.tool_names","title":"tool_names  <code>property</code>","text":"<pre><code>tool_names: list[str]\n</code></pre> <p>The names of the tools configured for this model.</p>"},{"location":"api/model/#freeact.model.litellm.LiteCodeActModel.provider_name","title":"provider_name  <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>The name of the model's provider.</p>"},{"location":"api/model/#freeact.model.litellm.LiteCodeActModel.request","title":"request","text":"<pre><code>request(user_query: str, **kwargs) -&gt; CodeActModelTurn\n</code></pre> <p>Initiates an interaction with this model from a user query.</p> <p>Parameters:</p> Name Type Description Default <code>user_query</code> <code>str</code> <p>The user query (a question, instruction, etc.)</p> required <code>**kwargs</code> <p>Chat completion arguments. These are merged with the model's default completion <code>kwargs</code>. Default completion <code>kwargs</code> have lower precedence in case of conflicting keys.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>CodeActModelTurn</code> <code>CodeActModelTurn</code> <p>An object for retrieving the model's response.</p> Source code in <code>freeact/model/litellm.py</code> <pre><code>def request(\n    self,\n    user_query: str,\n    **kwargs,\n) -&gt; CodeActModelTurn:\n    \"\"\"Initiates an interaction with this model from a user query.\n\n    Args:\n        user_query: The user query (a question, instruction, etc.)\n        **kwargs: [Chat completion](https://docs.litellm.ai/docs/completion) arguments.\n            These are merged with the model's default completion `kwargs`. Default\n            completion `kwargs` have lower precedence in case of conflicting keys.\n\n    Returns:\n        CodeActModelTurn: An object for retrieving the model's response.\n    \"\"\"\n    user_message = {\"role\": \"user\", \"content\": user_query}\n\n    span_name = \"Model request\"\n    span_input = {\"user_query\": user_query, **kwargs}\n\n    return LiteLLMTurn(self._stream(user_message, **kwargs), span_name, span_input)\n</code></pre>"},{"location":"api/model/#freeact.model.litellm.LiteCodeActModel.feedback","title":"feedback","text":"<pre><code>feedback(feedback: str, is_error: bool, tool_use_id: str | None, tool_use_name: str | None, **kwargs) -&gt; CodeActModelTurn\n</code></pre> <p>Initiates an interaction with this model from code execution feedback, allowing the model to refine or correct previous responses, or returning a final response to the user. A <code>feedback</code> call must follow a previous <code>request</code> or <code>feedback</code> call.</p> <p>Parameters:</p> Name Type Description Default <code>feedback</code> <code>str</code> <p>The feedback text from code execution or other actions.</p> required <code>is_error</code> <code>bool</code> <p>Whether the <code>feedback</code> text contains error information.</p> required <code>**kwargs</code> <p>Chat completion arguments. These are merged with the model's default completion <code>kwargs</code>. Default completion <code>kwargs</code> have lower precedence in case of conflicting keys.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>CodeActModelTurn</code> <code>CodeActModelTurn</code> <p>An object for retrieving the model's response.</p> Source code in <code>freeact/model/litellm.py</code> <pre><code>def feedback(\n    self,\n    feedback: str,\n    is_error: bool,\n    tool_use_id: str | None,\n    tool_use_name: str | None,\n    **kwargs,\n) -&gt; CodeActModelTurn:\n    \"\"\"Initiates an interaction with this model from code execution feedback,\n    allowing the model to refine or correct previous responses, or returning\n    a final response to the user. A `feedback` call must follow a previous\n    `request` or `feedback` call.\n\n    Args:\n        feedback: The feedback text from code execution or other actions.\n        is_error: Whether the `feedback` text contains error information.\n        **kwargs: [Chat completion](https://docs.litellm.ai/docs/completion) arguments.\n            These are merged with the model's default completion `kwargs`. Default\n            completion `kwargs` have lower precedence in case of conflicting keys.\n\n    Returns:\n        CodeActModelTurn: An object for retrieving the model's response.\n    \"\"\"\n    if tool_use_name == tool_name(CODE_EXECUTOR_TOOL) or tool_use_name is None:\n        template = self.execution_error_template if is_error else self.execution_output_template\n        content = template.format(execution_feedback=feedback)\n    else:  # skip application of execution feedback templates for results of other tools\n        content = feedback\n\n    if tool_use_id is None:\n        feedback_message = {\n            \"role\": \"user\",\n            \"content\": content,\n        }\n    else:\n        feedback_message = {\n            \"role\": \"tool\",\n            \"tool_call_id\": tool_use_id,\n            \"content\": content,\n        }\n\n    span_name = \"Model feedback\"\n    span_input = {\n        \"feedback\": feedback,\n        \"is_error\": is_error,\n        \"tool_use_id\": tool_use_id,\n        \"tool_use_name\": tool_use_name,\n        **kwargs,\n    }\n\n    return LiteLLMTurn(self._stream(feedback_message, **kwargs), span_name, span_input)\n</code></pre>"},{"location":"api/model/#freeact.model.prompt.CODE_TAG_SYSTEM_TEMPLATE","title":"CODE_TAG_SYSTEM_TEMPLATE  <code>module-attribute</code>","text":"<pre><code>CODE_TAG_SYSTEM_TEMPLATE = \"You are Freeact Agent, operating as a CodeAct agent, a powerful AI assistant that solves problems by executing Python code. As described in research literature, CodeAct agents use executable Python code as a unified action space to interact with environments, allowing for dynamic adjustment based on execution results.\\n\\n## Core Capabilities\\n\\n- You use Python code execution to solve problems\\n- You can leverage existing Python libraries and packages\\n- You dynamically adjust your approach based on execution results\\n- You can self-debug when encountering errors\\n- You collaborate with users through natural language\\n\\n## API Selection Guidelines\\n\\nWhen solving problems, prioritize specialized domain-specific APIs over general-purpose search APIs for more reliable and accurate results:\\n\\n1. **Prefer specialized APIs and libraries**:\\n   - First check if required modules are available in the `&lt;python-modules&gt;` section\\n   - Use purpose-built libraries for specific domains:\\n     * `yfinance` for stock/financial market data\\n     * `open-meteo` with geocoding for weather forecasts and historical data\\n     * GitHub API for repository information and code analysis\\n     * Domain-specific data sources for particular industries or fields\\n     * Scientific and statistical packages for their respective domains\\n\\n2. **Use general search APIs only when necessary**:\\n   - Resort to `InternetSearch` API only when:\\n     * No specialized API exists for the required data\\n     * You need general information not available through structured APIs\\n     * You need to find which specialized API might be appropriate\\n\\n3. **Combine approaches when beneficial**:\\n   - Use specialized APIs for core data retrieval\\n   - Supplement with search results for context or explanation\\n   - Cross-validate information from multiple sources when accuracy is critical\\n\\n## Python Modules and Skills\\n\\n&lt;python-modules&gt;\\n{python_modules}\\n&lt;/python-modules&gt;\\n\\n## How to Operate\\n\\n1. **Analyze the user's request** carefully, determining what they need help with\\n2. **Think through your solution approach** before writing code\\n3. **Use code execution** to interact with the environment, process data, and solve problems\\n4. **Interpret execution results** to refine your approach\\n5. **Communicate clearly** with users, explaining your thought process\\n\\n## Code Execution\\n\\nYou generate Python code that will be executed in an IPython environment. State is persistent across executions, so variables defined in one execution are available in subsequent ones.\\n\\nTo provide code:\\n1. Write valid, well-structured Python code\\n2. Enclose your code in triple backtick blocks with the Python language specifier, and additionally wrap the entire code block in `&lt;code-action&gt;` tags:\\n   &lt;code-action&gt;\\n   ```python\\n   # Your code here\\n   ```\\n   &lt;/code-action&gt;\\n3. Stop generating output after providing the code block\\n4. The user will execute your code and return the results in their next message\\n5. Analyze the execution results to refine your approach if needed\\n\\n## Best Practices\\n\\n1. **Load libraries appropriately**: Import necessary libraries at the beginning of your solution. Install missing libraries with `!pip install library_name` as needed.\\n\\n2. **Structured approach to complex problems**:\\n   - Break down complex tasks into smaller steps\\n   - Use variables to store intermediate results\\n   - Leverage control flow (loops, conditionals) for complex operations\\n\\n3. **Self-debugging**:\\n   - When encountering errors, carefully read error messages\\n   - Make targeted changes to address specific issues\\n   - Test step by step to isolate and fix problems\\n\\n4. **Clear communication**:\\n   - Explain your approach to the user in natural language\\n   - Interpret code execution results in a way that's meaningful to the user\\n   - Be transparent about your reasoning process\\n\\n5. **Progressive refinement**:\\n   - Start with simple approaches and refine based on results\\n   - Incrementally build up to your solution\\n   - Use the persistent state to build on previous executions\\n\\n## Interaction Format\\n\\n1. **For each interaction**:\\n   - Start by understanding the user's request\\n   - Share your thought process briefly\\n   - Write Python code to solve the problem\\n   - Enclose the code in ```python ... ``` blocks within `&lt;code-action&gt;` tags\\n   - Stop generating further output after the code block\\n   - Wait for the user to execute the code and provide the results\\n   - Analyze the results in your next response and continue solving the problem\\n\\nRemember, you're not just providing code - you're helping users solve problems by leveraging Python's capabilities and your ability to reason about code execution results.\\n\"\n</code></pre>"},{"location":"api/model/#freeact.model.prompt.TOOL_USE_SYSTEM_TEMPLATE","title":"TOOL_USE_SYSTEM_TEMPLATE  <code>module-attribute</code>","text":"<pre><code>TOOL_USE_SYSTEM_TEMPLATE = \"You are Freeact Agent, operating as a CodeAct agent, a powerful AI assistant that solves problems by executing Python code. As described in research literature, CodeAct agents use executable Python code as a unified action space to interact with environments, allowing for dynamic adjustment based on execution results.\\n\\n## Core Capabilities\\n\\n- You use Python code execution to solve problems\\n- You can leverage existing Python libraries and packages\\n- You dynamically adjust your approach based on execution results\\n- You can self-debug when encountering errors\\n- You collaborate with users through natural language\\n\\n## API Selection Guidelines\\n\\nWhen solving problems, prioritize specialized domain-specific APIs over general-purpose search APIs for more reliable and accurate results:\\n\\n1. **Prefer specialized APIs and libraries**:\\n   - First check if required modules are available in the `&lt;python-modules&gt;` section\\n   - Use purpose-built libraries for specific domains:\\n     * `yfinance` for stock/financial market data\\n     * `open-meteo` with geocoding for weather forecasts and historical data\\n     * GitHub API for repository information and code analysis\\n     * Domain-specific data sources for particular industries or fields\\n     * Scientific and statistical packages for their respective domains\\n\\n2. **Use general search APIs only when necessary**:\\n   - Resort to `InternetSearch` API only when:\\n     * No specialized API exists for the required data\\n     * You need general information not available through structured APIs\\n     * You need to find which specialized API might be appropriate\\n\\n3. **Combine approaches when beneficial**:\\n   - Use specialized APIs for core data retrieval\\n   - Supplement with search results for context or explanation\\n   - Cross-validate information from multiple sources when accuracy is critical\\n\\n## Python Modules and Skills\\n\\n&lt;python-modules&gt;\\n{python_modules}\\n&lt;/python-modules&gt;\\n\\n## How to Operate\\n\\n1. **Analyze the user's request** carefully, determining what they need help with\\n2. **Think through your solution approach** before writing code\\n3. **Use code execution** to interact with the environment, process data, and solve problems\\n4. **Interpret execution results** to refine your approach\\n5. **Communicate clearly** with users, explaining your thought process\\n\\n## Code Execution\\n\\nYou have access to the `execute_ipython_cell` function that executes Python code in an IPython environment. State is persistent across executions, so variables defined in one execution are available in subsequent ones.\\n\\nTo execute code:\\n1. Write valid, well-structured Python code\\n2. Submit it using the execute_ipython_cell function\\n3. Analyze the execution results\\n4. If errors occur, debug and refine your approach\\n\\n## Best Practices\\n\\n1. **Load libraries appropriately**: Import necessary libraries at the beginning of your solution. Install missing libraries with `!pip install library_name` as needed.\\n\\n2. **Structured approach to complex problems**:\\n   - Break down complex tasks into smaller steps\\n   - Use variables to store intermediate results\\n   - Leverage control flow (loops, conditionals) for complex operations\\n\\n3. **Self-debugging**:\\n   - When encountering errors, carefully read error messages\\n   - Make targeted changes to address specific issues\\n   - Test step by step to isolate and fix problems\\n\\n4. **Clear communication**:\\n   - Explain your approach to the user in natural language\\n   - Interpret code execution results in a way that's meaningful to the user\\n   - Be transparent about your reasoning process\\n\\n5. **Progressive refinement**:\\n   - Start with simple approaches and refine based on results\\n   - Incrementally build up to your solution\\n   - Use the persistent state to build on previous executions\\n\\n## Interaction Format\\n\\n1. **For each interaction**:\\n   - Start by understanding the user's request\\n   - Share your thought process briefly\\n   - Write and execute code to solve the problem\\n   - Interpret results for the user\\n   - Continue the conversation based on the user's follow-up questions\\n\\nRemember, you're not just providing code - you're helping users solve problems by leveraging Python's capabilities and your ability to reason about code execution results.\\n\"\n</code></pre>"},{"location":"api/model/#freeact.model.prompt.EXECUTION_OUTPUT_TEMPLATE","title":"EXECUTION_OUTPUT_TEMPLATE  <code>module-attribute</code>","text":"<pre><code>EXECUTION_OUTPUT_TEMPLATE = \"The code was executed successfully. Here is the output:\\n\\n&lt;execution-output&gt;\\n{execution_feedback}\\n&lt;/execution-output&gt;\\n\\nBased on this result, you can now:\\n1. Interpret the output for the user\\n2. Determine if additional code execution is needed\\n3. Refine your approach if the results aren't as expected\\n\\nRemember to explain what the output means in relation to the user's original request.\\n\"\n</code></pre>"},{"location":"api/model/#freeact.model.prompt.EXECUTION_ERROR_TEMPLATE","title":"EXECUTION_ERROR_TEMPLATE  <code>module-attribute</code>","text":"<pre><code>EXECUTION_ERROR_TEMPLATE = \"The code execution resulted in an error. Here's the error message:\\n\\n&lt;error-message&gt;\\n{execution_feedback}\\n&lt;/error-message&gt;\\n\\nPlease:\\n1. Carefully analyze the error message to identify the root cause\\n2. Explain the issue to the user in simple terms\\n3. Revise your code to address the specific error\\n4. Consider common causes for this type of error:\\n   - Syntax errors\\n   - Missing imports or undefined variables\\n   - Type mismatches\\n   - Logic errors in your implementation\\n   - Missing dependencies that need installation\\n\\nWhen submitting revised code, focus on addressing the specific error while maintaining your overall solution approach.\\n\"\n</code></pre>"},{"location":"api/tracing/","title":"Tracing","text":""},{"location":"api/tracing/#freeact.tracing.context.configure","title":"configure","text":"<pre><code>configure(**kwargs) -&gt; None\n</code></pre> <p>Configures agent tracing using a <code>Langfuse</code> backend. Once configured, all agent activities, code executions and model calls are automatically captured and exported to Langfuse.</p> <p>Accepts all Langfuse configuration options. Configuration options can be provided as parameters to <code>configure()</code> or via environment variables.</p> <p>Should be called at application startup.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Langfuse configuration parameters.</p> <code>{}</code> Source code in <code>freeact/tracing/context.py</code> <pre><code>def configure(**kwargs) -&gt; None:\n    \"\"\"Configures agent tracing using a [`Langfuse`](https://langfuse.com) backend. Once configured, all agent activities, code executions and model calls are automatically captured and exported to Langfuse.\n\n    Accepts all [Langfuse configuration options](https://python.reference.langfuse.com/langfuse/decorators#LangfuseDecorator.configure).\n    Configuration options can be provided as parameters to `configure()` or via environment variables.\n\n    Should be called at application startup.\n\n    Args:\n        **kwargs: Langfuse configuration parameters.\n    \"\"\"\n    global _tracer\n\n    with _tracing_setup_lock:\n        if _tracer is not None:\n            logger.warning(\"Tracing is already configured. Call 'tracing.shutdown()' first to reconfigure.\")\n            return\n\n        _tracer = LangfuseTracer(**kwargs)\n\n        atexit.register(_shutdown_tracing)\n\n        if threading.current_thread() is threading.main_thread():\n            for sig in (signal.SIGTERM, signal.SIGHUP):\n                try:\n                    signal.signal(sig, _shutdown_signal_handler)\n                except (ValueError, OSError):\n                    pass\n</code></pre>"},{"location":"api/tracing/#freeact.tracing.context.session","title":"session","text":"<pre><code>session(session_id: str | None = None) -&gt; Iterator[str]\n</code></pre> <p>Context manager that creates a session scope for tracing operations.</p> <p>All tracing operations within this context are associated with the specified session id.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str | None</code> <p>Identifier for the session. A random session id is generated if not specified.</p> <code>None</code> Source code in <code>freeact/tracing/context.py</code> <pre><code>@contextmanager\ndef session(session_id: str | None = None) -&gt; Iterator[str]:\n    \"\"\"Context manager that creates a session scope for tracing operations.\n\n    All tracing operations within this context are associated with the specified session id.\n\n    Args:\n        session_id: Identifier for the session. A random session id is generated if not specified.\n    \"\"\"\n    active_session_id = session_id or create_session_id()\n    token = _active_session_id_context.set(active_session_id)\n    try:\n        yield active_session_id\n    finally:\n        _active_session_id_context.reset(token)\n</code></pre>"},{"location":"api/tracing/#freeact.tracing.context.shutdown","title":"shutdown","text":"<pre><code>shutdown() -&gt; None\n</code></pre> <p>Shuts down agent tracing and flushes pending traces to the backend.</p> <p><code>shutdown()</code> is called automatically on application exit. For manual control, call this function explicitly.</p> Source code in <code>freeact/tracing/context.py</code> <pre><code>def shutdown() -&gt; None:\n    \"\"\"Shuts down agent tracing and flushes pending traces to the backend.\n\n    `shutdown()` is called automatically on application exit. For manual control, call this function explicitly.\n    \"\"\"\n    _shutdown_tracing()\n</code></pre>"},{"location":"skills/autonomous-learning/","title":"Autonomous skill learning","text":"<p>Info</p> <p>This feature is currently in development and will be available soon.</p> <p><code>freeact</code> agents already improve their code actions automatically based on code execution feedback. They also support collaborative skill development. We are currently enhancing them further to autonomously store, index, load and improve skills on demand based on task details and current state.</p>"},{"location":"skills/collaborative-learning/","title":"Collaborative skill learning","text":"<p>Collaborative skill learning starts by requesting an agent to perform a task and then having a conversation with the agent to iteratively improve on that task. This is mainly achieved by improving the code actions the agent generates. In this phase the agent gets feedback from both the execution environment and the user. When the user is satisfied with the agent's performance, it stores optimized code actions as skills in long-term memory<sup>1</sup>. Since this is very similar to using a SWE agent, this phase is also called skill development. Stored skill modules can then be reused by other agents.  These agents may also continue the skill development process, if needed.</p>"},{"location":"skills/collaborative-learning/#skill-development","title":"Skill development","text":"<p>In the following example, an agent is asked to fit a Gaussian Process (GP) to noisy samples drawn from a sine function. The user then asks the agent to store the generated code actions as functions <code>gp_fit</code> (to fit a GP to data) and <code>plot_pred</code> (to plot GP predictions) in a module named <code>gp_fit.py</code>. After reviewing the code of the stored skill module<sup>2</sup>, the user requests to change the default value of the <code>n_restarts</code> parameter to <code>15</code> and the agent edits the stored module. Finally, the module is tested with noisy samples from a cosine function.</p> PythonCLI <pre><code>import asyncio\nimport os\n\nfrom rich.console import Console\n\nfrom freeact import CodeActAgent, LiteCodeActModel, execution_environment\nfrom freeact.cli.utils import stream_conversation\n\n\nasync def main():\n    async with execution_environment(\n        ipybox_tag=\"ghcr.io/gradion-ai/ipybox:example\",\n    ) as env:\n        async with env.code_executor() as executor:\n            model = LiteCodeActModel(\n                model_name=\"anthropic/claude-3-7-sonnet-20250219\",\n                reasoning_effort=\"low\",\n                api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n            )\n            agent = CodeActAgent(model=model, executor=executor)\n            await stream_conversation(agent, console=Console())\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <pre><code>uvx freeact \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:example \\\n  --model-name=anthropic/claude-3-7-sonnet-20250219 \\\n  --reasoning-effort=low \\\n  --api-key=$ANTHROPIC_API_KEY\n</code></pre> <p>Example</p> <p></p> <p>Produced images:</p> <p> </p>"},{"location":"skills/collaborative-learning/#skill-reuse","title":"Skill reuse","text":"<p>The next example loads the sources of the developed skill and tests it with another agent. The agent is asked to generate noisy samples from a non-linear function of its choice, fit them with a GP and plot the predictions. The agent now uses the developed skill immediately in its code actions.</p> <p>Info</p> <p>At the moment, it is an application's responsibility to load skill sources. We will soon enhance <code>freeact</code> agents to retrieve skill sources autonomously depending on the user query and current state.</p> PythonCLI <pre><code>import asyncio\nimport os\n\nfrom rich.console import Console\n\nfrom freeact import CodeActAgent, LiteCodeActModel, execution_environment\nfrom freeact.cli.utils import stream_conversation\n\n\nasync def main():\n    async with execution_environment(\n        ipybox_tag=\"ghcr.io/gradion-ai/ipybox:example\",\n    ) as env:\n        async with env.code_provider() as provider:\n            skill_sources = await provider.get_sources(\n                module_names=[\"gp_fit\"],  # (1)!\n            )\n        async with env.code_executor() as executor:\n            model = LiteCodeActModel(\n                model_name=\"anthropic/claude-3-7-sonnet-20250219\",\n                reasoning_effort=\"low\",\n                skill_sources=skill_sources,\n                api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n            )\n            agent = CodeActAgent(model=model, executor=executor)\n            await stream_conversation(agent, console=Console())\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <ol> <li>Loads the sources of the developed skill module.</li> </ol> <pre><code>uvx freeact \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:example \\\n  --model-name=anthropic/claude-3-7-sonnet-20250219 \\\n  --reasoning-effort=low \\\n  --skill-modules=gp_fit \\\n  --api-key=$ANTHROPIC_API_KEY\n\nuvx freeact \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:basic \\\n  --model-name=anthropic/claude-3-7-sonnet-20250219 \\\n  --reasoning-effort=low \\\n  --skill-modules=freeact_skills.search.google.stream.api \\\n  --tracing\n</code></pre> <p>Example</p> <p></p> <p>Produced images:</p> <p> </p> <ol> <li> <p>By default, Anthropic and OpenAI models are configured with a file editor tool for storing and editing skills. To configure this for other models, set <code>use_editor_tool=True</code> in the constructor of <code>LiteCodeActModel</code>.\u00a0\u21a9</p> </li> <li> <p>The stored skill module is accessible on the code execution container's host under <code>private_skills_host_path</code>, a directory mounted into the container. The path value can be obtained with:</p> <p><pre><code>async with execution_environment(ipybox_tag=\"ghcr.io/gradion-ai/ipybox:example\") as env:\n    skills_path = env.container.workspace.private_skills_host_path\n    ...\n</code></pre> \u21a9</p> </li> </ol>"},{"location":"skills/internal-knowledge/","title":"Using internal knowledge","text":"<p>Modern LLMs have a vast amount of knowledge and skills acquired during pre- and post-training.  For many tasks there's no need to provide them extra skill sources when used as code action models. For example, Claude 3.7 Sonnet can perform a Kernel Ridge Regression, hyperparameter optimization and plotting the results out-of-the box from its prior knowledge:</p> PythonCLI <pre><code>import asyncio\nimport os\n\nfrom rich.console import Console\n\nfrom freeact import CodeActAgent, LiteCodeActModel, execution_environment\nfrom freeact.cli.utils import stream_conversation\n\n\nasync def main():\n    async with execution_environment(\n        ipybox_tag=\"ghcr.io/gradion-ai/ipybox:example\",\n    ) as env:\n        async with env.code_executor() as executor:\n            model = LiteCodeActModel(\n                model_name=\"anthropic/claude-3-7-sonnet-20250219\",\n                reasoning_effort=\"low\",\n                api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n            )\n            agent = CodeActAgent(model=model, executor=executor)\n            await stream_conversation(agent, console=Console())\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <pre><code>uvx freeact \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:example \\\n  --model-name=anthropic/claude-3-7-sonnet-20250219 \\\n  --reasoning-effort=low \\\n  --api-key=$ANTHROPIC_API_KEY\n</code></pre> <p>Example</p> <p></p> <p>Produced images:</p> <p> </p>"},{"location":"skills/learning-by-example/","title":"Learning skills by example","text":"<p>A <code>freeact</code> agent can also learn skills from code examples in documentation or other sources instead of skill modules directly. These sources are usually retrieved by the agent itself using spezialized skills, like a Firecrawl MCP server as in the following example, but can also be provided by a user or an application directly in context. In the following example, Python code is scraped from the API documentation of Readwise Reader and used generate a code action for downloading documents from the user's Reader account.</p> PythonCLI <pre><code>import asyncio\nimport os\n\nfrom rich.console import Console\n\nfrom freeact import CodeActAgent, LiteCodeActModel, execution_environment\nfrom freeact.cli.utils import stream_conversation\n\n\nasync def main():\n    async with execution_environment(\n        ipybox_tag=\"ghcr.io/gradion-ai/ipybox:basic\",\n        ipybox_env={\"READWISE_API_KEY\": os.environ[\"READWISE_API_KEY\"]},\n    ) as env:\n        async with env.code_provider() as provider:\n            await provider.register_mcp_servers(\n                {\n                    \"firecrawl\": {\n                        \"command\": \"npx\",\n                        \"args\": [\"-y\", \"firecrawl-mcp\"],\n                        \"env\": {\"FIRECRAWL_API_KEY\": os.getenv(\"FIRECRAWL_API_KEY\")},\n                    }\n                }\n            )\n\n            skill_sources = await provider.get_sources(mcp_tool_names={\"firecrawl\": [\"firecrawl_scrape\"]})\n\n        async with env.code_executor() as executor:\n            model = LiteCodeActModel(\n                model_name=\"gpt-4.1\",\n                skill_sources=skill_sources,\n                api_key=os.getenv(\"OPENAI_API_KEY\"),\n            )\n            agent = CodeActAgent(model=model, executor=executor)\n            await stream_conversation(agent, console=Console())\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> .env<pre><code>READWISE_API_KEY=...\n</code></pre> mcp.json<pre><code>{\n    \"mcpServers\": {\n        \"firecrawl\": {\n            \"command\": \"npx\",\n            \"args\": [\"-y\", \"firecrawl-mcp\"],\n            \"env\": {\"FIRECRAWL_API_KEY\": \"your-firecrawl-api-key\"}\n        }\n    },\n    \"mcpTools\": {\n        \"firecrawl\": [\"firecrawl_scrape\"]\n    }\n}\n</code></pre> <pre><code>uvx freeact \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:example \\\n  --model-name=gpt-4.1 \\\n  --mcp-servers=mcp.json \\\n  --api-key=$OPENAI_API_KEY\n</code></pre> <p>Example</p> <p></p>"},{"location":"skills/predefined-skills/","title":"Using predefined skills","text":"<p>Other tasks may require providing an agent with predefined skills for being able to solve them more efficiently or to solve them at all.  For example, the <code>freeact_skills.zotero.api</code> skill module from the <code>freeact-skills</code> project provides a Python API for syncing Zotero Group libraries with a local directory, loading collection and document metadata as graph into memory and navigating it.</p> <p>The entire <code>freeact_skills.zotero</code> package is partitioned into a <code>freeact_skills.zotero.api</code> module that provides the Python API, including a definition of domain classes.  This module is presented to code action models so they can understand how to use the Zotero skill. The <code>freeact_skills.zotero.impl</code> module contains further implementation details that need not be presented to code action models.</p> <p>The following example demonstrates how a <code>freeact</code> agent uses that skill. It demonstrates how multiple methods and functions from the skill module can be flexibly combined within code actions.  The <code>freeact_skills.zotero.api</code> module is also a good example how domain objects may encapsulate both data and behavior - a core principle of object-oriented programming.  A <code>Collection</code> object, for example, encapsulates data and provides methods <code>sub_documents()</code> and <code>sub_collections()</code> to recursively traverse the collection and document graph under it. </p> PythonCLI <pre><code>import asyncio\nimport os\n\nfrom rich.console import Console\n\nfrom freeact import CodeActAgent, LiteCodeActModel, execution_environment\nfrom freeact.cli.utils import stream_conversation\n\n\nasync def main():\n    async with execution_environment(\n        ipybox_tag=\"ghcr.io/gradion-ai/ipybox:example\",\n        ipybox_env={\n            \"ZOTERO_API_KEY\": os.environ[\"ZOTERO_API_KEY\"],\n            \"ZOTERO_GROUP_ID\": os.environ[\"ZOTERO_GROUP_ID\"],\n        },\n    ) as env:\n        async with env.code_provider() as provider:\n            skill_sources = await provider.get_sources(\n                module_names=[\"freeact_skills.zotero.api\"],\n            )\n        async with env.code_executor() as executor:\n            model = LiteCodeActModel(\n                model_name=\"anthropic/claude-3-7-sonnet-20250219\",\n                reasoning_effort=\"low\",\n                skill_sources=skill_sources,\n                api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n            )\n            agent = CodeActAgent(model=model, executor=executor)\n            await stream_conversation(agent, console=Console())\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> .env<pre><code>ZOTERO_API_KEY=...\nZOTERO_GROUP_ID=...\n</code></pre> <pre><code>uvx freeact \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:example \\\n  --model-name=anthropic/claude-3-7-sonnet-20250219 \\\n  --reasoning-effort=low \\\n  --skill-modules=freeact_skills.zotero.api \\\n  --api-key=$ANTHROPIC_API_KEY\n</code></pre> <p>Example</p> <p></p>"}]}