{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"<code>freeact</code>","text":"<p>A lightweight library for code-action based agents.</p>"},{"location":"#introduction","title":"Introduction","text":"<p><code>freeact</code> is a lightweight agent library that empowers language models to act as autonomous agents through executable code actions. By enabling agents to express their actions directly in code rather than through constrained formats like JSON, <code>freeact</code> provides a flexible and powerful approach to solving complex, open-ended problems that require dynamic solution paths.</p> <p>The library builds upon recent research demonstrating that code-based actions significantly outperform traditional agent approaches, with studies showing up to 20% higher success rates compared to conventional methods. While existing solutions often restrict agents to predefined tool sets, <code>freeact</code> removes these limitations by allowing agents to leverage the full power of the Python ecosystem, dynamically installing and utilizing any required libraries as needed.</p>"},{"location":"#key-capabilities","title":"Key capabilities","text":"<p><code>freeact</code> agents can autonomously improve their actions through learning from environmental feedback, execution results, and human guidance. They can store and reuse successful code actions as custom skills in long-term memory. These skills can be composed and interactively refined to build increasingly sophisticated capabilities, enabling efficient scaling to complex tasks.</p> <p><code>freeact</code> executes all code actions within <code>ipybox</code>, a secure execution environment built on IPython and Docker that can also be deployed locally. This ensures safe execution of dynamically generated code while maintaining full access to the Python ecosystem. Combined with its lightweight and extensible architecture, <code>freeact</code> provides a robust foundation for building adaptable AI agents that can resolve real-world challenges requiring dynamic problem-solving approaches.</p>"},{"location":"#next-steps","title":"Next steps","text":"<ul> <li>Quickstart - Launch your first <code>freeact</code> agent and interact with it on the command line</li> <li>Building blocks - Learn about the essential components of a <code>freeact</code> agent system</li> <li>Tutorials - Tutorials demonstrating the usage of <code>freeact</code> building blocks</li> <li>Command line - Guide to using <code>freeact</code> agents from the command line</li> <li>Supported models - Overview of models evaluated with <code>freeact</code></li> </ul>"},{"location":"#further-reading","title":"Further reading","text":"<ul> <li>Model integration - Guide for integrating new models into <code>freeact</code></li> <li>Execution environment - Overview of prebuilt and custom execution environments</li> <li>Streaming protocol - Specification for streaming model responses and execution results</li> </ul>"},{"location":"#status","title":"Status","text":"<p><code>freeact</code> is in an early stage of development, with ongoing development of new features. Community feedback and contributions are greatly appreciated as <code>freeact</code> continues to evolve.</p>"},{"location":"blocks/","title":"Building Blocks","text":"<p>A <code>freeact</code> agent system consists of five essential components that work together to enable code-action based autonomous agents:</p> <ol> <li> <p>Code action models implement the <code>CodeActModel</code> interface and either generate executable Python code snippets or produce final answers when tasks are complete. These models serve as the decision-making engine, determining what actions the agent should take based on user input, action history, environment observations, and code execution feedback. </p> </li> <li> <p>Code action agents orchestrate the interaction between models and the code execution environment. Built on the <code>CodeActAgent</code> class, they coordinate code execution flow, manage multi-turn conversations with users, and operate the end-to-end streaming protocol. Through cycles of action generation, execution, and feedback analysis, these agents drive an adaptive loop that enables solving open-ended tasks through dynamic solution paths.</p> </li> <li> <p>A code execution environment provides secure, sandboxed execution of code actions through <code>ipybox</code>, a specialized runtime built on IPython and Docker. It ensures safe execution while maintaining full access to the Python ecosystem, enabling agents to leverage external libraries and custom skills as needed. The runtime is implemented by the <code>executor</code> module and can be deployed locally or remotely.</p> </li> <li> <p>Custom Skills are reusable code modules derived from successful code actions, optionally optimized through interactive user-agent sessions. The agent helps transform code actions into well-structured Python packages, enabling progressive capability building. These skills are stored in long-term memory, can be shared across agent instances, and iteratively improved through continued agent-human collaboration. The <code>freeact-skills</code> project provides predefined skills.</p> </li> <li> <p>System Extensions enable customization of agent behavior through natural language specifications of additional rules, constraints, and workflows. These extensions support advanced capabilities such as human-in-the-loop processes, domain-specific knowledge integration, and agent runbooks, allowing the agent to adapt to specialized use cases and operational requirements.</p> </li> </ol>"},{"location":"blocks/#architectural-overview","title":"Architectural overview","text":"High-level architecture of a <code>freeact</code> agent system. It shows building blocks 1 - 4, system extensions are not shown."},{"location":"cli/","title":"Command line interface","text":"<p><code>freeact</code> provides a minimalistic command line interface (CLI) for running agents. It is currently intended for demonstration purposes only. Install <code>freeact</code> and run the following command to see all available options:</p> <pre><code>python -m freeact.cli --help\n</code></pre> <p>Tip</p> <p>Check quickstart, tutorials or supported models for usage examples.</p>"},{"location":"cli/#multiline-input","title":"Multiline input","text":"<p>The <code>freeact</code> CLI supports entering messages that span multiple lines in two ways:</p> <ol> <li>Copy-paste: You can directly copy and paste multiline content into the CLI</li> <li>Manual entry: Press <code>Alt+Enter</code> (Linux/Windows) or <code>Option+Enter</code> (macOS) to add a new line while typing</li> </ol> <p>To submit a multiline message, simply press <code>Enter</code>.</p> <p></p>"},{"location":"cli/#environment-variables","title":"Environment variables","text":"<p>Environment variables may be required for two purposes:</p> <ol> <li>Running skill modules in the execution environment, including:<ul> <li>Predefined skills</li> <li>Custom skills</li> </ul> </li> <li>Running code action models by <code>freeact</code> agents</li> </ol> <p>There are three ways to provide these environment variables:</p> <ul> <li>For skill modules (purpose 1): Variables must be defined in a <code>.env</code> file in your current working directory</li> <li>For code action models (purpose 2): Variables can be provided through:<ul> <li>A <code>.env</code> file in the current working directory</li> <li>Command-line arguments</li> <li>Shell environment variables</li> </ul> </li> </ul> <p>This is shown by example in the following two subsections.</p>"},{"location":"cli/#example-1","title":"Example 1","text":"<p>The quickstart example requires <code>ANTHROPIC_API_KEY</code> and <code>GOOGLE_API_KEY</code> to be defined in a <code>.env</code> file in the current directory. The <code>ANTHROPIC_API_KEY</code> is needed for the <code>claude-3-5-sonnet-20241022</code> code action model, while the <code>GOOGLE_API_KEY</code> is required for the <code>freeact_skills.search.google.stream.api</code> skill in the execution environment. Given a <code>.env</code> file with the following content:</p> .env<pre><code># Required for Claude 3.5 Sonnet\nANTHROPIC_API_KEY=...\n\n# Required for generative Google Search via Gemini 2\nGOOGLE_API_KEY=...\n</code></pre> <p>the following command will launch an agent with <code>claude-3-5-sonnet-20241022</code> as code action model configured with a generative Google search skill implemented by module <code>freeact_skills.search.google.stream.api</code>:</p> <pre><code>python -m freeact.cli \\\n  --model-name=claude-3-5-sonnet-20241022 \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:basic \\\n  --skill-modules=freeact_skills.search.google.stream.api\n</code></pre> <p>The API key can alternatively be passed as command-line argument:</p> <pre><code>python -m freeact.cli \\\n  --model-name=claude-3-5-sonnet-20241022 \\\n  --api-key=$ANTHROPIC_API_KEY \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:basic \\\n  --skill-modules=freeact_skills.search.google.stream.api\n</code></pre>"},{"location":"cli/#example-2","title":"Example 2","text":"<p>To use models from other providers, such as accounts/fireworks/models/deepseek-v3 hosted by Fireworks, you can either provide all required environment variables in a <code>.env</code> file:</p> .env<pre><code># Required for DeepSeek V3 hosted by Fireworks\nDEEPSEEK_BASE_URL=https://api.fireworks.ai/inference/v1\nDEEPSEEK_API_KEY=...\n\n# Required for generative Google Search via Gemini 2\nGOOGLE_API_KEY=...\n</code></pre> <p>and launch the agent with</p> <pre><code>python -m freeact.cli \\\n  --model-name=accounts/fireworks/models/deepseek-v3 \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:basic \\\n  --skill-modules=freeact_skills.search.google.stream.api\n</code></pre> <p>or pass the base URL and API key directly as command-line arguments:</p> <pre><code>python -m freeact.cli \\\n  --model-name=accounts/fireworks/models/deepseek-v3 \\\n  --base-url=https://api.fireworks.ai/inference/v1 \\\n  --api-key=$DEEPSEEK_API_KEY \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:basic \\\n  --skill-modules=freeact_skills.search.google.stream.api\n</code></pre>"},{"location":"environment/","title":"Execution environment","text":"<p><code>freeact</code> uses <code>ipybox</code> as its code execution environment, providing a secure Docker-based IPython runtime. You can either create a custom Docker image with your specific requirements or use prebuilt Docker images.</p>"},{"location":"environment/#custom-docker-image","title":"Custom Docker image","text":"<p>To build a custom <code>ipybox</code> Docker image with <code>freeact-skills</code> pre-installed, create a <code>dependencies.txt</code> file:</p> dependencies.txt<pre><code>freeact-skills = {version = \"*\", extras = [\"all\"]}\n# Add additional dependencies here if needed\n</code></pre> <p>Note</p> <p><code>dependencies.txt</code> must follow the Poetry dependency specification format.</p> <p>Then build the <code>ipybox</code> Docker image referencing the dependencies file:</p> <pre><code>python -m ipybox build -t gradion-ai/ipybox:custom -d dependencies.txt\n</code></pre> <p>To use the image, reference it in <code>CodeExecutionContainer</code> when creating an <code>ipybox</code> Docker container. Use the <code>env</code> argument to set any API keys required by the pre-installed skills. </p>"},{"location":"environment/#prebuilt-docker-images","title":"Prebuilt Docker images","text":"<p>We provide prebuilt Docker images with variants <code>minimal</code>, <code>basic</code>, and <code>example</code>. These variants have the following dependencies installed:</p> <ul> <li> <p><code>ghcr.io/gradion-ai/ipybox:minimal</code>: </p> docker/dependencies-minimal.txt<pre><code>freeact-skills = {version = \"0.0.8\", extras = [\"search-google\", \"search-perplexity\"]}\n</code></pre> </li> <li> <p><code>ghcr.io/gradion-ai/ipybox:basic</code>: </p> docker/dependencies-basic.txt<pre><code>freeact-skills = {version = \"0.0.8\", extras = [\"search-google\", \"search-perplexity\"]}\nmatplotlib = \"^3.10\"\nnumpy = \"^2.2\"\npandas = \"^2.2\"\nsympy = \"^1.13\"\n</code></pre> </li> <li> <p><code>ghcr.io/gradion-ai/ipybox:example</code>, used for the tutorials: </p> docker/dependencies-example.txt<pre><code>freeact-skills = {version = \"0.0.8\", extras = [\"all\"]}\nmatplotlib = \"^3.10\"\nnumpy = \"^2.2\"\npandas = \"^2.2\"\npygithub = \"^2.5\"\nsympy = \"^1.13\"\n</code></pre> </li> </ul> <p>Note</p> <p>Prebuilt images run with root privileges. For non-root execution, build a custom Docker image (and find further details in the <code>ipybox</code> installation guide).</p>"},{"location":"environment/#installing-dependencies-at-runtime","title":"Installing dependencies at runtime","text":"<p>In addition to letting an agent install required dependencies at runtime, you can install extra dependencies at runtime before launching an agent, which is useful for testing custom skills across agent sessions without rebuilding Docker images:</p> <pre><code>from freeact import execution_environment\n\nasync with execution_environment(ipybox_tag=\"ghcr.io/gradion-ai/ipybox:basic\") as env:\n    # Install the serpapi package in the current environment\n    await env.executor.execute(\"!pip install serpapi\")\n\n    # Import skill modules that depend on serpapi\n    skill_sources = await env.executor.get_module_sources(\n        module_names=[\"my_skill_module_1\", \"my_skill_module_2\"],\n    )\n\n    # Initialize agent with the new skills\n    # ...\n</code></pre> <p>For production use, it's recommended to include frequently used dependencies in a custom Docker image instead.</p>"},{"location":"evaluation/","title":"Evaluation results","text":"<p>We evaluated <code>freeact</code> with the following models:</p> <ul> <li>Claude 3.5 Sonnet (<code>claude-3-5-sonnet-20241022</code>)</li> <li>Claude 3.5 Haiku (<code>claude-3-5-haiku-20241022</code>)</li> <li>Gemini 2.0 Flash (<code>gemini-2.0-flash-exp</code>)</li> <li>Qwen 2.5 Coder 32B Instruct (<code>qwen2p5-coder-32b-instruct</code>)</li> <li>DeepSeek V3 (<code>deepseek-v3</code>)</li> <li>DeepSeek R1 (<code>deepseek-r1</code>)</li> </ul> <p>The evaluation uses two datasets:</p> <ol> <li>m-ric/agents_medium_benchmark_2</li> <li>m-ric/smol_agents_benchmark</li> </ol> <p>Both datasets were created by the smolagents team at \ud83e\udd17 Hugging Face and contain curated tasks from GAIA, GSM8K, SimpleQA, and MATH. We selected these datasets primarily for a quick evaluation of relative performance between models in a <code>freeact</code> setup, with the additional benefit of enabling comparisons with smolagents. To ensure fair comparisons with their published results, we used identical evaluation protocols and tools.</p> <p></p> <p>When comparing our results with smolagents using Claude 3.5 Sonnet on m-ric/agents_medium_benchmark_2 (only dataset with available smolagents reference data), we observed the following outcomes (evaluation conducted on 2025-01-07):</p> <p></p> <p>Interestingly, these results were achieved using zero-shot prompting in <code>freeact</code>, while the smolagents implementation utilizes few-shot prompting. You can find all evaluation details here.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#python-package","title":"Python package","text":"<pre><code>pip install freeact\n</code></pre>"},{"location":"installation/#development-installation","title":"Development installation","text":"<p>The development installation is described in the Development Guide.</p>"},{"location":"installation/#execution-environment","title":"Execution environment","text":"<p>For creating custom execution environments with your own dependency requirements, see Execution environment.</p>"},{"location":"integration/","title":"Model integration","text":"<p><code>freeact</code> provides both a low-level and high-level API for integrating new models.</p> <ul> <li>The low-level API defines the <code>CodeActModel</code> interface and related abstractions</li> <li>The high-level API provides a <code>GenericModel</code> class based on the OpenAI Python SDK</li> </ul>"},{"location":"integration/#low-level-api","title":"Low-level API","text":"<p>The low-level API is not further described here. For implementation examples, see the <code>freeact.model.claude</code> or <code>freeact.model.gemini</code> packages.</p>"},{"location":"integration/#high-level-api","title":"High-level API","text":"<p>The high-level API supports usage of models from any provider that is compatible with the OpenAI Python SDK. To use a model, you need to provide prompt templates that guide it to generate code actions. You can either reuse existing templates or create your own.</p> <p>The following subsections demonstrate this using Qwen 2.5 Coder 32B Instruct as an example, showing how to use it both via the Hugging Face Inference API and locally with ollama.</p>"},{"location":"integration/#prompt-templates","title":"Prompt templates","text":"<p>Start with model-specific prompt templates that guide Qwen 2.5 Coder Instruct models to generate code actions. For example:</p> freeact/model/qwen/prompt.py<pre><code>SYSTEM_TEMPLATE = \"\"\"You are a Python coding expert and ReAct agent that acts by writing executable code.\nAt each step I execute the code that you wrote in an IPython notebook and send you the execution result.\nThen continue with the next step by reasoning and writing executable code until you have a final answer.\nThe final answer must be in plain text or markdown (exclude code and exclude latex).\n\nYou can use any Python package from pypi.org and install it with !pip install ...\nAdditionally, you can also use modules defined in the following &lt;python-modules&gt; tags:\n\n&lt;python-modules&gt;\n{python_modules}\n&lt;/python-modules&gt;\n\nImportant: import these &lt;python-modules&gt; before using them.\n\"\"\"\n\nEXECUTION_OUTPUT_TEMPLATE = \"\"\"Here are the execution results of the code you generated:\n\n&lt;execution-results&gt;\n{execution_feedback}\n&lt;/execution-results&gt;\n\nProceed with the next step or respond with a final answer to the user question if you have sufficient information.\n\"\"\"\n\n\nEXECUTION_ERROR_TEMPLATE = \"\"\"The code you generated produced an error during execution:\n\n&lt;execution-error&gt;\n{execution_feedback}\n&lt;/execution-error&gt;\n\nTry to fix the error and continue answering the user question.\n\"\"\"\n</code></pre> <p>Tip</p> <p>While tested with Qwen 2.5 Coder Instruct, these prompt templates can also serve as a good starting point for other models (as we did for DeepSeek V3, for example).</p>"},{"location":"integration/#model-definition","title":"Model definition","text":"<p>Although we could instantiate <code>GenericModel</code> directly with these prompt templates, <code>freeact</code> provides a <code>QwenCoder</code> subclass for convenience:</p> freeact/model/qwen/model.py<pre><code>import os\nfrom typing import Any, Dict\n\nfrom freeact.model.generic.model import GenericModel\nfrom freeact.model.qwen.prompt import (\n    EXECUTION_ERROR_TEMPLATE,\n    EXECUTION_OUTPUT_TEMPLATE,\n    SYSTEM_TEMPLATE,\n)\n\n\nclass QwenCoder(GenericModel):\n    \"\"\"A specialized implementation of `GenericModel` for Qwen's Coder models.\n\n    This class configures `GenericModel` specifically for use with Qwen 2.5 Coder models,\n    It has been tested with *QwenCoder 2.5 Coder 32B Instruct*. Smaller models\n    in this series may require adjustments to the prompt templates.\n\n    Args:\n        model_name: The provider-specific name of the Qwen model to use.\n        api_key: Optional API key for Qwen. If not provided, reads from QWEN_API_KEY environment variable.\n        base_url: Optional base URL for the API. If not provided, reads from QWEN_BASE_URL environment variable.\n        skill_sources: Optional string containing Python skill module information to include in system template.\n        system_template: Prompt template for the system message that guides the model to generate code actions.\n            Must define a `{python_modules}` placeholder for the skill sources.\n        execution_output_template: Prompt template for formatting execution outputs.\n            Must define an `{execution_feedback}` placeholder.\n        execution_error_template: Prompt template for formatting execution errors.\n            Must define an `{execution_feedback}` placeholder.\n        run_kwargs: Optional dictionary of additional arguments passed to the model's\n            `request` and `feedback` methods. Defaults to a stop sequence that prevents\n            the model from guessing code execution outputs.\n        **kwargs: Additional keyword arguments passed to the `GenericModel` constructor.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        api_key: str | None = None,\n        base_url: str | None = None,\n        skill_sources: str | None = None,\n        system_template: str = SYSTEM_TEMPLATE,\n        execution_output_template: str = EXECUTION_OUTPUT_TEMPLATE,\n        execution_error_template: str = EXECUTION_ERROR_TEMPLATE,\n        run_kwargs: Dict[str, Any] | None = {\"stop\": [\"```output\"]},\n        **kwargs,\n    ):\n        super().__init__(\n            model_name=model_name,\n            api_key=api_key or os.getenv(\"QWEN_API_KEY\"),\n            base_url=base_url or os.getenv(\"QWEN_BASE_URL\"),\n            execution_output_template=execution_output_template,\n            execution_error_template=execution_error_template,\n            system_message=system_template.format(python_modules=skill_sources or \"\"),\n            run_kwargs=run_kwargs,\n            **kwargs,\n        )\n</code></pre>"},{"location":"integration/#model-usage","title":"Model usage","text":"<p>Here's a Python example that uses <code>QwenCoder</code> as code action model in a <code>freeact</code> agent. The model is accessed via the Hugging Face Inference API:</p> examples/qwen.py<pre><code>import asyncio\nimport os\n\nfrom rich.console import Console\n\nfrom freeact import CodeActAgent, QwenCoder, execution_environment\nfrom freeact.cli.utils import stream_conversation\n\n\nasync def main():\n    async with execution_environment(\n        ipybox_tag=\"ghcr.io/gradion-ai/ipybox:basic\",\n    ) as env:\n        skill_sources = await env.executor.get_module_sources(\n            module_names=[\"freeact_skills.search.google.stream.api\"],\n        )\n\n        model = QwenCoder(\n            model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n            base_url=\"https://api-inference.huggingface.co/v1/\",\n            api_key=os.environ.get(\"HF_TOKEN\"),  # (1)!\n            skill_sources=skill_sources,\n        )\n\n        agent = CodeActAgent(model=model, executor=env.executor)\n        await stream_conversation(agent, console=Console())  # (2)!\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <ol> <li> <p>Your Hugging Face user access token</p> </li> <li> <p>Interact with the agent via a CLI</p> </li> </ol> <p>Run it with:</p> <pre><code>HF_TOKEN=... python -m freeact.examples.qwen\n</code></pre> <p>Alternatively, use the <code>freeact</code> CLI directly:</p> <pre><code>python -m freeact.cli \\\n  --model-name=Qwen/Qwen2.5-Coder-32B-Instruct \\\n  --base-url=https://api-inference.huggingface.co/v1/ \\\n  --api-key=$HF_TOKEN \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:basic \\\n  --skill-modules=freeact_skills.search.google.stream.api\n</code></pre> <p>For using the same model deployed locally with ollama, modify <code>--model-name</code>, <code>--base-url</code> and <code>--api-key</code> to match your local deployment:</p> <pre><code>python -m freeact.cli \\\n  --model-name=qwen2.5-coder:32b-instruct-fp16 \\\n  --base-url=http://localhost:11434/v1 \\\n  --api-key=ollama \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:basic \\\n  --skill-modules=freeact_skills.search.google.stream.api\n</code></pre>"},{"location":"models/","title":"Supported models","text":"<p>For the following models, <code>freeact</code> provides model-specific prompt templates.</p> Model Release Evaluation Prompt Claude 3.5 Sonnet 2024-10-22 \u2713 optimized Claude 3.5 Haiku 2024-10-22 \u2713 optimized Gemini 2.0 Flash 2024-02-05 \u2713<sup>1</sup> draft Gemini 2.0 Flash Thinking 2024-02-05 \u2717 experimental Qwen 2.5 Coder 32B Instruct \u2713 draft DeepSeek V3 \u2713 draft DeepSeek R1<sup>2</sup> \u2713 experimental <p>Info</p> <p><code>freeact</code> additionally supports the integration of new models from any provider that is compatible with the OpenAI Python SDK, including open models deployed locally with ollama or TGI, for example.</p>"},{"location":"models/#command-line","title":"Command line","text":"<p>This section demonstrates how you can launch <code>freeact</code> agents with these models from the command line. All agents use the predefined <code>freeact_skills.search.google.stream.api</code> skill module for generative Google search. The required Gemini API key for that skill must be defined in a <code>.env</code> file in the current working directory:</p> .env<pre><code># Required for `freeact_skills.search.google.stream.api`\nGOOGLE_API_KEY=...\n</code></pre> <p>API keys and base URLs for code action models are provided as <code>--api-key</code> and <code>--base-url</code> arguments, respectively. Code actions are executed in a Docker container created from the prebuilt <code>ghcr.io/gradion-ai/ipybox:basic</code> image, passed as <code>--ipybox-tag</code> argument.</p> <p>Info</p> <p>The CLI documentation covers more details how environment variables can be passed to <code>freeact</code> agent systems.</p>"},{"location":"models/#claude-35-sonnet","title":"Claude 3.5 Sonnet","text":"<pre><code>python -m freeact.cli \\\n  --model-name=claude-3-5-sonnet-20241022 \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:basic \\\n  --skill-modules=freeact_skills.search.google.stream.api \\\n  --api-key=$ANTHROPIC_API_KEY\n</code></pre>"},{"location":"models/#claude-35-haiku","title":"Claude 3.5 Haiku","text":"<pre><code>python -m freeact.cli \\\n  --model-name=claude-3-5-haiku-20241022 \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:basic \\\n  --skill-modules=freeact_skills.search.google.stream.api \\\n  --api-key=$ANTHROPIC_API_KEY\n</code></pre>"},{"location":"models/#gemini-20-flash","title":"Gemini 2.0 Flash","text":"<pre><code>python -m freeact.cli \\\n  --model-name=gemini-2.0-flash \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:basic \\\n  --skill-modules=freeact_skills.search.google.stream.api \\\n  --api-key=$GOOGLE_API_KEY\n</code></pre>"},{"location":"models/#gemini-20-flash-thinking","title":"Gemini 2.0 Flash Thinking","text":"<pre><code>python -m freeact.cli \\\n  --model-name=gemini-2.0-flash-thinking-exp \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:basic \\\n  --skill-modules=freeact_skills.search.google.stream.api \\\n  --api-key=$GOOGLE_API_KEY\n</code></pre>"},{"location":"models/#qwen-25-coder-32b-instruct","title":"Qwen 2.5 Coder 32B Instruct","text":"<pre><code>python -m freeact.cli \\\n  --model-name=Qwen/Qwen2.5-Coder-32B-Instruct \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:basic \\\n  --skill-modules=freeact_skills.search.google.stream.api \\\n  --base-url=https://api-inference.huggingface.co/v1/ \\\n  --api-key=$HF_TOKEN\n</code></pre>"},{"location":"models/#deepseek-r1","title":"DeepSeek R1","text":"<pre><code>python -m freeact.cli \\\n  --model-name=accounts/fireworks/models/deepseek-r1 \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:basic \\\n  --skill-modules=freeact_skills.search.google.stream.api \\\n  --base-url=https://api.fireworks.ai/inference/v1 \\\n  --api-key=$FIREWORKS_API_KEY\n</code></pre>"},{"location":"models/#deepseek-v3","title":"DeepSeek V3","text":"<pre><code>python -m freeact.cli \\\n  --model-name=accounts/fireworks/models/deepseek-v3 \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:basic \\\n  --skill-modules=freeact_skills.search.google.stream.api \\\n  --base-url=https://api.fireworks.ai/inference/v1 \\\n  --api-key=$FIREWORKS_API_KEY\n</code></pre> <ol> <li> <p>We evaluated Gemini 2.0 Flash Experimental (<code>gemini-2.0-flash-exp</code>), released on 2024-12-11.\u00a0\u21a9</p> </li> <li> <p>DeepSeek R1 wasn't trained on agentic tool use but demonstrates strong performance with code actions, even surpassing Claude 3.5 Sonnet on the GAIA subset in our evaluation. See this article for further details.\u00a0\u21a9</p> </li> </ol>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Install <code>freeact</code> using pip:</p> <pre><code>pip install freeact\n</code></pre> <p>Create a <code>.env</code> file with Anthropic and Gemini API keys:</p> .env<pre><code># Required for Claude 3.5 Sonnet\nANTHROPIC_API_KEY=...\n\n# Required for generative Google Search via Gemini 2\nGOOGLE_API_KEY=...\n</code></pre> <p>Launch a <code>freeact</code> agent with generative Google Search skill using the CLI:</p> <pre><code>python -m freeact.cli \\\n  --model-name=claude-3-5-sonnet-20241022 \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:basic \\\n  --skill-modules=freeact_skills.search.google.stream.api\n</code></pre> <p>or an equivalent Python script:</p> examples/quickstart.py<pre><code>import asyncio\n\nfrom rich.console import Console\n\nfrom freeact import Claude, CodeActAgent, execution_environment\nfrom freeact.cli.utils import stream_conversation\n\n\nasync def main():\n    async with execution_environment(\n        ipybox_tag=\"ghcr.io/gradion-ai/ipybox:basic\",\n    ) as env:\n        skill_sources = await env.executor.get_module_sources(\n            module_names=[\"freeact_skills.search.google.stream.api\"],\n        )\n\n        model = Claude(model_name=\"claude-3-5-sonnet-20241022\", logger=env.logger)\n        agent = CodeActAgent(model=model, executor=env.executor)\n        await stream_conversation(agent, console=Console(), skill_sources=skill_sources)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Once launched, you can start interacting with the agent:</p>    Your browser does not support the video tag."},{"location":"streaming/","title":"Streaming protocol","text":"<p><code>freeact</code> implements an end-to-end streaming protocol for user-agent communication. The diagram below illustrates a simplified version of this protocol. For details, refer to:</p> <ul> <li><code>CodeActAgent</code> and <code>CodeActAgentTurn</code></li> <li><code>CodeActModel</code> and <code>CodeActModelTurn</code></li> <li><code>CodeExecutor</code> and <code>CodeExecution</code></li> </ul> <p> </p> Streaming protocol for user-agent communication (simplified)"},{"location":"api/agent/","title":"Agent","text":""},{"location":"api/agent/#freeact.agent.CodeActAgent","title":"CodeActAgent","text":"<pre><code>CodeActAgent(model: CodeActModel, executor: CodeExecutor)\n</code></pre> <p>An agent that iteratively generates and executes code actions to process user queries.</p> <p>The agent implements a loop that:</p> <ol> <li>Generates code actions using a <code>CodeActModel</code></li> <li>Executes the code using a <code>CodeExecutor</code></li> <li>Provides execution feedback to the <code>CodeActModel</code></li> <li>Continues until the model generates a final response</li> </ol> <p>The agent maintains conversational state and can have multiple interaction turns with the user.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>CodeActModel</code> <p>Model instance for generating code actions</p> required <code>executor</code> <code>CodeExecutor</code> <p>Executor instance for running the generated code</p> required Source code in <code>freeact/agent.py</code> <pre><code>def __init__(self, model: CodeActModel, executor: CodeExecutor):\n    self.model = model\n    self.executor = executor\n</code></pre>"},{"location":"api/agent/#freeact.agent.CodeActAgent.run","title":"run","text":"<pre><code>run(user_query: str, max_steps: int = 30, step_timeout: float = 120, **kwargs) -&gt; CodeActAgentTurn\n</code></pre> <p>Process a user query through a sequence of model interactions and code executions.</p> <p>Initiates an interaction turn that processes the user query through alternating steps of code action model interactions and code execution until a final response is generated by the model.</p> <p>Parameters:</p> Name Type Description Default <code>user_query</code> <code>str</code> <p>The input query from the user to process</p> required <code>max_steps</code> <code>int</code> <p>Maximum number of interaction steps before raising <code>MaxStepsReached</code></p> <code>30</code> <code>step_timeout</code> <code>float</code> <p>Timeout in seconds for each code execution step</p> <code>120</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the model</p> <code>{}</code> <p>Returns:</p> Type Description <code>CodeActAgentTurn</code> <p>A <code>CodeActAgentTurn</code> instance representing the complete interaction sequence</p> <p>Raises:</p> Type Description <code>MaxStepsReached</code> <p>If the interaction exceeds max_steps without completion</p> Source code in <code>freeact/agent.py</code> <pre><code>def run(\n    self,\n    user_query: str,\n    max_steps: int = 30,\n    step_timeout: float = 120,\n    **kwargs,\n) -&gt; CodeActAgentTurn:\n    \"\"\"Process a user query through a sequence of model interactions and code executions.\n\n    Initiates an interaction turn that processes the user query through alternating\n    steps of code action model interactions and code execution until a final response\n    is generated by the model.\n\n    Args:\n        user_query: The input query from the user to process\n        max_steps: Maximum number of interaction steps before raising `MaxStepsReached`\n        step_timeout: Timeout in seconds for each code execution step\n        **kwargs: Additional keyword arguments passed to the model\n\n    Returns:\n        A `CodeActAgentTurn` instance representing the complete interaction sequence\n\n    Raises:\n        MaxStepsReached: If the interaction exceeds max_steps without completion\n    \"\"\"\n    iter = self._stream(\n        user_query=user_query,\n        max_steps=max_steps,\n        step_timeout=step_timeout,\n        **kwargs,\n    )\n    return CodeActAgentTurn(iter)\n</code></pre>"},{"location":"api/agent/#freeact.agent.CodeActAgentResponse","title":"CodeActAgentResponse  <code>dataclass</code>","text":"<pre><code>CodeActAgentResponse(text: str)\n</code></pre> <p>Final response from the agent to the user for the current turn.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>str</code> <p>The final response text to present to the user</p>"},{"location":"api/agent/#freeact.agent.CodeActAgentTurn","title":"CodeActAgentTurn","text":"<pre><code>CodeActAgentTurn(iter: AsyncIterator[CodeActModelTurn | CodeExecution | CodeActModelResponse])\n</code></pre> <p>Represents a complete interaction turn between the user and agent.</p> <p>A turn consists of a sequence of model interaction turns and code executions, continuing until:</p> <ul> <li>The model provides a final response without code</li> <li>An error occurs</li> <li>Maximum steps are reached</li> </ul> <p>The turn can be processed either in bulk via <code>response()</code> or incrementally via <code>stream()</code>.</p> Source code in <code>freeact/agent.py</code> <pre><code>def __init__(self, iter: AsyncIterator[CodeActModelTurn | CodeExecution | CodeActModelResponse]):\n    self._iter = iter\n    self._response: CodeActAgentResponse | None = None\n</code></pre>"},{"location":"api/agent/#freeact.agent.CodeActAgentTurn.response","title":"response  <code>async</code>","text":"<pre><code>response() -&gt; CodeActAgentResponse\n</code></pre> <p>Get the final response for this interaction turn.</p> <p>Waits for the complete interaction sequence to finish, including any intermediate model interaction and code executions. The final response is cached after the first call.</p> <p>Returns:</p> Type Description <code>CodeActAgentResponse</code> <p>The final agent response containing the text to present to the user</p> Note <p>This method will process the entire interaction sequence if called before streaming is complete. For incremental processing, use the <code>stream()</code> method instead.</p> Source code in <code>freeact/agent.py</code> <pre><code>async def response(self) -&gt; CodeActAgentResponse:\n    \"\"\"Get the final response for this interaction turn.\n\n    Waits for the complete interaction sequence to finish, including any\n    intermediate model interaction and code executions. The final response\n    is cached after the first call.\n\n    Returns:\n        The final agent response containing the text to present to the user\n\n    Note:\n        This method will process the entire interaction sequence if called\n        before streaming is complete. For incremental processing, use the\n        `stream()` method instead.\n    \"\"\"\n    if self._response is None:\n        async for _ in self.stream():\n            pass\n    return self._response  # type: ignore\n</code></pre>"},{"location":"api/agent/#freeact.agent.CodeActAgentTurn.stream","title":"stream  <code>async</code>","text":"<pre><code>stream() -&gt; AsyncIterator[CodeActModelTurn | CodeExecution]\n</code></pre> <p>Stream the sequence of model turns and code executions.</p> <p>Yields each step in the interaction sequence as it occurs:</p> <ul> <li><code>CodeActModelTurn</code>: Model thinking and code action generation steps</li> <li><code>CodeExecution</code>: Code actions being executed in the execution environment</li> </ul> <p>The sequence continues until the model provides a final response, which is stored internally but not yielded.</p> <p>Yields:</p> Type Description <code>AsyncIterator[CodeActModelTurn | CodeExecution]</code> <p>Individual model turns and code executions in sequence</p> Note <p>The final <code>CodeActModelResponse</code> is not yielded but is stored internally and can be accessed via the <code>response()</code> method.</p> Source code in <code>freeact/agent.py</code> <pre><code>async def stream(self) -&gt; AsyncIterator[CodeActModelTurn | CodeExecution]:\n    \"\"\"Stream the sequence of model turns and code executions.\n\n    Yields each step in the interaction sequence as it occurs:\n\n    - `CodeActModelTurn`: Model thinking and code action generation steps\n    - `CodeExecution`: Code actions being executed in the execution environment\n\n    The sequence continues until the model provides a final response,\n    which is stored internally but not yielded.\n\n    Yields:\n        Individual model turns and code executions in sequence\n\n    Note:\n        The final `CodeActModelResponse` is not yielded but is stored\n        internally and can be accessed via the `response()` method.\n    \"\"\"\n    async for elem in self._iter:\n        match elem:\n            case CodeActModelResponse() as msg:\n                self._response = CodeActAgentResponse(text=msg.text)\n            case _:\n                yield elem\n</code></pre>"},{"location":"api/agent/#freeact.agent.CodeExecution","title":"CodeExecution","text":"<pre><code>CodeExecution(execution: Execution, images_dir: Path)\n</code></pre> <p>Represents a code execution in a <code>CodeExecutor</code> instance.</p> <p>Supports both bulk and streaming access to results generated by the executor.</p> <p>Attributes:</p> Name Type Description <code>execution</code> <p>The underlying <code>ipybox</code> execution instance</p> <code>images_dir</code> <p>Directory where generated images are saved</p> Source code in <code>freeact/agent.py</code> <pre><code>def __init__(self, execution: Execution, images_dir: Path):\n    self.execution = execution\n    self.images_dir = images_dir\n    self._result: CodeExecutionResult | None = None\n</code></pre>"},{"location":"api/agent/#freeact.agent.CodeExecution.result","title":"result  <code>async</code>","text":"<pre><code>result(timeout: float = 120) -&gt; CodeExecutionResult\n</code></pre> <p>Get the complete result of the code execution.</p> <p>Waits for the execution to finish and returns a <code>CodeExecutionResult</code> containing all output, generated images, and error status. The result is cached after the first call.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Maximum time in seconds to wait for execution completion</p> <code>120</code> <p>Returns:</p> Type Description <code>CodeExecutionResult</code> <p>A <code>CodeExecutionResult</code> containing the execution output, images, and error status</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If execution exceeds the specified timeout</p> Source code in <code>freeact/agent.py</code> <pre><code>async def result(self, timeout: float = 120) -&gt; CodeExecutionResult:\n    \"\"\"Get the complete result of the code execution.\n\n    Waits for the execution to finish and returns a `CodeExecutionResult` containing\n    all output, generated images, and error status. The result is cached after\n    the first call.\n\n    Args:\n        timeout: Maximum time in seconds to wait for execution completion\n\n    Returns:\n        A `CodeExecutionResult` containing the execution output, images, and error status\n\n    Raises:\n        TimeoutError: If execution exceeds the specified timeout\n    \"\"\"\n    if self._result is None:\n        async for _ in self.stream(timeout=timeout):\n            pass\n    return self._result  # type: ignore\n</code></pre>"},{"location":"api/agent/#freeact.agent.CodeExecution.stream","title":"stream  <code>async</code>","text":"<pre><code>stream(timeout: float = 120) -&gt; AsyncIterator[str]\n</code></pre> <p>Stream the execution output as it becomes available.</p> <p>Yields chunks of output text as they are produced by the execution. Generated images are not part of the stream but are stored internally in <code>CodeExecutionResult</code> which can be obtained by calling the <code>result()</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Maximum time in seconds to wait for execution completion</p> <code>120</code> <p>Yields:</p> Type Description <code>AsyncIterator[str]</code> <p>Chunks of code execution output text</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If execution exceeds the specified timeout</p> Source code in <code>freeact/agent.py</code> <pre><code>async def stream(self, timeout: float = 120) -&gt; AsyncIterator[str]:\n    \"\"\"Stream the execution output as it becomes available.\n\n    Yields chunks of output text as they are produced by the execution. Generated\n    images are not part of the stream but are stored internally in `CodeExecutionResult`\n    which can be obtained by calling the `result()` method.\n\n    Args:\n        timeout: Maximum time in seconds to wait for execution completion\n\n    Yields:\n        Chunks of code execution output text\n\n    Raises:\n        TimeoutError: If execution exceeds the specified timeout\n    \"\"\"\n    images = {}\n\n    try:\n        async for chunk in self.execution.stream(timeout=timeout):\n            yield chunk\n    except ExecutionError as e:\n        is_error = True\n        text = e.trace\n        yield text\n    except asyncio.TimeoutError:\n        is_error = True\n        text = \"Execution timed out\"\n        yield text\n    else:\n        result = await self.execution.result()\n        text = result.text\n        is_error = False\n\n        if result.images:\n            chunk = \"\\n\\nProduced images:\"\n            yield chunk\n            text += chunk\n\n        for i, image in enumerate(result.images):\n            path = await self._save_image(image)\n            chunk = f\"\\n![image_{i}]({path})\"\n            yield chunk\n            text += chunk\n            images[path] = image\n\n    self._result = CodeExecutionResult(text=text, images=images, is_error=is_error)\n</code></pre>"},{"location":"api/agent/#freeact.agent.CodeExecutionResult","title":"CodeExecutionResult  <code>dataclass</code>","text":"<pre><code>CodeExecutionResult(text: str, images: Dict[Path, Image], is_error: bool)\n</code></pre> <p>Result of executing code in a <code>CodeExecutor</code> instance.</p> <p>Stores the execution output, any generated images, and error status from running code in the execution environment.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Execution output text or error trace</p> required <code>images</code> <code>Dict[Path, Image]</code> <p>Dictionary mapping file paths to generated images</p> required <code>is_error</code> <code>bool</code> <p>Whether the execution resulted in an error</p> required"},{"location":"api/agent/#freeact.agent.MaxStepsReached","title":"MaxStepsReached","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the maximum number of steps per user query is reached.</p> <p>This exception indicates that the agent has reached its maximum allowed interaction steps while processing a user query.</p>"},{"location":"api/claude/","title":"Claude","text":""},{"location":"api/claude/#freeact.model.claude.model.ClaudeModelName","title":"ClaudeModelName  <code>module-attribute</code>","text":"<pre><code>ClaudeModelName = Literal['claude-3-5-haiku-20241022', 'claude-3-5-sonnet-20241022']\n</code></pre>"},{"location":"api/claude/#freeact.model.claude.model.Claude","title":"Claude","text":"<pre><code>Claude(logger: Logger, model_name: ClaudeModelName, prompt_caching: bool = False, system_extension: str | None = None, system_message: str | None = None, retry_max_attempts: int = 10, retry_wait_strategy: WaitStrategy = WaitExponential(multiplier=1, max=10, exp_base=2), **kwargs)\n</code></pre> <p>               Bases: <code>CodeActModel</code></p> <p>A <code>CodeActModel</code> implementation based on Anthropic's Claude API.</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger</code> <p>Logger instance for logging requests and responses.</p> required <code>model_name</code> <code>ClaudeModelName</code> <p>Name of the Claude model to use (e.g., \"claude-3-5-sonnet-20241022\").</p> required <code>prompt_caching</code> <code>bool</code> <p>Whether to enable prompt caching. Defaults to False.</p> <code>False</code> <code>system_extension</code> <code>str | None</code> <p>Additional system prompt text. Defaults to None.</p> <code>None</code> <code>system_message</code> <code>str | None</code> <p>Complete system message to override default. Defaults to None.</p> <code>None</code> <code>retry_max_attempts</code> <code>int</code> <p>Maximum number of retry attempts. Defaults to 10.</p> <code>10</code> <code>retry_wait_strategy</code> <code>WaitStrategy</code> <p>Wait strategy for retrying requests. Defaults to exponential backoff.</p> <code>WaitExponential(multiplier=1, max=10, exp_base=2)</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the Anthropic client.</p> <code>{}</code> Source code in <code>freeact/model/claude/model.py</code> <pre><code>def __init__(\n    self,\n    logger: Logger,\n    model_name: ClaudeModelName,\n    prompt_caching: bool = False,\n    system_extension: str | None = None,\n    system_message: str | None = None,\n    retry_max_attempts: int = 10,\n    retry_wait_strategy: WaitStrategy = WaitExponential(multiplier=1, max=10, exp_base=2),\n    **kwargs,\n):\n    if system_message and system_extension:\n        raise ValueError(\"If system_message is provided, system_extension must be None\")\n\n    if system_message:\n        self.system_message = system_message\n    else:\n        self.system_message = SYSTEM_TEMPLATE.format(extensions=system_extension or \"\")\n\n    self.logger = logger\n    self.model_name = model_name\n    self.prompt_caching = prompt_caching\n\n    self._history = []  # type: ignore\n    self._tool_names = [t[\"name\"] for t in TOOLS]\n\n    self._client = AsyncAnthropic(\n        default_headers={\n            \"anthropic-beta\": \"prompt-caching-2024-07-31\",\n        }\n        if prompt_caching\n        else None,\n        **kwargs,\n    )\n    self._retry_max_attempts = retry_max_attempts\n    self._retry_wait_strategy = retry_wait_strategy\n</code></pre>"},{"location":"api/deepseek/","title":"DeepSeek","text":""},{"location":"api/deepseek/#freeact.model.deepseek.model.DeepSeekV3","title":"DeepSeekV3","text":"<pre><code>DeepSeekV3(model_name: str, api_key: str | None = None, base_url: str | None = None, skill_sources: str | None = None, system_extension: str | None = None, system_template: str = SYSTEM_TEMPLATE, execution_output_template: str = EXECUTION_OUTPUT_TEMPLATE, execution_error_template: str = EXECUTION_ERROR_TEMPLATE, run_kwargs: Dict[str, Any] | None = None, **kwargs)\n</code></pre> <p>               Bases: <code>GenericModel</code></p> <p>A specialized implementation of <code>GenericModel</code> for DeepSeek V3.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The provider-specific name of the DeepSeek model to use.</p> required <code>api_key</code> <code>str | None</code> <p>Optional API key for DeepSeek. If not provided, reads from DEEPSEEK_API_KEY environment variable.</p> <code>None</code> <code>base_url</code> <code>str | None</code> <p>Optional base URL for the API. If not provided, reads from DEEPSEEK_BASE_URL environment variable.</p> <code>None</code> <code>skill_sources</code> <code>str | None</code> <p>Optional string containing Python skill module information to include in system template.</p> <code>None</code> <code>system_extension</code> <code>str | None</code> <p>System message extension for domain- or environment-specific instructions.</p> <code>None</code> <code>system_template</code> <code>str</code> <p>Prompt template for the system message that guides the model to generate code actions. Must define a <code>{python_modules}</code> placeholder for the <code>skill_sources</code>.</p> <code>SYSTEM_TEMPLATE</code> <code>execution_output_template</code> <code>str</code> <p>Prompt template for formatting execution outputs. Must define an <code>{execution_feedback}</code> placeholder.</p> <code>EXECUTION_OUTPUT_TEMPLATE</code> <code>execution_error_template</code> <code>str</code> <p>Prompt template for formatting execution errors. Must define an <code>{execution_feedback}</code> placeholder.</p> <code>EXECUTION_ERROR_TEMPLATE</code> <code>run_kwargs</code> <code>Dict[str, Any] | None</code> <p>Optional dictionary of additional arguments passed to the model's <code>request</code> and <code>feedback</code> methods.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the <code>GenericModel</code> constructor.</p> <code>{}</code> Source code in <code>freeact/model/deepseek/model.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    api_key: str | None = None,\n    base_url: str | None = None,\n    skill_sources: str | None = None,\n    system_extension: str | None = None,\n    system_template: str = v3.SYSTEM_TEMPLATE,\n    execution_output_template: str = v3.EXECUTION_OUTPUT_TEMPLATE,\n    execution_error_template: str = v3.EXECUTION_ERROR_TEMPLATE,\n    run_kwargs: Dict[str, Any] | None = None,\n    **kwargs,\n):\n    format_kwargs = {\n        \"python_modules\": skill_sources or \"\",\n    }\n\n    if \"{extensions}\" in system_template:\n        format_kwargs[\"extensions\"] = system_extension or \"\"\n\n    super().__init__(\n        model_name=model_name,\n        api_key=api_key or os.getenv(\"DEEPSEEK_API_KEY\"),\n        base_url=base_url or os.getenv(\"DEEPSEEK_BASE_URL\"),\n        execution_output_template=execution_output_template,\n        execution_error_template=execution_error_template,\n        system_message=system_template.format(**format_kwargs),\n        run_kwargs=run_kwargs,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/deepseek/#freeact.model.deepseek.model.DeepSeekR1","title":"DeepSeekR1","text":"<pre><code>DeepSeekR1(model_name: str, api_key: str | None = None, base_url: str | None = None, skill_sources: str | None = None, instruction_extension: str | None = EXAMPLE_EXTENSION, instruction_template: str = INSTRUCTION_TEMPLATE, execution_output_template: str = EXECUTION_OUTPUT_TEMPLATE, execution_error_template: str = EXECUTION_ERROR_TEMPLATE, run_kwargs: Dict[str, Any] | None = {'temperature': 0.6, 'max_tokens': 8192}, **kwargs)\n</code></pre> <p>               Bases: <code>GenericModel</code></p> <p>A specialized implementation of <code>GenericModel</code> for DeepSeek R1.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The provider-specific name of the DeepSeek model to use.</p> required <code>api_key</code> <code>str | None</code> <p>Optional API key for DeepSeek. If not provided, reads from DEEPSEEK_API_KEY environment variable.</p> <code>None</code> <code>base_url</code> <code>str | None</code> <p>Optional base URL for the API. If not provided, reads from DEEPSEEK_BASE_URL environment variable.</p> <code>None</code> <code>skill_sources</code> <code>str | None</code> <p>Optional string containing Python skill module information to include in system template.</p> <code>None</code> <code>instruction_extension</code> <code>str | None</code> <p>Domain- or environment-specific extensions to the <code>instruction_template</code>.</p> <code>EXAMPLE_EXTENSION</code> <code>instruction_template</code> <code>str</code> <p>Prompt template that guides the model to generate code actions. Must define a <code>{user_query}</code> placeholder for the user query, an <code>{extensions}</code> placeholder for the <code>instruction_extension</code> and a <code>{python_modules}</code> placeholder for the <code>skill_sources</code>.</p> <code>INSTRUCTION_TEMPLATE</code> <code>execution_output_template</code> <code>str</code> <p>Prompt template for formatting execution outputs. Must define an <code>{execution_feedback}</code> placeholder.</p> <code>EXECUTION_OUTPUT_TEMPLATE</code> <code>execution_error_template</code> <code>str</code> <p>Prompt template for formatting execution errors. Must define an <code>{execution_feedback}</code> placeholder.</p> <code>EXECUTION_ERROR_TEMPLATE</code> <code>run_kwargs</code> <code>Dict[str, Any] | None</code> <p>Optional dictionary of additional arguments passed to the model's <code>request</code> and <code>feedback</code> methods.</p> <code>{'temperature': 0.6, 'max_tokens': 8192}</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the <code>GenericModel</code> constructor.</p> <code>{}</code> Source code in <code>freeact/model/deepseek/model.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    api_key: str | None = None,\n    base_url: str | None = None,\n    skill_sources: str | None = None,\n    instruction_extension: str | None = r1.EXAMPLE_EXTENSION,\n    instruction_template: str = r1.INSTRUCTION_TEMPLATE,\n    execution_output_template: str = r1.EXECUTION_OUTPUT_TEMPLATE,\n    execution_error_template: str = r1.EXECUTION_ERROR_TEMPLATE,\n    run_kwargs: Dict[str, Any] | None = {\n        \"temperature\": 0.6,\n        \"max_tokens\": 8192,\n    },\n    **kwargs,\n):\n    self.instruction_template = instruction_template\n    self.instruction_kwargs = {\n        \"python_modules\": skill_sources or \"\",\n        \"extensions\": instruction_extension or \"\",\n    }\n\n    super().__init__(\n        model_name=model_name,\n        api_key=api_key or os.getenv(\"DEEPSEEK_API_KEY\"),\n        base_url=base_url or os.getenv(\"DEEPSEEK_BASE_URL\"),\n        execution_output_template=execution_output_template,\n        execution_error_template=execution_error_template,\n        system_message=None,\n        run_kwargs=run_kwargs,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/executor/","title":"Executor","text":""},{"location":"api/executor/#freeact.executor.CodeExecutionContainer","title":"CodeExecutionContainer","text":"<pre><code>CodeExecutionContainer(tag: str, env: dict[str, str] | None = None, port: int | None = None, show_pull_progress: bool = True, workspace_path: Path | str | None = None)\n</code></pre> <p>               Bases: <code>ExecutionContainer</code></p> <p>Context manager for managing code execution container lifecycle.</p> <p>Extends <code>ipybox</code>'s <code>ExecutionContainer</code> to provide workspace-specific bind mounts for skill directories. Handles creation, port mapping, volume binding, and cleanup of the container.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>Docker image tag to use for the container</p> required <code>env</code> <code>dict[str, str] | None</code> <p>Optional environment variables to set in the container</p> <code>None</code> <code>port</code> <code>int | None</code> <p>Optional host port to map to container's executor port. Random port used if not specified</p> <code>None</code> <code>show_pull_progress</code> <code>bool</code> <p>Whether to show progress when pulling the Docker image.</p> <code>True</code> <code>workspace_path</code> <code>Path | str | None</code> <p>Optional path to workspace directory, defaults to \"workspace\"</p> <code>None</code> Example <pre><code>async with CodeExecutionContainer(tag=\"gradion-ai/ipybox-example\", workspace_path=Path(\"workspace\")) as container:\n    # Container is running and available at container.port\n    ...\n# Container is automatically cleaned up after context exit\n</code></pre> Source code in <code>freeact/executor.py</code> <pre><code>def __init__(\n    self,\n    tag: str,\n    env: dict[str, str] | None = None,\n    port: int | None = None,\n    show_pull_progress: bool = True,\n    workspace_path: Path | str | None = None,\n):\n    self.workspace = Workspace(Path(workspace_path) if workspace_path else Path(\"workspace\"))\n\n    binds = {\n        self.workspace.private_skills_path: \"skills/private\",\n        self.workspace.shared_skills_path: \"skills/shared\",\n    }\n\n    super().__init__(tag=tag, binds=binds, env=env, port=port, show_pull_progress=show_pull_progress)\n</code></pre>"},{"location":"api/executor/#freeact.executor.CodeExecutionEnvironment","title":"CodeExecutionEnvironment  <code>dataclass</code>","text":"<pre><code>CodeExecutionEnvironment(container: CodeExecutionContainer, executor: CodeExecutor, logger: Logger)\n</code></pre> <p>A convenience class that bundles the container, executor and logger of an execution environment.</p> <p>Attributes:</p> Name Type Description <code>container</code> <code>CodeExecutionContainer</code> <p>Manages a Docker container of the execution environment</p> <code>executor</code> <code>CodeExecutor</code> <p>Manages an IPython kernel running in the container</p> <code>logger</code> <code>Logger</code> <p>Logger instance for recording model interactions</p>"},{"location":"api/executor/#freeact.executor.CodeExecutor","title":"CodeExecutor","text":"<pre><code>CodeExecutor(key: str, workspace: Workspace, *args, **kwargs)\n</code></pre> <p>               Bases: <code>ExecutionClient</code></p> <p>Context manager for executing code in an IPython kernel running in a <code>CodeExecutionContainer</code>.</p> <p>Provides stateful code execution within a container, maintaining kernel state between executions. Manages scoped skill and image storage directories.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Scope identifier used to:</p> <ul> <li>Create scoped private skill directories for an executor instance   (host path <code>{workspace.path}/skills/private/{key}</code>)</li> <li>Create scoped image storage directories for an executor instance   (host-only path <code>{workspace.path}/images/{key}</code>).</li> <li>Set the working directory to the scoped private skill directory</li> </ul> required <code>workspace</code> <code>Workspace</code> <p>a <code>Workspace</code> instance defining the skill directory structure</p> required <code>*args</code> <p>Additional arguments passed to the <code>ExecutionClient</code> constructor</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the <code>ExecutionClient</code> constructor</p> <code>{}</code> Example <pre><code>async with CodeExecutor(key=\"agent-1\", workspace=container.workspace, port=container.port) as executor:\n    # Execute code with access to skill directories\n    await executor.execute(\"print('Hello')\")\n</code></pre> Source code in <code>freeact/executor.py</code> <pre><code>def __init__(self, key: str, workspace: Workspace, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n    self.key = key\n    self.workspace = workspace\n\n    # Host mapping for working directory inside container\n    self.working_dir = workspace.private_skills_path / key\n\n    # images are stored on host only (for now)\n    self.images_dir = workspace.path / \"images\" / key\n</code></pre>"},{"location":"api/executor/#freeact.executor.CodeExecutor.skill_paths","title":"skill_paths  <code>property</code>","text":"<pre><code>skill_paths: List[Path]\n</code></pre> <p>A path list containing the shared skill path and the scoped private skill path (= working directory).</p>"},{"location":"api/executor/#freeact.executor.Workspace","title":"Workspace  <code>dataclass</code>","text":"<pre><code>Workspace(path: Path)\n</code></pre> <p>Represents a workspace containing shared and private agent skills. These are skills that are not pre-installed in the code execution container.</p> <p>Workspaces are bind-mounted into <code>CodeExecutionContainer</code> to share skills between a code execution container and the host machine.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Base path of the workspace directory structure on the host.</p> required"},{"location":"api/executor/#freeact.executor.Workspace.private_skills_path","title":"private_skills_path  <code>property</code>","text":"<pre><code>private_skills_path: Path\n</code></pre> <p>Path to private skills root directory.</p>"},{"location":"api/executor/#freeact.executor.Workspace.shared_skills_path","title":"shared_skills_path  <code>property</code>","text":"<pre><code>shared_skills_path: Path\n</code></pre> <p>Path to shared skills directory.</p>"},{"location":"api/executor/#freeact.executor.dotenv_variables","title":"dotenv_variables","text":"<pre><code>dotenv_variables(dotenv_path: Path | None = Path('.env'), export: bool = True, **kwargs) -&gt; Dict[str, str]\n</code></pre> <p>Load environment variables from a <code>.env</code> file.</p> <p>Reads environment variables from a <code>.env</code> file and optionally exports them to <code>os.environ</code>. If no path is provided, searches for a <code>.env</code> file in parent directories.</p> <p>Parameters:</p> Name Type Description Default <code>dotenv_path</code> <code>Path | None</code> <p>Path to the <code>.env</code> file. Defaults to <code>.env</code> in current directory.</p> <code>Path('.env')</code> <code>export</code> <code>bool</code> <p>Whether to export variables to current environment. Defaults to <code>True</code>.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments passed to <code>DotEnv</code> constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dictionary mapping environment variable names to their values.</p> Source code in <code>freeact/executor.py</code> <pre><code>def dotenv_variables(dotenv_path: Path | None = Path(\".env\"), export: bool = True, **kwargs) -&gt; Dict[str, str]:\n    \"\"\"Load environment variables from a `.env` file.\n\n    Reads environment variables from a `.env` file and optionally exports them to `os.environ`.\n    If no path is provided, searches for a `.env` file in parent directories.\n\n    Args:\n        dotenv_path: Path to the `.env` file. Defaults to `.env` in current directory.\n        export: Whether to export variables to current environment. Defaults to `True`.\n        **kwargs: Additional keyword arguments passed to `DotEnv` constructor.\n\n    Returns:\n        Dictionary mapping environment variable names to their values.\n    \"\"\"\n\n    if dotenv_path is None:\n        dotenv_path = find_dotenv()\n\n    dotenv = DotEnv(dotenv_path=dotenv_path, **kwargs)\n\n    if export:\n        dotenv.set_as_environment_variables()\n\n    return {k: v for k, v in dotenv.dict().items() if v is not None}\n</code></pre>"},{"location":"api/executor/#freeact.executor.execution_environment","title":"execution_environment  <code>async</code>","text":"<pre><code>execution_environment(executor_key: str = 'default', ipybox_tag: str = 'ghcr.io/gradion-ai/ipybox:minimal', env_vars: dict[str, str] = dotenv_variables(), workspace_path: Path | str = Path('workspace'), log_file: Path | str = Path('logs', 'agent.log'))\n</code></pre> <p>Convenience context manager for a local, sandboxed code execution environment.</p> <p>Sets up a complete environment for executing code securely on the local machine, including:</p> <ul> <li>A Docker container based on <code>ipybox</code></li> <li>A code executor connected to the container</li> <li>A logger for recording model interactions</li> </ul> <p>Parameters:</p> Name Type Description Default <code>executor_key</code> <code>str</code> <p>CodeExecutor key for private workspace directories</p> <code>'default'</code> <code>ipybox_tag</code> <code>str</code> <p>Tag of the <code>ipybox</code> Docker image to use</p> <code>'ghcr.io/gradion-ai/ipybox:minimal'</code> <code>env_vars</code> <code>dict[str, str]</code> <p>Environment variables to pass to the container</p> <code>dotenv_variables()</code> <code>workspace_path</code> <code>Path | str</code> <p>Path to workspace directory on host machine</p> <code>Path('workspace')</code> <code>log_file</code> <code>Path | str</code> <p>Path to log file for recording model interactions</p> <code>Path('logs', 'agent.log')</code> Source code in <code>freeact/executor.py</code> <pre><code>@asynccontextmanager\nasync def execution_environment(\n    executor_key: str = \"default\",\n    ipybox_tag: str = \"ghcr.io/gradion-ai/ipybox:minimal\",\n    env_vars: dict[str, str] = dotenv_variables(),\n    workspace_path: Path | str = Path(\"workspace\"),\n    log_file: Path | str = Path(\"logs\", \"agent.log\"),\n):\n    \"\"\"Convenience context manager for a local, sandboxed [code execution environment][freeact.executor.CodeExecutionEnvironment].\n\n    Sets up a complete environment for executing code securely on the local machine, including:\n\n    - A Docker container based on [`ipybox`](https://gradion-ai.github.io/ipybox)\n    - A code executor connected to the container\n    - A logger for recording model interactions\n\n    Args:\n        executor_key: [CodeExecutor][freeact.executor.CodeExecutor] key for private workspace directories\n        ipybox_tag: Tag of the `ipybox` Docker image to use\n        env_vars: Environment variables to pass to the container\n        workspace_path: Path to workspace directory on host machine\n        log_file: Path to log file for recording model interactions\n    \"\"\"\n    async with CodeExecutionContainer(\n        tag=ipybox_tag,\n        env=env_vars,\n        workspace_path=workspace_path,\n    ) as container:\n        async with CodeExecutor(\n            key=executor_key,\n            port=container.port,\n            workspace=container.workspace,\n        ) as executor:\n            async with Logger(file=log_file) as logger:\n                yield CodeExecutionEnvironment(container=container, executor=executor, logger=logger)\n</code></pre>"},{"location":"api/gemini/","title":"Gemini","text":""},{"location":"api/gemini/#freeact.model.gemini.model.chat.GeminiModelName","title":"GeminiModelName  <code>module-attribute</code>","text":"<pre><code>GeminiModelName = Literal['gemini-2.0-flash', 'gemini-2.0-flash-001', 'gemini-2.0-flash-lite-preview-02-05', 'gemini-2.0-flash-exp', 'gemini-2.0-flash-thinking-exp', 'gemini-2.0-flash-thinking-exp-01-21']\n</code></pre>"},{"location":"api/gemini/#freeact.model.gemini.model.chat.Gemini","title":"Gemini","text":"<pre><code>Gemini(model_name: GeminiModelName = 'gemini-2.0-flash', skill_sources: str | None = None, temperature: float = 0.0, max_tokens: int = 4096, **kwargs)\n</code></pre> <p>               Bases: <code>CodeActModel</code></p> <p>A <code>CodeActModel</code> implementation based on Google's Gemini 2 chat API.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>GeminiModelName</code> <p>The specific Gemini 2 model to use</p> <code>'gemini-2.0-flash'</code> <code>skill_sources</code> <code>str | None</code> <p>Skill module sources to include in the system instruction</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Controls randomness in the model's output (0.0 = deterministic)</p> <code>0.0</code> <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens in the model's response</p> <code>4096</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the Google Gen AI client.</p> <code>{}</code> Source code in <code>freeact/model/gemini/model/chat.py</code> <pre><code>def __init__(\n    self,\n    model_name: GeminiModelName = \"gemini-2.0-flash\",\n    skill_sources: str | None = None,\n    temperature: float = 0.0,\n    max_tokens: int = 4096,\n    **kwargs,\n):\n    is_thinking_model = \"thinking\" in model_name.lower()\n\n    if is_thinking_model:\n        # ------------------------------------------------------\n        #  EXPERIMENTAL\n        # ------------------------------------------------------\n        self.system_template = thinking.SYSTEM_TEMPLATE.format(\n            python_modules=skill_sources or \"\",\n            python_packages=thinking.EXAMPLE_PYTHON_PACKAGES,\n            rest_apis=thinking.EXAMPLE_REST_APIS,\n        )\n        self.execution_error_template = thinking.EXECUTION_ERROR_TEMPLATE\n        self.execution_output_template = thinking.EXECUTION_OUTPUT_TEMPLATE\n        self.thinking_config = ThinkingConfig(include_thoughts=True)\n    else:\n        self.system_template = default.SYSTEM_TEMPLATE.format(python_modules=skill_sources or \"\")\n        self.execution_error_template = default.EXECUTION_ERROR_TEMPLATE\n        self.execution_output_template = default.EXECUTION_OUTPUT_TEMPLATE\n        self.thinking_config = None\n\n    self._client = genai.Client(**kwargs, http_options={\"api_version\": \"v1alpha\"})\n    self._chat = self._client.aio.chats.create(\n        model=model_name,\n        config=GenerateContentConfig(\n            temperature=temperature,\n            max_output_tokens=max_tokens,\n            response_modalities=[\"TEXT\"],\n            system_instruction=self.system_template,\n            thinking_config=self.thinking_config,\n        ),\n    )\n</code></pre>"},{"location":"api/gemini/#freeact.model.gemini.model.live.GeminiLive","title":"GeminiLive  <code>async</code>","text":"<pre><code>GeminiLive(model_name: GeminiModelName = 'gemini-2.0-flash', skill_sources: str | None = None, temperature: float = 0.0, max_tokens: int = 4096, **kwargs)\n</code></pre> <p>Context manager for a <code>CodeActModel</code> implementation based on Google's Gemini 2 live API.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>GeminiModelName</code> <p>The specific Gemini 2 model to use</p> <code>'gemini-2.0-flash'</code> <code>skill_sources</code> <code>str | None</code> <p>Skill module sources to include in the system instruction.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Controls randomness in the model's output (0.0 = deterministic)</p> <code>0.0</code> <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens in the model's response</p> <code>4096</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the Google Gen AI client.</p> <code>{}</code> Example <pre><code>async with GeminiLive(model_name=\"gemini-2.0-flash\", skill_sources=skill_sources) as model:\n    # use model with active session to Gemini 2 live API\n    agent = CodeActAgent(model=model, ...)\n</code></pre> Source code in <code>freeact/model/gemini/model/live.py</code> <pre><code>@asynccontextmanager\nasync def GeminiLive(\n    model_name: GeminiModelName = \"gemini-2.0-flash\",\n    skill_sources: str | None = None,\n    temperature: float = 0.0,\n    max_tokens: int = 4096,\n    **kwargs,\n):\n    \"\"\"\n    Context manager for a `CodeActModel` implementation based on Google's Gemini 2 live API.\n\n    Args:\n        model_name: The specific Gemini 2 model to use\n        skill_sources: Skill module sources to include in the system instruction.\n        temperature: Controls randomness in the model's output (0.0 = deterministic)\n        max_tokens: Maximum number of tokens in the model's response\n        **kwargs: Additional keyword arguments to pass to the Google Gen AI client.\n\n    Example:\n        ```python\n        async with GeminiLive(model_name=\"gemini-2.0-flash\", skill_sources=skill_sources) as model:\n            # use model with active session to Gemini 2 live API\n            agent = CodeActAgent(model=model, ...)\n        ```\n    \"\"\"\n\n    client = genai.Client(http_options={\"api_version\": \"v1alpha\"}, **kwargs)\n    config = {\n        \"tools\": [],\n        \"generation_config\": {\n            \"temperature\": temperature,\n            \"max_output_tokens\": max_tokens,\n            \"response_modalities\": [\"TEXT\"],\n            \"system_instruction\": SYSTEM_TEMPLATE.format(\n                python_modules=skill_sources or \"\",\n            ),\n        },\n    }\n\n    async with client.aio.live.connect(model=model_name, config=config) as session:\n        yield _GeminiLive(session)\n</code></pre>"},{"location":"api/generic/","title":"Generic","text":""},{"location":"api/generic/#freeact.model.generic.model.GenericModel","title":"GenericModel","text":"<pre><code>GenericModel(model_name: str, execution_output_template: str, execution_error_template: str, system_message: str | None = None, run_kwargs: Dict[str, Any] | None = None, **kwargs)\n</code></pre> <p>               Bases: <code>CodeActModel</code></p> <p>A generic implementation of a code action model based on the OpenAI Python SDK.</p> <p>This class can be used to integrate any model that is accessible via the OpenAI Python SDK. See qwen for an implementation example.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The provider-specific name of the model.</p> required <code>system_message</code> <code>str | None</code> <p>The system message to guide the model to generate code actions.</p> <code>None</code> <code>execution_output_template</code> <code>str</code> <p>Prompt template for formatting successful execution feedback. Must define an <code>{execution_feedback}</code> placeholder.</p> required <code>execution_error_template</code> <code>str</code> <p>Prompt template for formatting execution error feedback. Must define an <code>{execution_feedback}</code> placeholder.</p> required <code>run_kwargs</code> <code>Dict[str, Any] | None</code> <p>Optional dictionary of additional arguments passed to the model's <code>request</code> and <code>feedback</code> methods.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to <code>AsyncOpenAI</code> client constructor. (e.g. <code>api_key</code>, <code>base_url</code>, ..., etc.).</p> <code>{}</code> Source code in <code>freeact/model/generic/model.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    execution_output_template: str,\n    execution_error_template: str,\n    system_message: str | None = None,\n    run_kwargs: Dict[str, Any] | None = None,\n    **kwargs,\n):\n    self.model_name = model_name\n    self.execution_output_template = execution_output_template\n    self.execution_error_template = execution_error_template\n    self.run_kwargs = run_kwargs or {}\n\n    self._history = []\n    self._client = AsyncOpenAI(**kwargs)\n\n    if system_message:\n        self._history.append({\"role\": \"system\", \"content\": system_message})\n</code></pre>"},{"location":"api/logger/","title":"Logger","text":""},{"location":"api/logger/#freeact.logger.Logger","title":"Logger","text":"<pre><code>Logger(file: str | Path | None = None)\n</code></pre> <p>An asynchronous logger supporting contextual logging with configurable output.</p> <p>Provides a queue-based logging system that can write to either a file or stdout. Supports contextual logging where entries can be grouped under specific contexts, and includes support for both message and error logging.</p> <p>Can be used either as an async context manager or directly:</p> <ul> <li> <p>As a context manager:     <pre><code>async with Logger(\"app.log\") as logger:\n    await logger.log(\"message\")\n    # Logger automatically closes\n</code></pre></p> </li> <li> <p>Direct usage:     <pre><code>logger = Logger(\"app.log\")\nawait logger.log(\"message\")\nawait logger.aclose()  # Must be explicitly closed\n</code></pre></p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | Path | None</code> <p>Path to the log file. If None, logs will be written to stdout</p> <code>None</code> <p>Initialize a new Logger instance.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | Path | None</code> <p>Path to the log file. If None, logs will be written to stdout.</p> <code>None</code> Source code in <code>freeact/logger.py</code> <pre><code>def __init__(self, file: str | Path | None = None):\n    \"\"\"Initialize a new Logger instance.\n\n    Args:\n        file: Path to the log file. If None, logs will be written to stdout.\n    \"\"\"\n    self.writer = FileWriter(file) if file else StdoutWriter()\n    self.var = ContextVar[List[str]](\"context\", default=[])\n    self.queue = asyncio.Queue()  # type: ignore\n    self.runner = asyncio.create_task(self._run())\n</code></pre>"},{"location":"api/logger/#freeact.logger.Logger.aclose","title":"aclose  <code>async</code>","text":"<pre><code>aclose()\n</code></pre> <p>Close the logger and process remaining entries.</p> <p>This method must be called explicitly when using the <code>Logger</code> directly (not as a context manager) to ensure all queued log entries are processed before shutting down.</p> <p>When using the <code>Logger</code> as a context manager, this method is called automatically.</p> Source code in <code>freeact/logger.py</code> <pre><code>async def aclose(self):\n    \"\"\"Close the logger and process remaining entries.\n\n    This method must be called explicitly when using the `Logger` directly (not as\n    a context manager) to ensure all queued log entries are processed before\n    shutting down.\n\n    When using the `Logger` as a context manager, this method is called automatically.\n    \"\"\"\n    # first process all queued entries\n    await self.queue.join()\n    # then cancel the queue consumer\n    self.runner.cancel()\n</code></pre>"},{"location":"api/logger/#freeact.logger.Logger.context","title":"context  <code>async</code>","text":"<pre><code>context(frame: str)\n</code></pre> <p>A context manager that adds an additional context frame to the current context.</p> Example <pre><code>async with logger.context(\"User Login\"):\n    await logger.log(\"Attempting login\")  # Logs with \"User Login\" context\n    await db.query(...)\n    await logger.log(\"Login successful\")  # Logs with \"User Login\" context\n\n    # Contexts can be nested\n    async with logger.context(\"Profile\"):\n        # Logs with \"User Login / Profile\" context\n        await logger.log(\"Loading user profile\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>str</code> <p>The context frame name to add</p> required <p>Yields:</p> Type Description <p>The Logger instance.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Re-raises any exception that occurs within the context after logging it.</p> Source code in <code>freeact/logger.py</code> <pre><code>@asynccontextmanager\nasync def context(self, frame: str):\n    \"\"\"A context manager that adds an additional context frame to the current context.\n\n    Example:\n        ```python\n        async with logger.context(\"User Login\"):\n            await logger.log(\"Attempting login\")  # Logs with \"User Login\" context\n            await db.query(...)\n            await logger.log(\"Login successful\")  # Logs with \"User Login\" context\n\n            # Contexts can be nested\n            async with logger.context(\"Profile\"):\n                # Logs with \"User Login / Profile\" context\n                await logger.log(\"Loading user profile\")\n        ```\n\n    Args:\n        frame: The context frame name to add\n\n    Yields:\n        The Logger instance.\n\n    Raises:\n        Exception: Re-raises any exception that occurs within the context after logging it.\n    \"\"\"\n    context = self.var.get().copy()\n    context.append(frame)\n    token = self.var.set(context)\n\n    try:\n        yield self\n    except Exception as e:\n        await self.log_error(e)\n        raise\n    finally:\n        self.var.reset(token)\n</code></pre>"},{"location":"api/logger/#freeact.logger.Logger.log","title":"log  <code>async</code>","text":"<pre><code>log(message: str, metadata: dict[str, Any] | None = None)\n</code></pre> <p>Log a message with optional metadata.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to log</p> required <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of additional data to include</p> <code>None</code> Source code in <code>freeact/logger.py</code> <pre><code>async def log(self, message: str, metadata: dict[str, Any] | None = None):\n    \"\"\"Log a message with optional metadata.\n\n    Args:\n        message: The message to log\n        metadata: Optional dictionary of additional data to include\n    \"\"\"\n    entry = MessageEntry(\n        context=self.var.get(),\n        message=message,\n        caller=self._get_caller_module_name(),\n        metadata=metadata,\n    )\n    await self.queue.put(entry)\n</code></pre>"},{"location":"api/logger/#freeact.logger.Logger.log_error","title":"log_error  <code>async</code>","text":"<pre><code>log_error(e: Exception)\n</code></pre> <p>Log an exception with its full traceback.</p> <p>Parameters:</p> Name Type Description Default <code>e</code> <code>Exception</code> <p>The exception to log.</p> required Source code in <code>freeact/logger.py</code> <pre><code>async def log_error(self, e: Exception):\n    \"\"\"Log an exception with its full traceback.\n\n    Args:\n        e: The exception to log.\n    \"\"\"\n    entry = ErrorEntry(\n        context=self.var.get(),\n        error=e,\n    )\n    await self.queue.put(entry)\n</code></pre>"},{"location":"api/model/","title":"Model","text":"<p>This module defines the interfaces of code action models. A code action model is a model that responds with code if it decides to perform an action in the environment.</p> <p>This module defines the core interfaces that code action models must implement to work with the <code>freeact</code> agent system. It defines abstract base classes for:</p> <ul> <li>The main model interface (<code>CodeActModel</code>)</li> <li>Model interaction turns (<code>CodeActModelTurn</code>)</li> <li>Model responses (<code>CodeActModelResponse</code>)</li> </ul>"},{"location":"api/model/#freeact.model.base.CodeActModel","title":"CodeActModel","text":"<p>               Bases: <code>ABC</code></p> <p>Interface for models that can generate code actions.</p> <p>A code action model handles both initial user queries and feedback from code execution results. It decides when to generate code for execution and when to provide final responses to the user.</p>"},{"location":"api/model/#freeact.model.base.CodeActModel.feedback","title":"feedback  <code>abstractmethod</code>","text":"<pre><code>feedback(feedback: str, is_error: bool, tool_use_id: str | None, tool_use_name: str | None, **kwargs) -&gt; CodeActModelTurn\n</code></pre> <p>Create a new interaction turn from execution feedback.</p> <p>Initiates a new interaction based on feedback from previous code execution, allowing the model to refine or correct its responses. A feedback turn must follow a previous request turn or feedback turn.</p> <p>Parameters:</p> Name Type Description Default <code>feedback</code> <code>str</code> <p>The feedback text from code execution.</p> required <code>is_error</code> <code>bool</code> <p>Whether the feedback represents an error condition.</p> required <code>tool_use_id</code> <code>str | None</code> <p>Identifier for the specific tool use instance.</p> required <code>tool_use_name</code> <code>str | None</code> <p>Name of the tool that was used.</p> required <code>**kwargs</code> <p>Additional model-specific parameters for the feedback.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>CodeActModelTurn</code> <code>CodeActModelTurn</code> <p>A new turn object representing this interaction.</p> Source code in <code>freeact/model/base.py</code> <pre><code>@abstractmethod\ndef feedback(\n    self,\n    feedback: str,\n    is_error: bool,\n    tool_use_id: str | None,\n    tool_use_name: str | None,\n    **kwargs,\n) -&gt; CodeActModelTurn:\n    \"\"\"Create a new interaction turn from execution feedback.\n\n    Initiates a new interaction based on feedback from previous code execution,\n    allowing the model to refine or correct its responses. A feedback turn must\n    follow a previous request turn or feedback turn.\n\n    Args:\n        feedback (str): The feedback text from code execution.\n        is_error (bool): Whether the feedback represents an error condition.\n        tool_use_id (str | None): Identifier for the specific tool use instance.\n        tool_use_name (str | None): Name of the tool that was used.\n        **kwargs: Additional model-specific parameters for the feedback.\n\n    Returns:\n        CodeActModelTurn: A new turn object representing this interaction.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/model/#freeact.model.base.CodeActModel.request","title":"request  <code>abstractmethod</code>","text":"<pre><code>request(user_query: str, **kwargs) -&gt; CodeActModelTurn\n</code></pre> <p>Creates a new interaction turn from a user query.</p> <p>Parameters:</p> Name Type Description Default <code>user_query</code> <code>str</code> <p>The user's input query or request</p> required <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>CodeActModelTurn</code> <code>CodeActModelTurn</code> <p>A new turn object representing this interaction.</p> Source code in <code>freeact/model/base.py</code> <pre><code>@abstractmethod\ndef request(self, user_query: str, **kwargs) -&gt; CodeActModelTurn:\n    \"\"\"Creates a new interaction turn from a user query.\n\n    Args:\n        user_query: The user's input query or request\n        **kwargs: Additional model-specific parameters\n\n    Returns:\n        CodeActModelTurn: A new turn object representing this interaction.\n    \"\"\"\n</code></pre>"},{"location":"api/model/#freeact.model.base.CodeActModelResponse","title":"CodeActModelResponse  <code>dataclass</code>","text":"<pre><code>CodeActModelResponse(text: str, is_error: bool, token_usage: Dict[str, int] = dict())\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>A response from a code action model.</p> <p>Represents a single response from the model, which may contain executable code, error information, and tool usage metadata.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>str</code> <p>The raw text response from the model.</p> <code>is_error</code> <code>bool</code> <p>Whether this response represents an error condition.</p> <code>token_usage</code> <code>Dict[str, int]</code> <p>Provider-specific token usage data.</p>"},{"location":"api/model/#freeact.model.base.CodeActModelResponse.code","title":"code  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>code: str | None\n</code></pre> <p>Executable code from the model response if present.</p>"},{"location":"api/model/#freeact.model.base.CodeActModelTurn","title":"CodeActModelTurn","text":"<p>               Bases: <code>ABC</code></p> <p>A single turn of interaction with a code action model.</p> <p>Supports both bulk response retrieval and incremental streaming of the model's output. Each turn represents one complete model interaction, whether from a user query or execution feedback.</p>"},{"location":"api/model/#freeact.model.base.CodeActModelTurn.response","title":"response  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>response() -&gt; CodeActModelResponse\n</code></pre> <p>Get the complete response for this model interaction turn.</p> <p>Waits for and returns the full model response, including any code blocks and metadata about tool usage.</p> Source code in <code>freeact/model/base.py</code> <pre><code>@abstractmethod\nasync def response(self) -&gt; CodeActModelResponse:\n    \"\"\"Get the complete response for this model interaction turn.\n\n    Waits for and returns the full model response, including any code blocks\n    and metadata about tool usage.\n    \"\"\"\n</code></pre>"},{"location":"api/model/#freeact.model.base.CodeActModelTurn.stream","title":"stream  <code>abstractmethod</code>","text":"<pre><code>stream(emit_retry: bool = False) -&gt; AsyncIterator[str | StreamRetry]\n</code></pre> <p>Stream the model's response as it is generated.</p> <p>Yields chunks of the response as they become available, allowing for real-time processing of model output.</p> <p>Parameters:</p> Name Type Description Default <code>emit_retry</code> <code>bool</code> <p>If <code>True</code>, emit <code>StreamRetry</code> objects when retries occur.              If <code>False</code>, handle retries silently. Defaults to <code>False</code>.</p> <code>False</code> <p>Yields:</p> Type Description <code>AsyncIterator[str | StreamRetry]</code> <p>str | StreamRetry: Either a chunk of the response text, or a <code>StreamRetry</code>              object if a retry event occurs and <code>emit_retry</code> is <code>True</code>.</p> Source code in <code>freeact/model/base.py</code> <pre><code>@abstractmethod\ndef stream(self, emit_retry: bool = False) -&gt; AsyncIterator[str | StreamRetry]:\n    \"\"\"Stream the model's response as it is generated.\n\n    Yields chunks of the response as they become available, allowing for\n    real-time processing of model output.\n\n    Args:\n        emit_retry (bool): If `True`, emit `StreamRetry` objects when retries occur.\n                         If `False`, handle retries silently. Defaults to `False`.\n\n    Yields:\n        str | StreamRetry: Either a chunk of the response text, or a `StreamRetry`\n                         object if a retry event occurs and `emit_retry` is `True`.\n    \"\"\"\n</code></pre>"},{"location":"api/model/#freeact.model.base.StreamRetry","title":"StreamRetry  <code>dataclass</code>","text":"<pre><code>StreamRetry(cause: str, retry_wait_time: float)\n</code></pre> <p>Emitted in a response stream to inform about a retry event.</p> <p>Used when streaming responses encounter temporary failures and need to retry.</p> <p>Attributes:</p> Name Type Description <code>cause</code> <code>str</code> <p>The reason for the retry attempt.</p> <code>retry_wait_time</code> <code>float</code> <p>Time in seconds to wait before retrying.</p>"},{"location":"api/qwen/","title":"Qwen","text":""},{"location":"api/qwen/#freeact.model.qwen.model.QwenCoder","title":"QwenCoder","text":"<pre><code>QwenCoder(model_name: str, api_key: str | None = None, base_url: str | None = None, skill_sources: str | None = None, system_template: str = SYSTEM_TEMPLATE, execution_output_template: str = EXECUTION_OUTPUT_TEMPLATE, execution_error_template: str = EXECUTION_ERROR_TEMPLATE, run_kwargs: Dict[str, Any] | None = {'stop': ['```output']}, **kwargs)\n</code></pre> <p>               Bases: <code>GenericModel</code></p> <p>A specialized implementation of <code>GenericModel</code> for Qwen's Coder models.</p> <p>This class configures <code>GenericModel</code> specifically for use with Qwen 2.5 Coder models, It has been tested with QwenCoder 2.5 Coder 32B Instruct. Smaller models in this series may require adjustments to the prompt templates.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The provider-specific name of the Qwen model to use.</p> required <code>api_key</code> <code>str | None</code> <p>Optional API key for Qwen. If not provided, reads from QWEN_API_KEY environment variable.</p> <code>None</code> <code>base_url</code> <code>str | None</code> <p>Optional base URL for the API. If not provided, reads from QWEN_BASE_URL environment variable.</p> <code>None</code> <code>skill_sources</code> <code>str | None</code> <p>Optional string containing Python skill module information to include in system template.</p> <code>None</code> <code>system_template</code> <code>str</code> <p>Prompt template for the system message that guides the model to generate code actions. Must define a <code>{python_modules}</code> placeholder for the skill sources.</p> <code>SYSTEM_TEMPLATE</code> <code>execution_output_template</code> <code>str</code> <p>Prompt template for formatting execution outputs. Must define an <code>{execution_feedback}</code> placeholder.</p> <code>EXECUTION_OUTPUT_TEMPLATE</code> <code>execution_error_template</code> <code>str</code> <p>Prompt template for formatting execution errors. Must define an <code>{execution_feedback}</code> placeholder.</p> <code>EXECUTION_ERROR_TEMPLATE</code> <code>run_kwargs</code> <code>Dict[str, Any] | None</code> <p>Optional dictionary of additional arguments passed to the model's <code>request</code> and <code>feedback</code> methods. Defaults to a stop sequence that prevents the model from guessing code execution outputs.</p> <code>{'stop': ['```output']}</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the <code>GenericModel</code> constructor.</p> <code>{}</code> Source code in <code>freeact/model/qwen/model.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    api_key: str | None = None,\n    base_url: str | None = None,\n    skill_sources: str | None = None,\n    system_template: str = SYSTEM_TEMPLATE,\n    execution_output_template: str = EXECUTION_OUTPUT_TEMPLATE,\n    execution_error_template: str = EXECUTION_ERROR_TEMPLATE,\n    run_kwargs: Dict[str, Any] | None = {\"stop\": [\"```output\"]},\n    **kwargs,\n):\n    super().__init__(\n        model_name=model_name,\n        api_key=api_key or os.getenv(\"QWEN_API_KEY\"),\n        base_url=base_url or os.getenv(\"QWEN_BASE_URL\"),\n        execution_output_template=execution_output_template,\n        execution_error_template=execution_error_template,\n        system_message=system_template.format(python_modules=skill_sources or \"\"),\n        run_kwargs=run_kwargs,\n        **kwargs,\n    )\n</code></pre>"},{"location":"tutorials/","title":"Overview","text":"<ol> <li>Basic usage - Learn how to set up an agent, model, and code execution environment. This minimal setup demonstrates running generative Google searches and plotting the results.</li> <li>Skill development - Learn how to develop and improve custom skills in a conversation with the agent. The agent leverages its software engineering capabilities to support this process.</li> <li>System extensions - Learn how to define custom agent behavior and constraints through system extensions in natural language. This enables human-in-the-loop workflows, proactive agents, and more.</li> </ol> <p>All tutorials use a prebuilt Docker image for sandboxed code execution and the <code>freeact</code> CLI for user-agent interactions. The Basic usage tutorial additionally demonstrates the minimal Python code needed to implement a <code>freeact</code> agent.</p>"},{"location":"tutorials/basics/","title":"Basic usage","text":"<p>A <code>freeact</code> agent system consists of:</p> <ul> <li>A code execution Docker container, managed by the <code>CodeExecutionContainer</code> context manager. This tutorial uses the prebuilt <code>ghcr.io/gradion-ai/ipybox:example</code> image.</li> <li>A code executor, managed by the <code>CodeExecutor</code> context manager. It manages an IPython kernel's lifecycle within the container and handles code execution.</li> <li>A code action model that generates code actions to be executed by the executor. Models must implement the interfaces defined in the <code>freeact.model</code> package. This tutorial uses <code>Claude</code>, configured with <code>claude-3-5-sonnet-20241022</code> as model name.</li> <li>A <code>CodeActAgent</code> configured with both the model and executor. It orchestrates their interaction until a final response is ready.</li> </ul> examples/basics.py<pre><code>import asyncio\nimport os\n\nfrom dotenv import load_dotenv\n\nfrom examples.utils import stream_conversation\nfrom freeact import (\n    Claude,\n    CodeActAgent,\n    CodeExecutionContainer,\n    CodeExecutor,\n)\nfrom freeact.logger import Logger\n\n\nasync def main():\n    api_keys = {\n        \"ANTHROPIC_API_KEY\": os.environ[\"ANTHROPIC_API_KEY\"],\n        \"GOOGLE_API_KEY\": os.environ[\"GOOGLE_API_KEY\"],\n    }\n\n    async with CodeExecutionContainer(\n        tag=\"ghcr.io/gradion-ai/ipybox:example\",  # (2)!\n        env=api_keys,\n        workspace_path=\"workspace\",  # (3)!\n    ) as container:\n        async with CodeExecutor(\n            key=\"example\",  # (4)!\n            port=container.port,  # (5)!\n            workspace=container.workspace,\n        ) as executor:\n            skill_sources = await executor.get_module_sources(\n                [\"freeact_skills.search.google.stream.api\"],  # (6)!\n            )\n            async with Logger(file=\"logs/agent.log\") as logger:  # (7)!\n                model = Claude(model_name=\"claude-3-5-sonnet-20241022\", logger=logger)\n                agent = CodeActAgent(model=model, executor=executor)\n                await stream_conversation(agent, skill_sources=skill_sources)  # (1)!\n\n\nif __name__ == \"__main__\":\n    load_dotenv()\n    asyncio.run(main())\n</code></pre> <ol> <li> examples/utils.py::stream_conversation<pre><code>async def stream_conversation(agent: CodeActAgent, **kwargs):\n    while True:\n        user_message = await ainput(\"User message: ('q' to quit) \")\n\n        if user_message.lower() == \"q\":\n            break\n\n        agent_turn = agent.run(user_message, **kwargs)\n        await stream_turn(agent_turn)\n\n\nasync def stream_turn(agent_turn: CodeActAgentTurn):\n    produced_images: Dict[Path, Image.Image] = {}\n\n    async for activity in agent_turn.stream():\n        match activity:\n            case CodeActModelTurn() as turn:\n                print(\"Agent response:\")\n                async for s in turn.stream():\n                    print(s, end=\"\", flush=True)\n                print()\n\n                response = await turn.response()\n                if response.code:\n                    print(\"\\n```python\")\n                    print(response.code)\n                    print(\"```\\n\")\n\n            case CodeExecution() as execution:\n                print(\"Execution result:\")\n                async for s in execution.stream():\n                    print(s, end=\"\", flush=True)\n                result = await execution.result()\n                produced_images.update(result.images)\n                print()\n\n    if produced_images:\n        print(\"\\n\\nProduced images:\")\n    for path in produced_images.keys():\n        print(str(path))\n</code></pre> </li> <li> <p>Tag of the <code>ipybox</code> Docker image.</p> </li> <li> <p>Path to the workspace directory on the host machine. This directory enables sharing custom skills modules between the container and host machine (see Skill development tutorial).</p> </li> <li> <p>Key for this executor's private workspace directories:</p> <ul> <li><code>workspace/skills/private/example</code>: Private skills and working directory</li> <li><code>workspace/images/example</code>: Directory for storing produced images</li> </ul> </li> <li> <p>Container host port. Automatically allocated by <code>CodeExecutionContainer</code> but can be manually specified.</p> </li> <li> <p>Skill modules on the <code>executor</code>'s Python path that can be resolved to their source code and metadata. This information is included in the code action <code>model</code>'s context.</p> </li> <li> <p>A contextual <code>Logger</code> for recording messages and metadata.</p> </li> </ol> <p>A <code>CodeActAgent</code> can engage in multi-turn conversations with a user. Each turn is initiated using the agent's <code>run</code> method. We use the <code>stream_conversation</code> (1) helper function to <code>run</code> the agent and stream the output from both the agent's model and code executor to <code>stdout</code>.</p> <ol> <li>examples/utils.py::stream_conversation<pre><code>async def stream_conversation(agent: CodeActAgent, **kwargs):\n    while True:\n        user_message = await ainput(\"User message: ('q' to quit) \")\n\n        if user_message.lower() == \"q\":\n            break\n\n        agent_turn = agent.run(user_message, **kwargs)\n        await stream_turn(agent_turn)\n\n\nasync def stream_turn(agent_turn: CodeActAgentTurn):\n    produced_images: Dict[Path, Image.Image] = {}\n\n    async for activity in agent_turn.stream():\n        match activity:\n            case CodeActModelTurn() as turn:\n                print(\"Agent response:\")\n                async for s in turn.stream():\n                    print(s, end=\"\", flush=True)\n                print()\n\n                response = await turn.response()\n                if response.code:\n                    print(\"\\n```python\")\n                    print(response.code)\n                    print(\"```\\n\")\n\n            case CodeExecution() as execution:\n                print(\"Execution result:\")\n                async for s in execution.stream():\n                    print(s, end=\"\", flush=True)\n                result = await execution.result()\n                produced_images.update(result.images)\n                print()\n\n    if produced_images:\n        print(\"\\n\\nProduced images:\")\n    for path in produced_images.keys():\n        print(str(path))\n</code></pre></li> </ol> <p>This tutorial uses the <code>freeact_skills.search.google.stream.api</code> skill module from the <code>freeact-skills</code> project to process queries that require internet searches. This module provides generative Google search capabilities powered by the Gemini 2 API.</p> <p>The skill module's source code is obtained from the <code>executor</code> and passed to the model through the agent's <code>run</code> method. Other model implementations may require skill module sources to be passed through their constructor instead.</p>"},{"location":"tutorials/basics/#setup","title":"Setup","text":"<p>Install <code>freeact</code> with:</p> <pre><code>pip install freeact\n</code></pre> <p>The tutorials require an <code>ANTHROPIC_API_KEY</code> for the Claude API and a <code>GOOGLE_API_KEY</code> for the Gemini 2 API. You can get them from Anthropic Console and Google AI Studio. Add them to a <code>.env</code> file in the current working directory:</p> .env<pre><code>ANTHROPIC_API_KEY=...\nGOOGLE_API_KEY=...\n</code></pre> <p>The tutorials use the prebuilt <code>ghcr.io/gradion-ai/ipybox:example</code> Docker image for sandboxed code execution.</p>"},{"location":"tutorials/basics/#running","title":"Running","text":"<p>Download the Python example</p> <pre><code>mkdir examples\ncurl -o examples/basics.py https://raw.githubusercontent.com/gradion-ai/freeact/refs/heads/main/examples/basics.py\ncurl -o examples/utils.py https://raw.githubusercontent.com/gradion-ai/freeact/refs/heads/main/examples/utils.py\n</code></pre> <p>and run it with:</p> <pre><code>python examples/basics.py\n</code></pre> <p>For formatted and colored console output, as shown in the example conversation, you can use the <code>freeact</code> CLI:</p> <pre><code>python -m freeact.cli \\\n  --model-name=claude-3-5-sonnet-20241022 \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:example \\\n  --executor-key=example \\\n  --skill-modules=freeact_skills.search.google.stream.api\n</code></pre> <p>To use Gemini instead of Claude, run:</p> <pre><code>python -m freeact.cli \\\n  --model-name=gemini-2.0-flash \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:example \\\n  --executor-key=example \\\n  --skill-modules=freeact_skills.search.google.stream.api\n</code></pre> <p>See also Supported models for other CLI examples.</p>"},{"location":"tutorials/basics/#example-conversation","title":"Example conversation","text":"<p>Produced images:</p> <p></p>"},{"location":"tutorials/extend/","title":"System extensions","text":"<p>This tutorial demonstrates how <code>freeact</code> agents can be customized through system extensions. These may include runbooks, constraints, domain-specific rules, or whatever you think is relevant for the agent's behavior. You'll see:</p> <ul> <li>How to implement a human-in-the-loop workflow, enabling users to review and edit the agent's action plans</li> <li>How to make the agent more proactive by configuring it to suggest 3 follow-up actions after each response</li> <li>How to implement domain-specific rules (demonstrated by an example that multiplies temperatures by 3.17 in weather-related responses)</li> </ul> <p>Note</p> <p>System extensions are currently only supported for Claude models and DeepSeek V3.</p> <p>The example conversation below was guided by this system extension:</p> extension.txt<pre><code>Your overall workflow instructions (= runbook):\n- Start answering an initial user query\n- In your final answer to the user query, additionally suggest 3 follow up actions the user can take\n- Let the user choose one of the follow up actions or choose another action if none of the follow up actions are relevant\n- After sharing your reasoning in &lt;thinking&gt; tags, you MUST ask the user for feedback.\n  - do not generate and execute code yet at this point.\n  - the user may either confirm your reasoning and let you proceed with generating code\n  - or ask you to modify your reasoning and reasoning steps based on their feedback\n- Repeat the overall workflow with the chosen follow up action\n\nDomain-specific rules:\n- If the initial query or a follow-up action is related to the weather in a given city, output the current temperature in the city multiplied by 3.17.\n</code></pre> <p>It was initiated with the following <code>freeact.cli</code> command, referencing the <code>extension.txt</code> file with the <code>--system-extension</code> option and reusing the developed <code>weather.weather_report</code> skill from the Skill development tutorial.</p> <pre><code>python -m freeact.cli \\\n  --model-name=claude-3-5-sonnet-20241022 \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:example \\\n  --executor-key=example \\\n  --skill-modules=freeact_skills.search.google.stream.api \\\n  --skill-modules=freeact_skills.zotero.api \\\n  --skill-modules=freeact_skills.reader.api \\\n  --skill-modules=weather.weather_report \\\n  --system-extension=extension.txt\n</code></pre>"},{"location":"tutorials/extend/#example-conversation","title":"Example conversation","text":""},{"location":"tutorials/skills/","title":"Skill development","text":"<p>This tutorial demonstrates how to develop and use custom skills through two examples: first creating a weather reporting skill from scratch, and then using that skill to generate weather comparisons and visualizations.</p> <p>Prerequisites</p> <p>It is recommended to complete the Basic Usage tutorial before proceeding with this one, as it covers fundamental concepts that are built upon here.</p>"},{"location":"tutorials/skills/#interactive-development","title":"Interactive development","text":"<p>This section demonstrates the interactive development of a custom weather reporting skill through a conversation between a user and a <code>freeact</code> agent. You'll see how the agent assists with software engineering tasks, evolving a simple weather query into a Python package that can:</p> <ul> <li>Fetch current weather conditions for any city</li> <li>Resolve city names to geographical coordinates</li> <li>Retrieve historical weather data</li> <li>Return structured data for both current and historical weather</li> </ul> <p>The development progresses through several iterations:</p> <ol> <li>Creating a code action to fetch Vienna's weather</li> <li>Converting it into a proper Python package</li> <li>Adding city name coordinate resolution</li> <li>Incorporating historical weather data</li> <li>Testing with various cities (New York, Vienna, Salzburg)</li> </ol> <p>The interactively developed <code>weather.weather_report</code> (1) skill uses the Open-Meteo API for weather data and geocoding.</p> <ol> <li><pre><code>\"\"\"Module for getting weather reports for cities.\"\"\"\n\nfrom datetime import datetime, timedelta\nfrom typing import Any, Dict\n\nimport requests\n\n\ndef get_weather_report(city_name: str, n_days: int = 7) -&gt; Dict[str, Any]:\n    \"\"\"Get current and historical weather report for a given city.\n\n    Args:\n        city_name: Name of the city to get weather for\n        n_days: Number of past days to get historical data for (excluding current day)\n\n    Returns:\n        Dictionary containing:\n        - temperature: Current temperature in Celsius\n        - humidity: Current relative humidity percentage\n        - measurement_time: Timestamp of current measurement\n        - coordinates: Dict with latitude and longitude\n        - city: City name used for query\n        - history: List of daily measurements for past n_days (excluding current day), each containing:\n            - date: Date of measurement\n            - temperature: Average daily temperature in Celsius\n            - humidity: Average daily relative humidity percentage\n    \"\"\"\n    # First get coordinates using geocoding API\n    geocoding_url = f\"https://geocoding-api.open-meteo.com/v1/search?name={city_name}&amp;count=1&amp;language=en&amp;format=json\"\n    geo_response = requests.get(geocoding_url)\n    geo_data = geo_response.json()\n\n    if not geo_data.get(\"results\"):\n        raise ValueError(f\"Could not find coordinates for city: {city_name}\")\n\n    location = geo_data[\"results\"][0]\n    lat = location[\"latitude\"]\n    lon = location[\"longitude\"]\n\n    # Calculate date range for historical data\n    end_date = datetime.now().date() - timedelta(days=1)  # yesterday\n    start_date = end_date - timedelta(days=n_days - 1)\n\n    # Get current and historical weather data using coordinates\n    weather_url = (\n        f\"https://api.open-meteo.com/v1/forecast?\"\n        f\"latitude={lat}&amp;longitude={lon}\"\n        f\"&amp;current=temperature_2m,relative_humidity_2m\"\n        f\"&amp;daily=temperature_2m_mean,relative_humidity_2m_mean\"\n        f\"&amp;timezone=auto\"\n        f\"&amp;start_date={start_date}&amp;end_date={end_date}\"\n    )\n    weather_response = requests.get(weather_url)\n    weather_data = weather_response.json()\n\n    current = weather_data[\"current\"]\n    daily = weather_data[\"daily\"]\n\n    # Process historical data\n    history = []\n    for i in range(len(daily[\"time\"])):\n        history.append(\n            {\n                \"date\": datetime.fromisoformat(daily[\"time\"][i]).date(),\n                \"temperature\": daily[\"temperature_2m_mean\"][i],\n                \"humidity\": daily[\"relative_humidity_2m_mean\"][i],\n            }\n        )\n\n    return {\n        \"temperature\": current[\"temperature_2m\"],\n        \"humidity\": current[\"relative_humidity_2m\"],\n        \"measurement_time\": datetime.fromisoformat(current[\"time\"]),\n        \"coordinates\": {\"latitude\": lat, \"longitude\": lon},\n        \"city\": location[\"name\"],\n        \"history\": history,\n    }\n</code></pre></li> </ol> <p>Note</p> <p>Interactive skill development is currently only supported for Claude models.</p> <p>The example conversation below was initiated with the following <code>freeact.cli</code> command<sup>1</sup>. The developed <code>weather.weather_report</code> skill is written to the <code>workspace/skills/private/example</code> directory.</p> <pre><code>python -m freeact.cli \\\n  --model-name=claude-3-5-sonnet-20241022 \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:example \\\n  --executor-key=example \\\n  --skill-modules=freeact_skills.search.google.stream.api \\\n  --skill-modules=freeact_skills.zotero.api \\\n  --skill-modules=freeact_skills.reader.api\n</code></pre>"},{"location":"tutorials/skills/#example-conversation","title":"Example conversation","text":""},{"location":"tutorials/skills/#skill-usage","title":"Skill usage","text":"<p>This section demonstrates how to use the previously developed weather reporting skill in a separate conversation. After loading <code>weather.weather_report</code> as additional skill<sup>2</sup>, we'll see how the agent can:</p> <ul> <li>Create multi-city weather reports with much smaller code actions</li> <li>Create data visualizations comparing weather patterns</li> <li>Provide clear, natural language summaries of weather conditions</li> </ul> <p>The example shows a request for weather data from Paris and Berlin, where the agent automatically:</p> <ol> <li>Retrieves current conditions and 4-day history for both cities using the <code>weather.weather_report</code> skill</li> <li>Creates comparative visualizations of temperature and humidity trends</li> <li>Presents the data in both graphical and text formats</li> </ol> <p>The example conversation was initiated with the following <code>freeact.cli</code> command. The developed skill is is read from <code>workspace/skills/private/example</code>. If we moved it to <code>workspace/skills/shared</code>, it would be available to all executors regardless of their <code>executor-key</code> value.</p> <pre><code>python -m freeact.cli \\\n  --model-name=claude-3-5-sonnet-20241022 \\\n  --ipybox-tag=ghcr.io/gradion-ai/ipybox:example \\\n  --executor-key=example \\\n  --skill-modules=freeact_skills.search.google.stream.api \\\n  --skill-modules=freeact_skills.zotero.api \\\n  --skill-modules=freeact_skills.reader.api \\\n  --skill-modules=weather.weather_report\n</code></pre> <p>Note</p> <p>If you've developed a custom skill that has external dependencies, you either need to build a custom Docker image with the required dependencies or need to install them at runtime prior to launching an agent.</p>"},{"location":"tutorials/skills/#example-conversation_1","title":"Example conversation","text":"<p>Produced images:</p> <p></p> <ol> <li> <p>The provided skill modules are not needed in this example but included to demonstrate that their usage is optional.\u00a0\u21a9</p> </li> <li> <p>We may automate this process in the future e.g. to load skill modules based on the details of a user's query.\u00a0\u21a9</p> </li> </ol>"}]}